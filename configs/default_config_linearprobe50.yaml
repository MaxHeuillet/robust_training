dataset: "Flowers"  # dataset (default: cifar10)
backbone: "convnext_tiny.fb_in1k"  # load backbone
ft_type: "linear_probing"  # fine tuning type

loss_function: "CLASSIC_AT"  # TRADES_v2
seed: 0  # random seed
epochs: 50  # number of total iterations to run

lr1: 1e-3 #this will be ignored with gradient tracking false
lr2: null
weight_decay1: 0.0 #this will be ignored with gradient tracking false
weight_decay2: null
scheduler: null
nb_completed_trials: null


use_rs: False
epsilon: 0.015686275  # epsilon of trades (4/255)
step_size: 0.007843137  # step size of trades (2/255)
perturb_steps: 10  # number of steps of trades
distance: "Linf"  # distance of trades
beta: 1.0  # beta of trades

# arguments for diffusion augmented learning
project_name: null # the experiment type
exp_id: null

statedicts_path: "~/my_backbones/"  # TODO: update, where we load the pretrained model
trained_statedicts_path: "~/my_backbones/trained_state_dicts/" # TODO: update, where we save fine tuned model
datasets_path: "~/data/" # where we load uncompressed dataset
hpo_path: "~/hpo_results/" # where we save the results of HPO
work_path: "~/work_dir/"
configs_path: "./configs/" # where we save the results of HPO
results_path: "./results/" # where we save the measured accuracies

# statedicts_path: "~/scratch/state_dicts_share/"  # TODO: update, where we load the pretrained model
# trained_statedicts_path: "~/scratch/trained_state_dicts/" # TODO: update, where we save fine tuned model
# datasets_path: "~/scratch/data/" # where we load uncompressed dataset
# hpo_path: "~/scratch/hpo_results/" # where we save the results of HPO
# work_path: "$SLURM_TMPDIR/"
# configs_path: "./configs/" # where we save the results of HPO
# results_path: "./results/" # where we save the measured accuracies
