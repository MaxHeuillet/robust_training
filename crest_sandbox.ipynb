{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Arguments: Namespace(approx_moment=True, approx_with_coreset=True, arch='resnet50', batch_num_mul=5, batch_size=128, check_interval=1, check_thresh_factor=0.1, data_dir='./data', dataset='CIFAR10', device=device(type='cuda'), drop_interval=20, drop_learned=False, drop_thresh=0.1, epochs=10, gamma=0.1, gpu=[0], interval_mul=1.0, log_dir='./logs', logger=<Logger logs-2024-08-09-12-23-00 (INFO)>, lr=0.1, lr_milestones=[100, 150], min_train_size=40000, momentum=0.9, num_classes=10, num_minibatch_coreset=5, num_workers=1, partition_start=0, random_subset_size=0.01, resume_from_epoch=0, runs=1, save_freq=200, seed=0, selection_method='crest', shuffle=True, smtk=0, statedict_dir='./state_dicts', subset_start_epoch=0, train_frac=0.1, use_wandb=False, warm_start_epochs=20, watch_interval=5, weight_decay=0.0001)\n",
      "Time: 2024-08-09 12:23:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataloader\n",
      "load dataloader\n"
     ]
    }
   ],
   "source": [
    "import models_local.resnet_cifar10\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import get_args\n",
    "from datasets import IndexedDataset\n",
    "import logging\n",
    "import time\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "# # Set up logging\n",
    "\n",
    "logger = logging.getLogger( args.log_dir.split('/')[-1] + time.strftime(\"-%Y-%m-%d-%H-%M-%S\") )\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=f\"{args.log_dir}/output.log\",\n",
    "    filemode=\"a\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,)\n",
    "\n",
    "# define a Handler which writes INFO messages or higher to the sys.stderr\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "logger.addHandler(ch)\n",
    "args.logger = logger\n",
    "\n",
    "# Print arguments\n",
    "args.logger.info(\"Arguments: {}\".format(args))\n",
    "args.logger.info(\"Time: {}\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "train_dataset = IndexedDataset(args, train=True)\n",
    "args.train_size = len(train_dataset)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        IndexedDataset(args, train=False),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "model = models_local.resnet_cifar10.resnet50(pretrained=False, progress=True).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainers import CRESTTrainer\n",
    "trainer = CRESTTrainer(args, model, train_dataset, val_loader )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 LR 0.000000\n",
      "Epoch 0:\tTrain Loss: 2.762605\tTrain Acc: 0.101758\tVal Loss: 2.516127\tVal Acc: 0.105300\n",
      "Epoch 0:\tData Loading Time: 0.001307\tForward Time: 0.013011\tBackward Time: 0.015480\n",
      "Epoch 1 LR 0.005000\n",
      "[||||||||||||||||||||]100% [Iteration 13 of 13]Updating train loader and weights with subset of size 650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] [Greedy], pred size: (500, 10)\n",
      "Greedy: selecting [13 13 13 13 13 13 13 13 13 13] elements\n",
      "time (sec) for computing facility location: (0.0002856254577636719, 0.00025343894958496094, 0.0002627372741699219, 0.00035643577575683594, 0.00036334991455078125, 0.0002682209014892578, 0.0003025531768798828, 0.0002455711364746094, 0.00024771690368652344, 0.00024628639221191406) similarity time (0.0008320808410644531, 0.00023412704467773438, 0.00024247169494628906, 0.00023698806762695312, 0.00027251243591308594, 0.0002503395080566406, 0.00030684471130371094, 0.00022935867309570312, 0.0002799034118652344, 0.0002200603485107422)\n",
      "Epoch [1] [Greedy], pred size: (500, 10)\n",
      "Greedy: selecting [13 13 13 13 13 13 13 13 13 13] elements\n",
      "time (sec) for computing facility location: (0.0003192424774169922, 0.00030922889709472656, 0.00027251243591308594, 0.00026297569274902344, 0.0002720355987548828, 0.00026917457580566406, 0.0002536773681640625, 0.0002512931823730469, 0.00025653839111328125, 0.0002551078796386719) similarity time (0.0002663135528564453, 0.0002694129943847656, 0.0002484321594238281, 0.00024199485778808594, 0.00023365020751953125, 0.00023984909057617188, 0.00023651123046875, 0.0002262592315673828, 0.00022602081298828125, 0.00022554397583007812)\n",
      "Epoch [1] [Greedy], pred size: (500, 10)\n",
      "Greedy: selecting [13 13 13 13 13 13 13 13 13 13] elements\n",
      "time (sec) for computing facility location: (0.00028586387634277344, 0.0002779960632324219, 0.00028967857360839844, 0.0002791881561279297, 0.00026798248291015625, 0.00029730796813964844, 0.00026798248291015625, 0.00027751922607421875, 0.0002658367156982422, 0.0002601146697998047) similarity time (0.0002894401550292969, 0.0002727508544921875, 0.0002777576446533203, 0.0002493858337402344, 0.0002357959747314453, 0.00023984909057617188, 0.00023746490478515625, 0.00023484230041503906, 0.00022745132446289062, 0.0002353191375732422)\n",
      "Epoch [1] [Greedy], pred size: (500, 10)\n",
      "Greedy: selecting [13 13 13 13 13 13 13 13 13 13] elements\n",
      "time (sec) for computing facility location: (0.0002841949462890625, 0.0002613067626953125, 0.00027179718017578125, 0.00026917457580566406, 0.0002665519714355469, 0.000255584716796875, 0.00026345252990722656, 0.00026035308837890625, 0.0002658367156982422, 0.0002689361572265625) similarity time (0.0002906322479248047, 0.0002377033233642578, 0.0002472400665283203, 0.00024056434631347656, 0.0002353191375732422, 0.00023221969604492188, 0.00023245811462402344, 0.0002315044403076172, 0.0002315044403076172, 0.00023102760314941406)\n",
      "Epoch [1] [Greedy], pred size: (500, 10)\n",
      "Greedy: selecting [13 13 13 13 13 13 13 13 13 13] elements\n",
      "time (sec) for computing facility location: (0.00029206275939941406, 0.0002684593200683594, 0.0002779960632324219, 0.00027632713317871094, 0.0002777576446533203, 0.0002665519714355469, 0.00026988983154296875, 0.0002658367156982422, 0.00034236907958984375, 0.0002803802490234375) similarity time (0.0002808570861816406, 0.00024366378784179688, 0.0002391338348388672, 0.00023937225341796875, 0.0002391338348388672, 0.0002319812774658203, 0.0002307891845703125, 0.00023674964904785156, 0.0003638267517089844, 0.00028395652770996094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mheuillet/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1177.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 2.94 GiB of which 6.12 MiB is free. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 48.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/crest/trainers/base_trainer.py:127\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_checkpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mresume_from_epoch)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mresume_from_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_epoch(epoch)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch(epoch)\n",
      "File \u001b[0;32m~/Desktop/robust_training/crest/trainers/crest_trainer.py:48\u001b[0m, in \u001b[0;36mCRESTTrainer._train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_train_loader_and_weights()\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_quadratic_approximation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m training_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\n",
      "File \u001b[0;32m~/Desktop/robust_training/crest/trainers/crest_trainer.py:164\u001b[0m, in \u001b[0;36mCRESTTrainer._get_quadratic_approximation\u001b[0;34m(self, epoch, training_step)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# approximate with hessian diagonal\u001b[39;00m\n\u001b[1;32m    163\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 164\u001b[0m gf_tmp, ggf_tmp, ggf_tmp_moment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_approx_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m approx_batch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgf \u001b[38;5;241m=\u001b[39m gf_tmp \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(idx)\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/crest/adahessian.py:111\u001b[0m, in \u001b[0;36mAdahessian.step\u001b[0;34m(self, momentum)\u001b[0m\n\u001b[1;32m    109\u001b[0m hutchinson_trace_moment \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m momentum:\n\u001b[0;32m--> 111\u001b[0m     hut_traces, reduced_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     reduced_grads \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (p, group, grad, hut_trace) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, groups, grads, hut_traces):\n",
      "File \u001b[0;32m~/Desktop/robust_training/crest/adahessian.py:72\u001b[0m, in \u001b[0;36mAdahessian.get_trace\u001b[0;34m(self, params, grads)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient tensor \u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m does not have grad_fn. When calling\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     64\u001b[0m                 i\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m  set to True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     70\u001b[0m v \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint_like(p, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[0;32m---> 72\u001b[0m hvs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     74\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m reduced_grads \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m hutchinson_trace \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    408\u001b[0m         grad_outputs_\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    424\u001b[0m     ):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 2.94 GiB of which 6.12 MiB is free. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 2.77 GiB is allocated by PyTorch, and 48.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
