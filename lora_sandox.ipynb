{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import lora\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return sample, label\n",
    "\n",
    "def add_lora(model):\n",
    "\n",
    "    # the list layers contains all the elements of the neural architecture on which you want to add lora matrices\n",
    "    layers = [ model.conv1, model.layer1[0].conv1, model.layer1[0].conv2, model.layer1[0].conv3,\n",
    "                model.layer2[0].conv1, model.layer2[0].conv2, model.layer2[0].conv3,\n",
    "                model.layer3[0].conv1, model.layer3[0].conv2, model.layer3[0].conv3,\n",
    "                model.layer4[0].conv1, model.layer4[0].conv2, model.layer4[0].conv3, model.fc ]\n",
    "\n",
    "    for conv_layer in layers:\n",
    "        lora_param = lora.layer_parametrization(conv_layer, device=\"cuda\", rank=10, lora_alpha=1)\n",
    "        parametrize.register_parametrization(conv_layer, 'weight', lora_param)\n",
    "\n",
    "    lora.set_lora_gradients(model, layers)\n",
    "\n",
    "# model = resnet50().to('cuda')\n",
    "from models_local import resnet_imagenet\n",
    "model = resnet_imagenet.ResNet(resnet_imagenet.Bottleneck, [3, 4, 6, 3], )\n",
    "state_dict = torch.load('./state_dicts/resnet50_imagenet1k.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "model.to('cuda')\n",
    "\n",
    "add_lora(model)\n",
    "\n",
    "size = 224\n",
    "data = [torch.rand(3, size, size) for _ in range(100)]\n",
    "labels = torch.randint(0, 10, (100,))\n",
    "dataset = CustomDataset(data, labels)\n",
    "loader = DataLoader(dataset, batch_size=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.parametrizations.weight.original torch.float32\n",
      "conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "bn1.weight torch.float32\n",
      "bn1.bias torch.float32\n",
      "layer1.0.conv1.parametrizations.weight.original torch.float32\n",
      "layer1.0.conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "layer1.0.conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "layer1.0.bn1.weight torch.float32\n",
      "layer1.0.bn1.bias torch.float32\n",
      "layer1.0.conv2.parametrizations.weight.original torch.float32\n",
      "layer1.0.conv2.parametrizations.weight.0.mat_A torch.float32\n",
      "layer1.0.conv2.parametrizations.weight.0.mat_B torch.float32\n",
      "layer1.0.bn2.weight torch.float32\n",
      "layer1.0.bn2.bias torch.float32\n",
      "layer1.0.conv3.parametrizations.weight.original torch.float32\n",
      "layer1.0.conv3.parametrizations.weight.0.mat_A torch.float32\n",
      "layer1.0.conv3.parametrizations.weight.0.mat_B torch.float32\n",
      "layer1.0.bn3.weight torch.float32\n",
      "layer1.0.bn3.bias torch.float32\n",
      "layer1.0.downsample.0.weight torch.float32\n",
      "layer1.0.downsample.1.weight torch.float32\n",
      "layer1.0.downsample.1.bias torch.float32\n",
      "layer1.1.conv1.weight torch.float32\n",
      "layer1.1.bn1.weight torch.float32\n",
      "layer1.1.bn1.bias torch.float32\n",
      "layer1.1.conv2.weight torch.float32\n",
      "layer1.1.bn2.weight torch.float32\n",
      "layer1.1.bn2.bias torch.float32\n",
      "layer1.1.conv3.weight torch.float32\n",
      "layer1.1.bn3.weight torch.float32\n",
      "layer1.1.bn3.bias torch.float32\n",
      "layer1.2.conv1.weight torch.float32\n",
      "layer1.2.bn1.weight torch.float32\n",
      "layer1.2.bn1.bias torch.float32\n",
      "layer1.2.conv2.weight torch.float32\n",
      "layer1.2.bn2.weight torch.float32\n",
      "layer1.2.bn2.bias torch.float32\n",
      "layer1.2.conv3.weight torch.float32\n",
      "layer1.2.bn3.weight torch.float32\n",
      "layer1.2.bn3.bias torch.float32\n",
      "layer2.0.conv1.parametrizations.weight.original torch.float32\n",
      "layer2.0.conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "layer2.0.conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "layer2.0.bn1.weight torch.float32\n",
      "layer2.0.bn1.bias torch.float32\n",
      "layer2.0.conv2.parametrizations.weight.original torch.float32\n",
      "layer2.0.conv2.parametrizations.weight.0.mat_A torch.float32\n",
      "layer2.0.conv2.parametrizations.weight.0.mat_B torch.float32\n",
      "layer2.0.bn2.weight torch.float32\n",
      "layer2.0.bn2.bias torch.float32\n",
      "layer2.0.conv3.parametrizations.weight.original torch.float32\n",
      "layer2.0.conv3.parametrizations.weight.0.mat_A torch.float32\n",
      "layer2.0.conv3.parametrizations.weight.0.mat_B torch.float32\n",
      "layer2.0.bn3.weight torch.float32\n",
      "layer2.0.bn3.bias torch.float32\n",
      "layer2.0.downsample.0.weight torch.float32\n",
      "layer2.0.downsample.1.weight torch.float32\n",
      "layer2.0.downsample.1.bias torch.float32\n",
      "layer2.1.conv1.weight torch.float32\n",
      "layer2.1.bn1.weight torch.float32\n",
      "layer2.1.bn1.bias torch.float32\n",
      "layer2.1.conv2.weight torch.float32\n",
      "layer2.1.bn2.weight torch.float32\n",
      "layer2.1.bn2.bias torch.float32\n",
      "layer2.1.conv3.weight torch.float32\n",
      "layer2.1.bn3.weight torch.float32\n",
      "layer2.1.bn3.bias torch.float32\n",
      "layer2.2.conv1.weight torch.float32\n",
      "layer2.2.bn1.weight torch.float32\n",
      "layer2.2.bn1.bias torch.float32\n",
      "layer2.2.conv2.weight torch.float32\n",
      "layer2.2.bn2.weight torch.float32\n",
      "layer2.2.bn2.bias torch.float32\n",
      "layer2.2.conv3.weight torch.float32\n",
      "layer2.2.bn3.weight torch.float32\n",
      "layer2.2.bn3.bias torch.float32\n",
      "layer2.3.conv1.weight torch.float32\n",
      "layer2.3.bn1.weight torch.float32\n",
      "layer2.3.bn1.bias torch.float32\n",
      "layer2.3.conv2.weight torch.float32\n",
      "layer2.3.bn2.weight torch.float32\n",
      "layer2.3.bn2.bias torch.float32\n",
      "layer2.3.conv3.weight torch.float32\n",
      "layer2.3.bn3.weight torch.float32\n",
      "layer2.3.bn3.bias torch.float32\n",
      "layer3.0.conv1.parametrizations.weight.original torch.float32\n",
      "layer3.0.conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "layer3.0.conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "layer3.0.bn1.weight torch.float32\n",
      "layer3.0.bn1.bias torch.float32\n",
      "layer3.0.conv2.parametrizations.weight.original torch.float32\n",
      "layer3.0.conv2.parametrizations.weight.0.mat_A torch.float32\n",
      "layer3.0.conv2.parametrizations.weight.0.mat_B torch.float32\n",
      "layer3.0.bn2.weight torch.float32\n",
      "layer3.0.bn2.bias torch.float32\n",
      "layer3.0.conv3.parametrizations.weight.original torch.float32\n",
      "layer3.0.conv3.parametrizations.weight.0.mat_A torch.float32\n",
      "layer3.0.conv3.parametrizations.weight.0.mat_B torch.float32\n",
      "layer3.0.bn3.weight torch.float32\n",
      "layer3.0.bn3.bias torch.float32\n",
      "layer3.0.downsample.0.weight torch.float32\n",
      "layer3.0.downsample.1.weight torch.float32\n",
      "layer3.0.downsample.1.bias torch.float32\n",
      "layer3.1.conv1.weight torch.float32\n",
      "layer3.1.bn1.weight torch.float32\n",
      "layer3.1.bn1.bias torch.float32\n",
      "layer3.1.conv2.weight torch.float32\n",
      "layer3.1.bn2.weight torch.float32\n",
      "layer3.1.bn2.bias torch.float32\n",
      "layer3.1.conv3.weight torch.float32\n",
      "layer3.1.bn3.weight torch.float32\n",
      "layer3.1.bn3.bias torch.float32\n",
      "layer3.2.conv1.weight torch.float32\n",
      "layer3.2.bn1.weight torch.float32\n",
      "layer3.2.bn1.bias torch.float32\n",
      "layer3.2.conv2.weight torch.float32\n",
      "layer3.2.bn2.weight torch.float32\n",
      "layer3.2.bn2.bias torch.float32\n",
      "layer3.2.conv3.weight torch.float32\n",
      "layer3.2.bn3.weight torch.float32\n",
      "layer3.2.bn3.bias torch.float32\n",
      "layer3.3.conv1.weight torch.float32\n",
      "layer3.3.bn1.weight torch.float32\n",
      "layer3.3.bn1.bias torch.float32\n",
      "layer3.3.conv2.weight torch.float32\n",
      "layer3.3.bn2.weight torch.float32\n",
      "layer3.3.bn2.bias torch.float32\n",
      "layer3.3.conv3.weight torch.float32\n",
      "layer3.3.bn3.weight torch.float32\n",
      "layer3.3.bn3.bias torch.float32\n",
      "layer3.4.conv1.weight torch.float32\n",
      "layer3.4.bn1.weight torch.float32\n",
      "layer3.4.bn1.bias torch.float32\n",
      "layer3.4.conv2.weight torch.float32\n",
      "layer3.4.bn2.weight torch.float32\n",
      "layer3.4.bn2.bias torch.float32\n",
      "layer3.4.conv3.weight torch.float32\n",
      "layer3.4.bn3.weight torch.float32\n",
      "layer3.4.bn3.bias torch.float32\n",
      "layer3.5.conv1.weight torch.float32\n",
      "layer3.5.bn1.weight torch.float32\n",
      "layer3.5.bn1.bias torch.float32\n",
      "layer3.5.conv2.weight torch.float32\n",
      "layer3.5.bn2.weight torch.float32\n",
      "layer3.5.bn2.bias torch.float32\n",
      "layer3.5.conv3.weight torch.float32\n",
      "layer3.5.bn3.weight torch.float32\n",
      "layer3.5.bn3.bias torch.float32\n",
      "layer4.0.conv1.parametrizations.weight.original torch.float32\n",
      "layer4.0.conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "layer4.0.conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "layer4.0.bn1.weight torch.float32\n",
      "layer4.0.bn1.bias torch.float32\n",
      "layer4.0.conv2.parametrizations.weight.original torch.float32\n",
      "layer4.0.conv2.parametrizations.weight.0.mat_A torch.float32\n",
      "layer4.0.conv2.parametrizations.weight.0.mat_B torch.float32\n",
      "layer4.0.bn2.weight torch.float32\n",
      "layer4.0.bn2.bias torch.float32\n",
      "layer4.0.conv3.parametrizations.weight.original torch.float32\n",
      "layer4.0.conv3.parametrizations.weight.0.mat_A torch.float32\n",
      "layer4.0.conv3.parametrizations.weight.0.mat_B torch.float32\n",
      "layer4.0.bn3.weight torch.float32\n",
      "layer4.0.bn3.bias torch.float32\n",
      "layer4.0.downsample.0.weight torch.float32\n",
      "layer4.0.downsample.1.weight torch.float32\n",
      "layer4.0.downsample.1.bias torch.float32\n",
      "layer4.1.conv1.weight torch.float32\n",
      "layer4.1.bn1.weight torch.float32\n",
      "layer4.1.bn1.bias torch.float32\n",
      "layer4.1.conv2.weight torch.float32\n",
      "layer4.1.bn2.weight torch.float32\n",
      "layer4.1.bn2.bias torch.float32\n",
      "layer4.1.conv3.weight torch.float32\n",
      "layer4.1.bn3.weight torch.float32\n",
      "layer4.1.bn3.bias torch.float32\n",
      "layer4.2.conv1.weight torch.float32\n",
      "layer4.2.bn1.weight torch.float32\n",
      "layer4.2.bn1.bias torch.float32\n",
      "layer4.2.conv2.weight torch.float32\n",
      "layer4.2.bn2.weight torch.float32\n",
      "layer4.2.bn2.bias torch.float32\n",
      "layer4.2.conv3.weight torch.float32\n",
      "layer4.2.bn3.weight torch.float32\n",
      "layer4.2.bn3.bias torch.float32\n",
      "fc.bias torch.float32\n",
      "fc.parametrizations.weight.original torch.float32\n",
      "fc.parametrizations.weight.0.mat_A torch.float32\n",
      "fc.parametrizations.weight.0.mat_B torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3666,  0.7376, -2.0797,  0.0657,  1.8145, -0.0556, -0.6642, -0.1099,\n",
      "         1.6018, -0.8198, -0.9710,  0.6459, -0.1881, -1.1766, -1.0657,  0.2742,\n",
      "        -0.6090,  1.9299, -0.6550,  1.0151, -2.0453, -0.3560,  0.3401,  1.1605,\n",
      "         0.1847, -0.5019,  1.4328, -0.7946, -0.5052, -1.8266, -0.5565, -0.6208,\n",
      "         0.4370,  0.9058, -0.7025, -0.1474,  0.6897, -0.5001,  0.7955, -0.3704,\n",
      "        -1.3695,  1.0997,  0.2136, -0.7356, -1.3617,  1.6388, -0.5650,  0.8238,\n",
      "        -0.7352,  1.4763,  2.7770,  0.4328,  0.3671, -0.2704, -0.7148,  1.1171,\n",
      "         0.1192, -0.8432, -0.2024,  2.1278,  0.3821, -0.3201, -0.7054, -0.5404],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight[0].mat_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight[0].mat_B[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0133,  0.0147, -0.0154, -0.0230, -0.0409, -0.0430, -0.0708],\n",
      "        [ 0.0041,  0.0058,  0.0149,  0.0206,  0.0022, -0.0209, -0.0385],\n",
      "        [ 0.0223,  0.0236,  0.0161,  0.0588,  0.1028,  0.0626,  0.0520],\n",
      "        [ 0.0232,  0.0042, -0.0459, -0.0487, -0.0164,  0.0402,  0.0658],\n",
      "        [-0.0009,  0.0278, -0.0101, -0.0554, -0.1272, -0.0766,  0.0078],\n",
      "        [ 0.0036,  0.0480,  0.0621,  0.0844,  0.0243, -0.0337, -0.0157],\n",
      "        [-0.0800, -0.0322, -0.0178,  0.0342,  0.0354,  0.0224,  0.0017]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight.original[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init x_adv\n",
      "infer\n",
      "kl loss\n",
      "gradient compute\n",
      "other operations\n",
      "infer\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 2.94 GiB of which 4.12 MiB is free. Including non-PyTorch memory, this process has 2.93 GiB memory in use. Of the allocated memory 2.78 GiB is allocated by PyTorch, and 43.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;66;03m# logits = model(data)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;66;03m# loss = criterion(logits,target)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 18\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrades\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrades_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_natural\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
      "File \u001b[0;32m~/Desktop/robust_training/trades.py:117\u001b[0m, in \u001b[0;36mtrades_loss\u001b[0;34m(model, x_natural, y, optimizer, step_size, epsilon, perturb_steps, beta, distance)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m     logits_nat, logits_adv \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_natural\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkl loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m     loss_kl \u001b[38;5;241m=\u001b[39m criterion_kl(F\u001b[38;5;241m.\u001b[39mlog_softmax(logits_adv, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), F\u001b[38;5;241m.\u001b[39msoftmax(logits_nat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/models_local/resnet_imagenet.py:145\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x_natural, x_adv)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_adv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     logits_nat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_impl(x_natural) \u001b[38;5;66;03m#self.model(x_natural)\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     logits_adv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#self.model(x_adv)\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits_nat, logits_adv\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/robust_training/models_local/resnet_imagenet.py:129\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    128\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m--> 129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    132\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 2.94 GiB of which 4.12 MiB is free. Including non-PyTorch memory, this process has 2.93 GiB memory in use. Of the allocated memory 2.78 GiB is allocated by PyTorch, and 43.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import trades\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Assuming 'model' and 'optimizer' are already defined and configured\n",
    "model.train()  # Ensure the model is in training mode\n",
    "scaler = GradScaler()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for data, target in loader:\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "                # logits = model(data)\n",
    "                # loss = criterion(logits,target)\n",
    "    with autocast():\n",
    "        logits, loss = trades.trades_loss(model=model, x_natural=data, y=target, optimizer=optimizer,)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "\n",
    "    # print('loss', loss)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # optimizer.zero_grad()  # Clear previous gradients\n",
    "    # logits_nat = model(data)  # Forward pass\n",
    "\n",
    "    # # Assuming you have a loss function and target labels to compare with\n",
    "    # loss = criterion(logits_nat, target)  # Calculate loss\n",
    "    # loss.backward()  # Backward pass to calculate gradients\n",
    "\n",
    "    # Access gradients of a particular parameter, for example, the first layer's weights\n",
    "    # for name, parameter in model.named_parameters():\n",
    "    #     if parameter.grad is not None:\n",
    "    #         print(f\"Gradient size for {name}: {parameter.grad.size()}\")\n",
    "\n",
    "    # # Proceed with optimizer step if needed\n",
    "    # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "for data, target in loader:\n",
    "    \n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "    optimizer.zero_grad()\n",
    "    logits_nat = model(data)\n",
    "    # loss = criterion(logits,target)\n",
    "    # print('loss', loss)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9575,  0.1928, -0.4407,  0.6611,  1.2258, -0.7848,  0.0152, -0.4324,\n",
      "        -2.3184,  1.0024,  0.9970, -1.7276, -1.5179, -0.9680, -1.0733, -0.5053,\n",
      "         0.0181, -0.5896,  0.4828,  0.1066, -0.6343,  0.0386, -0.3664, -0.9748,\n",
      "         1.0746, -0.9033,  1.0556,  0.7733, -0.9709, -2.2500, -0.0554, -2.2023,\n",
      "        -0.1340,  0.7774, -0.1766, -0.0161,  0.4462, -1.7782,  1.8442,  1.7463,\n",
      "         0.6906,  1.0282, -0.8912,  1.0148, -1.5610, -1.0916, -1.3502,  0.4384,\n",
      "         0.1075, -0.2373,  1.3168,  0.2664,  1.6963,  1.3352, -0.3247, -1.3422,\n",
      "        -0.4286, -0.5899, -0.0792, -0.4842, -2.9525,  0.2606, -0.8432, -1.0796],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight[0].mat_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0030,  0.0023,  0.0011,  0.0021, -0.0024,  0.0011,  0.0032,  0.0028,\n",
      "        -0.0031,  0.0034], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight[0].mat_B[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0002,  0.0204,  0.0120, -0.0117,  0.0048,  0.0408,  0.0248],\n",
      "        [-0.0418, -0.0147, -0.0225,  0.0543, -0.0298,  0.0335,  0.0003],\n",
      "        [-0.0220, -0.0219,  0.0298, -0.0174,  0.0036,  0.0399, -0.0435],\n",
      "        [-0.0197, -0.0251,  0.0499, -0.0140, -0.0510, -0.0500,  0.0119],\n",
      "        [-0.0058, -0.0120,  0.0267,  0.0366, -0.0328,  0.0133, -0.0200],\n",
      "        [ 0.0522,  0.0435, -0.0193,  0.0181,  0.0252,  0.0232,  0.0311],\n",
      "        [ 0.0139,  0.0073, -0.0141, -0.0039,  0.0301, -0.0426,  0.0161]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight.original[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
