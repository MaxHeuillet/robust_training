{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet_cifar10:\n\tMissing key(s) in state_dict: \"conv1.parametrizations.weight.original\", \"conv1.parametrizations.weight.0.mat_A\", \"conv1.parametrizations.weight.0.mat_B\", \"layer1.0.conv1.parametrizations.weight.original\", \"layer1.0.conv1.parametrizations.weight.0.mat_A\", \"layer1.0.conv1.parametrizations.weight.0.mat_B\", \"layer1.0.conv2.parametrizations.weight.original\", \"layer1.0.conv2.parametrizations.weight.0.mat_A\", \"layer1.0.conv2.parametrizations.weight.0.mat_B\", \"layer1.0.conv3.parametrizations.weight.original\", \"layer1.0.conv3.parametrizations.weight.0.mat_A\", \"layer1.0.conv3.parametrizations.weight.0.mat_B\", \"layer2.0.conv1.parametrizations.weight.original\", \"layer2.0.conv1.parametrizations.weight.0.mat_A\", \"layer2.0.conv1.parametrizations.weight.0.mat_B\", \"layer2.0.conv2.parametrizations.weight.original\", \"layer2.0.conv2.parametrizations.weight.0.mat_A\", \"layer2.0.conv2.parametrizations.weight.0.mat_B\", \"layer2.0.conv3.parametrizations.weight.original\", \"layer2.0.conv3.parametrizations.weight.0.mat_A\", \"layer2.0.conv3.parametrizations.weight.0.mat_B\", \"layer3.0.conv1.parametrizations.weight.original\", \"layer3.0.conv1.parametrizations.weight.0.mat_A\", \"layer3.0.conv1.parametrizations.weight.0.mat_B\", \"layer3.0.conv2.parametrizations.weight.original\", \"layer3.0.conv2.parametrizations.weight.0.mat_A\", \"layer3.0.conv2.parametrizations.weight.0.mat_B\", \"layer3.0.conv3.parametrizations.weight.original\", \"layer3.0.conv3.parametrizations.weight.0.mat_A\", \"layer3.0.conv3.parametrizations.weight.0.mat_B\", \"layer4.0.conv1.parametrizations.weight.original\", \"layer4.0.conv1.parametrizations.weight.0.mat_A\", \"layer4.0.conv1.parametrizations.weight.0.mat_B\", \"layer4.0.conv2.parametrizations.weight.original\", \"layer4.0.conv2.parametrizations.weight.0.mat_A\", \"layer4.0.conv2.parametrizations.weight.0.mat_B\", \"layer4.0.conv3.parametrizations.weight.original\", \"layer4.0.conv3.parametrizations.weight.0.mat_A\", \"layer4.0.conv3.parametrizations.weight.0.mat_B\", \"fc.parametrizations.weight.original\", \"fc.parametrizations.weight.0.mat_A\", \"fc.parametrizations.weight.0.mat_B\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.conv2.weight\", \"layer1.0.conv3.weight\", \"layer2.0.conv1.weight\", \"layer2.0.conv2.weight\", \"layer2.0.conv3.weight\", \"layer3.0.conv1.weight\", \"layer3.0.conv2.weight\", \"layer3.0.conv3.weight\", \"layer4.0.conv1.weight\", \"layer4.0.conv2.weight\", \"layer4.0.conv3.weight\", \"fc.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m add_lora(target_layers,model)\n\u001b[1;32m     19\u001b[0m statedict \u001b[38;5;241m=\u001b[39m load_statedict(args)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatedict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet_cifar10:\n\tMissing key(s) in state_dict: \"conv1.parametrizations.weight.original\", \"conv1.parametrizations.weight.0.mat_A\", \"conv1.parametrizations.weight.0.mat_B\", \"layer1.0.conv1.parametrizations.weight.original\", \"layer1.0.conv1.parametrizations.weight.0.mat_A\", \"layer1.0.conv1.parametrizations.weight.0.mat_B\", \"layer1.0.conv2.parametrizations.weight.original\", \"layer1.0.conv2.parametrizations.weight.0.mat_A\", \"layer1.0.conv2.parametrizations.weight.0.mat_B\", \"layer1.0.conv3.parametrizations.weight.original\", \"layer1.0.conv3.parametrizations.weight.0.mat_A\", \"layer1.0.conv3.parametrizations.weight.0.mat_B\", \"layer2.0.conv1.parametrizations.weight.original\", \"layer2.0.conv1.parametrizations.weight.0.mat_A\", \"layer2.0.conv1.parametrizations.weight.0.mat_B\", \"layer2.0.conv2.parametrizations.weight.original\", \"layer2.0.conv2.parametrizations.weight.0.mat_A\", \"layer2.0.conv2.parametrizations.weight.0.mat_B\", \"layer2.0.conv3.parametrizations.weight.original\", \"layer2.0.conv3.parametrizations.weight.0.mat_A\", \"layer2.0.conv3.parametrizations.weight.0.mat_B\", \"layer3.0.conv1.parametrizations.weight.original\", \"layer3.0.conv1.parametrizations.weight.0.mat_A\", \"layer3.0.conv1.parametrizations.weight.0.mat_B\", \"layer3.0.conv2.parametrizations.weight.original\", \"layer3.0.conv2.parametrizations.weight.0.mat_A\", \"layer3.0.conv2.parametrizations.weight.0.mat_B\", \"layer3.0.conv3.parametrizations.weight.original\", \"layer3.0.conv3.parametrizations.weight.0.mat_A\", \"layer3.0.conv3.parametrizations.weight.0.mat_B\", \"layer4.0.conv1.parametrizations.weight.original\", \"layer4.0.conv1.parametrizations.weight.0.mat_A\", \"layer4.0.conv1.parametrizations.weight.0.mat_B\", \"layer4.0.conv2.parametrizations.weight.original\", \"layer4.0.conv2.parametrizations.weight.0.mat_A\", \"layer4.0.conv2.parametrizations.weight.0.mat_B\", \"layer4.0.conv3.parametrizations.weight.original\", \"layer4.0.conv3.parametrizations.weight.0.mat_A\", \"layer4.0.conv3.parametrizations.weight.0.mat_B\", \"fc.parametrizations.weight.original\", \"fc.parametrizations.weight.0.mat_A\", \"fc.parametrizations.weight.0.mat_B\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.conv2.weight\", \"layer1.0.conv3.weight\", \"layer2.0.conv1.weight\", \"layer2.0.conv2.weight\", \"layer2.0.conv3.weight\", \"layer3.0.conv1.weight\", \"layer3.0.conv2.weight\", \"layer3.0.conv3.weight\", \"layer4.0.conv1.weight\", \"layer4.0.conv2.weight\", \"layer4.0.conv3.weight\", \"fc.weight\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from architectures import load_architecture, load_statedict, add_lora\n",
    "\n",
    "from utils import get_args\n",
    "\n",
    "args = get_args()\n",
    "args.arch = 'resnet50'\n",
    "args.dataset = 'CIFAR10'\n",
    "args.selection_method = 'none'\n",
    "args.batch_size = 128\n",
    "\n",
    "model, target_layers = load_architecture(args,)\n",
    "\n",
    "add_lora(target_layers,model)\n",
    "\n",
    "statedict = load_statedict(args)\n",
    "\n",
    "model.load_state_dict(statedict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.parametrizations.weight.original torch.float32\n",
      "conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "bn1.weight torch.float32\n",
      "bn1.bias torch.float32\n",
      "layer1.0.conv1.parametrizations.weight.original torch.float32\n",
      "layer1.0.conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "layer1.0.conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "layer1.0.bn1.weight torch.float32\n",
      "layer1.0.bn1.bias torch.float32\n",
      "layer1.0.conv2.parametrizations.weight.original torch.float32\n",
      "layer1.0.conv2.parametrizations.weight.0.mat_A torch.float32\n",
      "layer1.0.conv2.parametrizations.weight.0.mat_B torch.float32\n",
      "layer1.0.bn2.weight torch.float32\n",
      "layer1.0.bn2.bias torch.float32\n",
      "layer1.0.conv3.parametrizations.weight.original torch.float32\n",
      "layer1.0.conv3.parametrizations.weight.0.mat_A torch.float32\n",
      "layer1.0.conv3.parametrizations.weight.0.mat_B torch.float32\n",
      "layer1.0.bn3.weight torch.float32\n",
      "layer1.0.bn3.bias torch.float32\n",
      "layer1.0.downsample.0.weight torch.float32\n",
      "layer1.0.downsample.1.weight torch.float32\n",
      "layer1.0.downsample.1.bias torch.float32\n",
      "layer1.1.conv1.weight torch.float32\n",
      "layer1.1.bn1.weight torch.float32\n",
      "layer1.1.bn1.bias torch.float32\n",
      "layer1.1.conv2.weight torch.float32\n",
      "layer1.1.bn2.weight torch.float32\n",
      "layer1.1.bn2.bias torch.float32\n",
      "layer1.1.conv3.weight torch.float32\n",
      "layer1.1.bn3.weight torch.float32\n",
      "layer1.1.bn3.bias torch.float32\n",
      "layer1.2.conv1.weight torch.float32\n",
      "layer1.2.bn1.weight torch.float32\n",
      "layer1.2.bn1.bias torch.float32\n",
      "layer1.2.conv2.weight torch.float32\n",
      "layer1.2.bn2.weight torch.float32\n",
      "layer1.2.bn2.bias torch.float32\n",
      "layer1.2.conv3.weight torch.float32\n",
      "layer1.2.bn3.weight torch.float32\n",
      "layer1.2.bn3.bias torch.float32\n",
      "layer2.0.conv1.parametrizations.weight.original torch.float32\n",
      "layer2.0.conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "layer2.0.conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "layer2.0.bn1.weight torch.float32\n",
      "layer2.0.bn1.bias torch.float32\n",
      "layer2.0.conv2.parametrizations.weight.original torch.float32\n",
      "layer2.0.conv2.parametrizations.weight.0.mat_A torch.float32\n",
      "layer2.0.conv2.parametrizations.weight.0.mat_B torch.float32\n",
      "layer2.0.bn2.weight torch.float32\n",
      "layer2.0.bn2.bias torch.float32\n",
      "layer2.0.conv3.parametrizations.weight.original torch.float32\n",
      "layer2.0.conv3.parametrizations.weight.0.mat_A torch.float32\n",
      "layer2.0.conv3.parametrizations.weight.0.mat_B torch.float32\n",
      "layer2.0.bn3.weight torch.float32\n",
      "layer2.0.bn3.bias torch.float32\n",
      "layer2.0.downsample.0.weight torch.float32\n",
      "layer2.0.downsample.1.weight torch.float32\n",
      "layer2.0.downsample.1.bias torch.float32\n",
      "layer2.1.conv1.weight torch.float32\n",
      "layer2.1.bn1.weight torch.float32\n",
      "layer2.1.bn1.bias torch.float32\n",
      "layer2.1.conv2.weight torch.float32\n",
      "layer2.1.bn2.weight torch.float32\n",
      "layer2.1.bn2.bias torch.float32\n",
      "layer2.1.conv3.weight torch.float32\n",
      "layer2.1.bn3.weight torch.float32\n",
      "layer2.1.bn3.bias torch.float32\n",
      "layer2.2.conv1.weight torch.float32\n",
      "layer2.2.bn1.weight torch.float32\n",
      "layer2.2.bn1.bias torch.float32\n",
      "layer2.2.conv2.weight torch.float32\n",
      "layer2.2.bn2.weight torch.float32\n",
      "layer2.2.bn2.bias torch.float32\n",
      "layer2.2.conv3.weight torch.float32\n",
      "layer2.2.bn3.weight torch.float32\n",
      "layer2.2.bn3.bias torch.float32\n",
      "layer2.3.conv1.weight torch.float32\n",
      "layer2.3.bn1.weight torch.float32\n",
      "layer2.3.bn1.bias torch.float32\n",
      "layer2.3.conv2.weight torch.float32\n",
      "layer2.3.bn2.weight torch.float32\n",
      "layer2.3.bn2.bias torch.float32\n",
      "layer2.3.conv3.weight torch.float32\n",
      "layer2.3.bn3.weight torch.float32\n",
      "layer2.3.bn3.bias torch.float32\n",
      "layer3.0.conv1.parametrizations.weight.original torch.float32\n",
      "layer3.0.conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "layer3.0.conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "layer3.0.bn1.weight torch.float32\n",
      "layer3.0.bn1.bias torch.float32\n",
      "layer3.0.conv2.parametrizations.weight.original torch.float32\n",
      "layer3.0.conv2.parametrizations.weight.0.mat_A torch.float32\n",
      "layer3.0.conv2.parametrizations.weight.0.mat_B torch.float32\n",
      "layer3.0.bn2.weight torch.float32\n",
      "layer3.0.bn2.bias torch.float32\n",
      "layer3.0.conv3.parametrizations.weight.original torch.float32\n",
      "layer3.0.conv3.parametrizations.weight.0.mat_A torch.float32\n",
      "layer3.0.conv3.parametrizations.weight.0.mat_B torch.float32\n",
      "layer3.0.bn3.weight torch.float32\n",
      "layer3.0.bn3.bias torch.float32\n",
      "layer3.0.downsample.0.weight torch.float32\n",
      "layer3.0.downsample.1.weight torch.float32\n",
      "layer3.0.downsample.1.bias torch.float32\n",
      "layer3.1.conv1.weight torch.float32\n",
      "layer3.1.bn1.weight torch.float32\n",
      "layer3.1.bn1.bias torch.float32\n",
      "layer3.1.conv2.weight torch.float32\n",
      "layer3.1.bn2.weight torch.float32\n",
      "layer3.1.bn2.bias torch.float32\n",
      "layer3.1.conv3.weight torch.float32\n",
      "layer3.1.bn3.weight torch.float32\n",
      "layer3.1.bn3.bias torch.float32\n",
      "layer3.2.conv1.weight torch.float32\n",
      "layer3.2.bn1.weight torch.float32\n",
      "layer3.2.bn1.bias torch.float32\n",
      "layer3.2.conv2.weight torch.float32\n",
      "layer3.2.bn2.weight torch.float32\n",
      "layer3.2.bn2.bias torch.float32\n",
      "layer3.2.conv3.weight torch.float32\n",
      "layer3.2.bn3.weight torch.float32\n",
      "layer3.2.bn3.bias torch.float32\n",
      "layer3.3.conv1.weight torch.float32\n",
      "layer3.3.bn1.weight torch.float32\n",
      "layer3.3.bn1.bias torch.float32\n",
      "layer3.3.conv2.weight torch.float32\n",
      "layer3.3.bn2.weight torch.float32\n",
      "layer3.3.bn2.bias torch.float32\n",
      "layer3.3.conv3.weight torch.float32\n",
      "layer3.3.bn3.weight torch.float32\n",
      "layer3.3.bn3.bias torch.float32\n",
      "layer3.4.conv1.weight torch.float32\n",
      "layer3.4.bn1.weight torch.float32\n",
      "layer3.4.bn1.bias torch.float32\n",
      "layer3.4.conv2.weight torch.float32\n",
      "layer3.4.bn2.weight torch.float32\n",
      "layer3.4.bn2.bias torch.float32\n",
      "layer3.4.conv3.weight torch.float32\n",
      "layer3.4.bn3.weight torch.float32\n",
      "layer3.4.bn3.bias torch.float32\n",
      "layer3.5.conv1.weight torch.float32\n",
      "layer3.5.bn1.weight torch.float32\n",
      "layer3.5.bn1.bias torch.float32\n",
      "layer3.5.conv2.weight torch.float32\n",
      "layer3.5.bn2.weight torch.float32\n",
      "layer3.5.bn2.bias torch.float32\n",
      "layer3.5.conv3.weight torch.float32\n",
      "layer3.5.bn3.weight torch.float32\n",
      "layer3.5.bn3.bias torch.float32\n",
      "layer4.0.conv1.parametrizations.weight.original torch.float32\n",
      "layer4.0.conv1.parametrizations.weight.0.mat_A torch.float32\n",
      "layer4.0.conv1.parametrizations.weight.0.mat_B torch.float32\n",
      "layer4.0.bn1.weight torch.float32\n",
      "layer4.0.bn1.bias torch.float32\n",
      "layer4.0.conv2.parametrizations.weight.original torch.float32\n",
      "layer4.0.conv2.parametrizations.weight.0.mat_A torch.float32\n",
      "layer4.0.conv2.parametrizations.weight.0.mat_B torch.float32\n",
      "layer4.0.bn2.weight torch.float32\n",
      "layer4.0.bn2.bias torch.float32\n",
      "layer4.0.conv3.parametrizations.weight.original torch.float32\n",
      "layer4.0.conv3.parametrizations.weight.0.mat_A torch.float32\n",
      "layer4.0.conv3.parametrizations.weight.0.mat_B torch.float32\n",
      "layer4.0.bn3.weight torch.float32\n",
      "layer4.0.bn3.bias torch.float32\n",
      "layer4.0.downsample.0.weight torch.float32\n",
      "layer4.0.downsample.1.weight torch.float32\n",
      "layer4.0.downsample.1.bias torch.float32\n",
      "layer4.1.conv1.weight torch.float32\n",
      "layer4.1.bn1.weight torch.float32\n",
      "layer4.1.bn1.bias torch.float32\n",
      "layer4.1.conv2.weight torch.float32\n",
      "layer4.1.bn2.weight torch.float32\n",
      "layer4.1.bn2.bias torch.float32\n",
      "layer4.1.conv3.weight torch.float32\n",
      "layer4.1.bn3.weight torch.float32\n",
      "layer4.1.bn3.bias torch.float32\n",
      "layer4.2.conv1.weight torch.float32\n",
      "layer4.2.bn1.weight torch.float32\n",
      "layer4.2.bn1.bias torch.float32\n",
      "layer4.2.conv2.weight torch.float32\n",
      "layer4.2.bn2.weight torch.float32\n",
      "layer4.2.bn2.bias torch.float32\n",
      "layer4.2.conv3.weight torch.float32\n",
      "layer4.2.bn3.weight torch.float32\n",
      "layer4.2.bn3.bias torch.float32\n",
      "fc.bias torch.float32\n",
      "fc.parametrizations.weight.original torch.float32\n",
      "fc.parametrizations.weight.0.mat_A torch.float32\n",
      "fc.parametrizations.weight.0.mat_B torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3666,  0.7376, -2.0797,  0.0657,  1.8145, -0.0556, -0.6642, -0.1099,\n",
      "         1.6018, -0.8198, -0.9710,  0.6459, -0.1881, -1.1766, -1.0657,  0.2742,\n",
      "        -0.6090,  1.9299, -0.6550,  1.0151, -2.0453, -0.3560,  0.3401,  1.1605,\n",
      "         0.1847, -0.5019,  1.4328, -0.7946, -0.5052, -1.8266, -0.5565, -0.6208,\n",
      "         0.4370,  0.9058, -0.7025, -0.1474,  0.6897, -0.5001,  0.7955, -0.3704,\n",
      "        -1.3695,  1.0997,  0.2136, -0.7356, -1.3617,  1.6388, -0.5650,  0.8238,\n",
      "        -0.7352,  1.4763,  2.7770,  0.4328,  0.3671, -0.2704, -0.7148,  1.1171,\n",
      "         0.1192, -0.8432, -0.2024,  2.1278,  0.3821, -0.3201, -0.7054, -0.5404],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight[0].mat_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight[0].mat_B[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0133,  0.0147, -0.0154, -0.0230, -0.0409, -0.0430, -0.0708],\n",
      "        [ 0.0041,  0.0058,  0.0149,  0.0206,  0.0022, -0.0209, -0.0385],\n",
      "        [ 0.0223,  0.0236,  0.0161,  0.0588,  0.1028,  0.0626,  0.0520],\n",
      "        [ 0.0232,  0.0042, -0.0459, -0.0487, -0.0164,  0.0402,  0.0658],\n",
      "        [-0.0009,  0.0278, -0.0101, -0.0554, -0.1272, -0.0766,  0.0078],\n",
      "        [ 0.0036,  0.0480,  0.0621,  0.0844,  0.0243, -0.0337, -0.0157],\n",
      "        [-0.0800, -0.0322, -0.0178,  0.0342,  0.0354,  0.0224,  0.0017]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight.original[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init x_adv\n",
      " GPU Memory Allocated: 236351488 bytes\n",
      " GPU Memory Cached: 270532608 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1664037376 bytes\n",
      " GPU Memory Cached: 1759510528 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 981042176 bytes\n",
      " GPU Memory Cached: 1761607680 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1699004928 bytes\n",
      " GPU Memory Cached: 2522873856 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 998003712 bytes\n",
      " GPU Memory Cached: 2522873856 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1709664768 bytes\n",
      " GPU Memory Cached: 2524971008 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1012489216 bytes\n",
      " GPU Memory Cached: 2524971008 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1723472384 bytes\n",
      " GPU Memory Cached: 2524971008 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1025817600 bytes\n",
      " GPU Memory Cached: 2524971008 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1738603008 bytes\n",
      " GPU Memory Cached: 2573205504 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1040847872 bytes\n",
      " GPU Memory Cached: 2573205504 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1751677440 bytes\n",
      " GPU Memory Cached: 2573205504 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1053686784 bytes\n",
      " GPU Memory Cached: 2573205504 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1767168512 bytes\n",
      " GPU Memory Cached: 2573205504 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1070117888 bytes\n",
      " GPU Memory Cached: 2573205504 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1778805248 bytes\n",
      " GPU Memory Cached: 2573205504 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1081136128 bytes\n",
      " GPU Memory Cached: 2573205504 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1795643904 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1099473920 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1806391808 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      "init x_adv\n",
      " GPU Memory Allocated: 244902912 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1683844608 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 989251584 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1695589888 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 998686720 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1713094144 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1016868864 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1723201024 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1027336192 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1741428224 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1045803008 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1750382080 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1053231104 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1768971776 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1071921152 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1778705920 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1081790464 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1795657216 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1100161024 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1806444032 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      "init x_adv\n",
      " GPU Memory Allocated: 244902912 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1683844608 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 989251584 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1695589888 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 998686720 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1713094144 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1016868864 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1723201024 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1027336192 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1741428224 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1045803008 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1750382080 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1053231104 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1768971776 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1071921152 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1778705920 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1081790464 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1795657216 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1100161024 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1806444032 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      "init x_adv\n",
      " GPU Memory Allocated: 246542336 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1685484032 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 990891008 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1697229312 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1000326144 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1714733568 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1018508288 bytes\n",
      " GPU Memory Cached: 2621440000 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1724840448 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1028975616 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1743067648 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1047442432 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1752021504 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1054870528 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1770611200 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1073560576 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1780345344 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1083429888 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1797296640 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1101800448 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1808083456 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      "init x_adv\n",
      " GPU Memory Allocated: 246542336 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1685484032 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 990891008 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1697229312 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1000326144 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1714733568 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1018508288 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1724840448 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1028975616 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1743067648 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1047442432 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1752021504 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1054870528 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1770611200 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1073560576 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1780345344 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1083429888 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1797296640 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1101800448 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1808083456 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      "init x_adv\n",
      " GPU Memory Allocated: 246542336 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1685484032 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 990891008 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1697229312 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1000326144 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1714733568 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1018508288 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1724840448 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1028975616 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1743067648 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1047442432 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1752021504 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1054870528 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1770611200 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1073560576 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1780345344 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1083429888 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1797296640 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 1101800448 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1808083456 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      "init x_adv\n",
      " GPU Memory Allocated: 241061888 bytes\n",
      " GPU Memory Cached: 2623537152 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1244006912 bytes\n",
      " GPU Memory Cached: 2640314368 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 763888128 bytes\n",
      " GPU Memory Cached: 2640314368 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1241033216 bytes\n",
      " GPU Memory Cached: 2678063104 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 760857088 bytes\n",
      " GPU Memory Cached: 2678063104 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1262385664 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 782303744 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1259514368 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 778957312 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1280834048 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 800576000 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1278241280 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 798274048 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1298962944 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 817566208 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1299003904 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 818827776 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1315379712 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n",
      " GPU Memory Allocated: 835297792 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "infer\n",
      "kl loss\n",
      " GPU Memory Allocated: 1316846080 bytes\n",
      " GPU Memory Cached: 2680160256 bytes\n",
      "\n",
      "gradient compute\n",
      "other operations\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import trades\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# # Creates a summary writer logging to log_dir\n",
    "# writer = SummaryWriter('runs/your_experiment_name')\n",
    "\n",
    "\n",
    "# Assuming 'model' and 'optimizer' are already defined and configured\n",
    "model.train()  # Ensure the model is in training mode\n",
    "scaler = GradScaler()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for data, target in loader:\n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "                # logits = model(data)\n",
    "                # loss = criterion(logits,target)\n",
    "    with autocast():\n",
    "        logits, loss = trades.trades_loss(model=model, x_natural=data, y=target, optimizer=optimizer,)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "\n",
    "    # print('loss', loss)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # optimizer.zero_grad()  # Clear previous gradients\n",
    "    # logits_nat = model(data)  # Forward pass\n",
    "\n",
    "    # # Assuming you have a loss function and target labels to compare with\n",
    "    # loss = criterion(logits_nat, target)  # Calculate loss\n",
    "    # loss.backward()  # Backward pass to calculate gradients\n",
    "\n",
    "    # Access gradients of a particular parameter, for example, the first layer's weights\n",
    "    # for name, parameter in model.named_parameters():\n",
    "    #     if parameter.grad is not None:\n",
    "    #         print(f\"Gradient size for {name}: {parameter.grad.size()}\")\n",
    "\n",
    "    # # Proceed with optimizer step if needed\n",
    "    # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "for data, target in loader:\n",
    "    \n",
    "    data, target = data.to('cuda'), target.to('cuda')\n",
    "    optimizer.zero_grad()\n",
    "    logits_nat = model(data)\n",
    "    # loss = criterion(logits,target)\n",
    "    # print('loss', loss)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9575,  0.1928, -0.4407,  0.6611,  1.2258, -0.7848,  0.0152, -0.4324,\n",
      "        -2.3184,  1.0024,  0.9970, -1.7276, -1.5179, -0.9680, -1.0733, -0.5053,\n",
      "         0.0181, -0.5896,  0.4828,  0.1066, -0.6343,  0.0386, -0.3664, -0.9748,\n",
      "         1.0746, -0.9033,  1.0556,  0.7733, -0.9709, -2.2500, -0.0554, -2.2023,\n",
      "        -0.1340,  0.7774, -0.1766, -0.0161,  0.4462, -1.7782,  1.8442,  1.7463,\n",
      "         0.6906,  1.0282, -0.8912,  1.0148, -1.5610, -1.0916, -1.3502,  0.4384,\n",
      "         0.1075, -0.2373,  1.3168,  0.2664,  1.6963,  1.3352, -0.3247, -1.3422,\n",
      "        -0.4286, -0.5899, -0.0792, -0.4842, -2.9525,  0.2606, -0.8432, -1.0796],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight[0].mat_A[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0030,  0.0023,  0.0011,  0.0021, -0.0024,  0.0011,  0.0032,  0.0028,\n",
      "        -0.0031,  0.0034], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight[0].mat_B[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0002,  0.0204,  0.0120, -0.0117,  0.0048,  0.0408,  0.0248],\n",
      "        [-0.0418, -0.0147, -0.0225,  0.0543, -0.0298,  0.0335,  0.0003],\n",
      "        [-0.0220, -0.0219,  0.0298, -0.0174,  0.0036,  0.0399, -0.0435],\n",
      "        [-0.0197, -0.0251,  0.0499, -0.0140, -0.0510, -0.0500,  0.0119],\n",
      "        [-0.0058, -0.0120,  0.0267,  0.0366, -0.0328,  0.0133, -0.0200],\n",
      "        [ 0.0522,  0.0435, -0.0193,  0.0181,  0.0252,  0.0232,  0.0311],\n",
      "        [ 0.0139,  0.0073, -0.0141, -0.0039,  0.0301, -0.0426,  0.0161]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.conv1.parametrizations.weight.original[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
