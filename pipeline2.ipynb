{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import models \n",
    "import utils\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import lora \n",
    "# import utils\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.parametrize as parametrize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.resnet\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "# from models import lenet\n",
    "\n",
    "model = models.resnet.resnet50(pretrained=False, progress=True).to('cuda')\n",
    "# model = models.lenet.LeNet().to('cuda')\n",
    "\n",
    "# model = models.resnet.resnet50(pretrained=True, progress=True, device=\"cuda\").to('cuda')\n",
    "\n",
    "# layers = [ model.conv1, model.layer1[0].conv1, model.layer1[0].conv2, model.layer1[0].conv3,\n",
    "#                 model.layer2[0].conv1, model.layer2[0].conv2, model.layer2[0].conv3,\n",
    "#                 model.layer3[0].conv1, model.layer3[0].conv2, model.layer3[0].conv3,\n",
    "#                 model.layer4[0].conv1, model.layer4[0].conv2, model.layer4[0].conv3, model.fc ]\n",
    "\n",
    "# for conv_layer in layers:\n",
    "#     lora_param = lora.layer_parametrization(conv_layer, device=\"cuda\", rank=10, lora_alpha=1)\n",
    "#     parametrize.register_parametrization(conv_layer, 'weight', lora_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter conv1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter bn1.weight\n",
      "Freezing non-LoRA parameter bn1.bias\n",
      "Freezing non-LoRA parameter layer1.0.conv1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer1.0.bn1.weight\n",
      "Freezing non-LoRA parameter layer1.0.bn1.bias\n",
      "Freezing non-LoRA parameter layer1.0.conv2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer1.0.bn2.weight\n",
      "Freezing non-LoRA parameter layer1.0.bn2.bias\n",
      "Freezing non-LoRA parameter layer1.0.conv3.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer1.0.bn3.weight\n",
      "Freezing non-LoRA parameter layer1.0.bn3.bias\n",
      "Freezing non-LoRA parameter layer1.0.downsample.0.weight\n",
      "Freezing non-LoRA parameter layer1.0.downsample.1.weight\n",
      "Freezing non-LoRA parameter layer1.0.downsample.1.bias\n",
      "Freezing non-LoRA parameter layer1.1.conv1.weight\n",
      "Freezing non-LoRA parameter layer1.1.bn1.weight\n",
      "Freezing non-LoRA parameter layer1.1.bn1.bias\n",
      "Freezing non-LoRA parameter layer1.1.conv2.weight\n",
      "Freezing non-LoRA parameter layer1.1.bn2.weight\n",
      "Freezing non-LoRA parameter layer1.1.bn2.bias\n",
      "Freezing non-LoRA parameter layer1.1.conv3.weight\n",
      "Freezing non-LoRA parameter layer1.1.bn3.weight\n",
      "Freezing non-LoRA parameter layer1.1.bn3.bias\n",
      "Freezing non-LoRA parameter layer1.2.conv1.weight\n",
      "Freezing non-LoRA parameter layer1.2.bn1.weight\n",
      "Freezing non-LoRA parameter layer1.2.bn1.bias\n",
      "Freezing non-LoRA parameter layer1.2.conv2.weight\n",
      "Freezing non-LoRA parameter layer1.2.bn2.weight\n",
      "Freezing non-LoRA parameter layer1.2.bn2.bias\n",
      "Freezing non-LoRA parameter layer1.2.conv3.weight\n",
      "Freezing non-LoRA parameter layer1.2.bn3.weight\n",
      "Freezing non-LoRA parameter layer1.2.bn3.bias\n",
      "Freezing non-LoRA parameter layer2.0.conv1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer2.0.bn1.weight\n",
      "Freezing non-LoRA parameter layer2.0.bn1.bias\n",
      "Freezing non-LoRA parameter layer2.0.conv2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer2.0.bn2.weight\n",
      "Freezing non-LoRA parameter layer2.0.bn2.bias\n",
      "Freezing non-LoRA parameter layer2.0.conv3.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer2.0.bn3.weight\n",
      "Freezing non-LoRA parameter layer2.0.bn3.bias\n",
      "Freezing non-LoRA parameter layer2.0.downsample.0.weight\n",
      "Freezing non-LoRA parameter layer2.0.downsample.1.weight\n",
      "Freezing non-LoRA parameter layer2.0.downsample.1.bias\n",
      "Freezing non-LoRA parameter layer2.1.conv1.weight\n",
      "Freezing non-LoRA parameter layer2.1.bn1.weight\n",
      "Freezing non-LoRA parameter layer2.1.bn1.bias\n",
      "Freezing non-LoRA parameter layer2.1.conv2.weight\n",
      "Freezing non-LoRA parameter layer2.1.bn2.weight\n",
      "Freezing non-LoRA parameter layer2.1.bn2.bias\n",
      "Freezing non-LoRA parameter layer2.1.conv3.weight\n",
      "Freezing non-LoRA parameter layer2.1.bn3.weight\n",
      "Freezing non-LoRA parameter layer2.1.bn3.bias\n",
      "Freezing non-LoRA parameter layer2.2.conv1.weight\n",
      "Freezing non-LoRA parameter layer2.2.bn1.weight\n",
      "Freezing non-LoRA parameter layer2.2.bn1.bias\n",
      "Freezing non-LoRA parameter layer2.2.conv2.weight\n",
      "Freezing non-LoRA parameter layer2.2.bn2.weight\n",
      "Freezing non-LoRA parameter layer2.2.bn2.bias\n",
      "Freezing non-LoRA parameter layer2.2.conv3.weight\n",
      "Freezing non-LoRA parameter layer2.2.bn3.weight\n",
      "Freezing non-LoRA parameter layer2.2.bn3.bias\n",
      "Freezing non-LoRA parameter layer2.3.conv1.weight\n",
      "Freezing non-LoRA parameter layer2.3.bn1.weight\n",
      "Freezing non-LoRA parameter layer2.3.bn1.bias\n",
      "Freezing non-LoRA parameter layer2.3.conv2.weight\n",
      "Freezing non-LoRA parameter layer2.3.bn2.weight\n",
      "Freezing non-LoRA parameter layer2.3.bn2.bias\n",
      "Freezing non-LoRA parameter layer2.3.conv3.weight\n",
      "Freezing non-LoRA parameter layer2.3.bn3.weight\n",
      "Freezing non-LoRA parameter layer2.3.bn3.bias\n",
      "Freezing non-LoRA parameter layer3.0.conv1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer3.0.bn1.weight\n",
      "Freezing non-LoRA parameter layer3.0.bn1.bias\n",
      "Freezing non-LoRA parameter layer3.0.conv2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer3.0.bn2.weight\n",
      "Freezing non-LoRA parameter layer3.0.bn2.bias\n",
      "Freezing non-LoRA parameter layer3.0.conv3.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer3.0.bn3.weight\n",
      "Freezing non-LoRA parameter layer3.0.bn3.bias\n",
      "Freezing non-LoRA parameter layer3.0.downsample.0.weight\n",
      "Freezing non-LoRA parameter layer3.0.downsample.1.weight\n",
      "Freezing non-LoRA parameter layer3.0.downsample.1.bias\n",
      "Freezing non-LoRA parameter layer3.1.conv1.weight\n",
      "Freezing non-LoRA parameter layer3.1.bn1.weight\n",
      "Freezing non-LoRA parameter layer3.1.bn1.bias\n",
      "Freezing non-LoRA parameter layer3.1.conv2.weight\n",
      "Freezing non-LoRA parameter layer3.1.bn2.weight\n",
      "Freezing non-LoRA parameter layer3.1.bn2.bias\n",
      "Freezing non-LoRA parameter layer3.1.conv3.weight\n",
      "Freezing non-LoRA parameter layer3.1.bn3.weight\n",
      "Freezing non-LoRA parameter layer3.1.bn3.bias\n",
      "Freezing non-LoRA parameter layer3.2.conv1.weight\n",
      "Freezing non-LoRA parameter layer3.2.bn1.weight\n",
      "Freezing non-LoRA parameter layer3.2.bn1.bias\n",
      "Freezing non-LoRA parameter layer3.2.conv2.weight\n",
      "Freezing non-LoRA parameter layer3.2.bn2.weight\n",
      "Freezing non-LoRA parameter layer3.2.bn2.bias\n",
      "Freezing non-LoRA parameter layer3.2.conv3.weight\n",
      "Freezing non-LoRA parameter layer3.2.bn3.weight\n",
      "Freezing non-LoRA parameter layer3.2.bn3.bias\n",
      "Freezing non-LoRA parameter layer3.3.conv1.weight\n",
      "Freezing non-LoRA parameter layer3.3.bn1.weight\n",
      "Freezing non-LoRA parameter layer3.3.bn1.bias\n",
      "Freezing non-LoRA parameter layer3.3.conv2.weight\n",
      "Freezing non-LoRA parameter layer3.3.bn2.weight\n",
      "Freezing non-LoRA parameter layer3.3.bn2.bias\n",
      "Freezing non-LoRA parameter layer3.3.conv3.weight\n",
      "Freezing non-LoRA parameter layer3.3.bn3.weight\n",
      "Freezing non-LoRA parameter layer3.3.bn3.bias\n",
      "Freezing non-LoRA parameter layer3.4.conv1.weight\n",
      "Freezing non-LoRA parameter layer3.4.bn1.weight\n",
      "Freezing non-LoRA parameter layer3.4.bn1.bias\n",
      "Freezing non-LoRA parameter layer3.4.conv2.weight\n",
      "Freezing non-LoRA parameter layer3.4.bn2.weight\n",
      "Freezing non-LoRA parameter layer3.4.bn2.bias\n",
      "Freezing non-LoRA parameter layer3.4.conv3.weight\n",
      "Freezing non-LoRA parameter layer3.4.bn3.weight\n",
      "Freezing non-LoRA parameter layer3.4.bn3.bias\n",
      "Freezing non-LoRA parameter layer3.5.conv1.weight\n",
      "Freezing non-LoRA parameter layer3.5.bn1.weight\n",
      "Freezing non-LoRA parameter layer3.5.bn1.bias\n",
      "Freezing non-LoRA parameter layer3.5.conv2.weight\n",
      "Freezing non-LoRA parameter layer3.5.bn2.weight\n",
      "Freezing non-LoRA parameter layer3.5.bn2.bias\n",
      "Freezing non-LoRA parameter layer3.5.conv3.weight\n",
      "Freezing non-LoRA parameter layer3.5.bn3.weight\n",
      "Freezing non-LoRA parameter layer3.5.bn3.bias\n",
      "Freezing non-LoRA parameter layer4.0.conv1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer4.0.bn1.weight\n",
      "Freezing non-LoRA parameter layer4.0.bn1.bias\n",
      "Freezing non-LoRA parameter layer4.0.conv2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer4.0.bn2.weight\n",
      "Freezing non-LoRA parameter layer4.0.bn2.bias\n",
      "Freezing non-LoRA parameter layer4.0.conv3.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter layer4.0.bn3.weight\n",
      "Freezing non-LoRA parameter layer4.0.bn3.bias\n",
      "Freezing non-LoRA parameter layer4.0.downsample.0.weight\n",
      "Freezing non-LoRA parameter layer4.0.downsample.1.weight\n",
      "Freezing non-LoRA parameter layer4.0.downsample.1.bias\n",
      "Freezing non-LoRA parameter layer4.1.conv1.weight\n",
      "Freezing non-LoRA parameter layer4.1.bn1.weight\n",
      "Freezing non-LoRA parameter layer4.1.bn1.bias\n",
      "Freezing non-LoRA parameter layer4.1.conv2.weight\n",
      "Freezing non-LoRA parameter layer4.1.bn2.weight\n",
      "Freezing non-LoRA parameter layer4.1.bn2.bias\n",
      "Freezing non-LoRA parameter layer4.1.conv3.weight\n",
      "Freezing non-LoRA parameter layer4.1.bn3.weight\n",
      "Freezing non-LoRA parameter layer4.1.bn3.bias\n",
      "Freezing non-LoRA parameter layer4.2.conv1.weight\n",
      "Freezing non-LoRA parameter layer4.2.bn1.weight\n",
      "Freezing non-LoRA parameter layer4.2.bn1.bias\n",
      "Freezing non-LoRA parameter layer4.2.conv2.weight\n",
      "Freezing non-LoRA parameter layer4.2.bn2.weight\n",
      "Freezing non-LoRA parameter layer4.2.bn2.bias\n",
      "Freezing non-LoRA parameter layer4.2.conv3.weight\n",
      "Freezing non-LoRA parameter layer4.2.bn3.weight\n",
      "Freezing non-LoRA parameter layer4.2.bn3.bias\n",
      "Freezing non-LoRA parameter fc.bias\n",
      "Freezing non-LoRA parameter fc.parametrizations.weight.original\n",
      "Parameter: conv1.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 64])\n",
      "Parameter: conv1.parametrizations.weight.0.mat_B, Shape: torch.Size([27, 10])\n",
      "Parameter: layer1.0.conv1.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 64])\n",
      "Parameter: layer1.0.conv1.parametrizations.weight.0.mat_B, Shape: torch.Size([64, 10])\n",
      "Parameter: layer1.0.conv2.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 64])\n",
      "Parameter: layer1.0.conv2.parametrizations.weight.0.mat_B, Shape: torch.Size([576, 10])\n",
      "Parameter: layer1.0.conv3.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 256])\n",
      "Parameter: layer1.0.conv3.parametrizations.weight.0.mat_B, Shape: torch.Size([64, 10])\n",
      "Parameter: layer2.0.conv1.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 128])\n",
      "Parameter: layer2.0.conv1.parametrizations.weight.0.mat_B, Shape: torch.Size([256, 10])\n",
      "Parameter: layer2.0.conv2.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 128])\n",
      "Parameter: layer2.0.conv2.parametrizations.weight.0.mat_B, Shape: torch.Size([1152, 10])\n",
      "Parameter: layer2.0.conv3.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 512])\n",
      "Parameter: layer2.0.conv3.parametrizations.weight.0.mat_B, Shape: torch.Size([128, 10])\n",
      "Parameter: layer3.0.conv1.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 256])\n",
      "Parameter: layer3.0.conv1.parametrizations.weight.0.mat_B, Shape: torch.Size([512, 10])\n",
      "Parameter: layer3.0.conv2.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 256])\n",
      "Parameter: layer3.0.conv2.parametrizations.weight.0.mat_B, Shape: torch.Size([2304, 10])\n",
      "Parameter: layer3.0.conv3.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 1024])\n",
      "Parameter: layer3.0.conv3.parametrizations.weight.0.mat_B, Shape: torch.Size([256, 10])\n",
      "Parameter: layer4.0.conv1.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 512])\n",
      "Parameter: layer4.0.conv1.parametrizations.weight.0.mat_B, Shape: torch.Size([1024, 10])\n",
      "Parameter: layer4.0.conv2.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 512])\n",
      "Parameter: layer4.0.conv2.parametrizations.weight.0.mat_B, Shape: torch.Size([4608, 10])\n",
      "Parameter: layer4.0.conv3.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 2048])\n",
      "Parameter: layer4.0.conv3.parametrizations.weight.0.mat_B, Shape: torch.Size([512, 10])\n",
      "Parameter: fc.parametrizations.weight.0.mat_A, Shape: torch.Size([10, 2048])\n",
      "Parameter: fc.parametrizations.weight.0.mat_B, Shape: torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'mat' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "for layer in layers:\n",
    "  layer.parametrizations[\"weight\"][0].requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "   if param.requires_grad:\n",
    "        print(f\"Parameter: {name}, Shape: {param.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "iteration 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i) )\n\u001b[1;32m     28\u001b[0m pool_loader \u001b[38;5;241m=\u001b[39m DataLoader( Subset(pool_dataset, pool_indices), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m query_indices \u001b[38;5;241m=\u001b[39m \u001b[43mactive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muncertainty_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactive_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m global_query_indices \u001b[38;5;241m=\u001b[39m [ pool_indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m query_indices]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m global_query_indices:\n",
      "File \u001b[0;32m~/Desktop/robust_training/active.py:92\u001b[0m, in \u001b[0;36muncertainty_sampling\u001b[0;34m(model, loader, n_instances)\u001b[0m\n\u001b[1;32m     89\u001b[0m uncertainties \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, _ \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     93\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     94\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax( model(images), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:173\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    171\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "import torch.optim as optim\n",
    "import active\n",
    "import trades\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mean_cifar10 = (0.4914, 0.4822, 0.4465)\n",
    "std_cifar10 = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10, std_cifar10)  ])\n",
    "\n",
    "pool_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "pool_loader = DataLoader(pool_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "adapt_dataset = active.EmptyDataset()\n",
    "\n",
    "pool_indices = list( range(len(pool_loader.dataset ) ) ) \n",
    "n_queries = 1\n",
    "active_batch_size = 10\n",
    "\n",
    "for i in range(n_queries):\n",
    "\n",
    "    print( 'iteration {}'.format(i) )\n",
    "    \n",
    "    pool_loader = DataLoader( Subset(pool_dataset, pool_indices), batch_size=128, shuffle=False)\n",
    "    query_indices = active.uncertainty_sampling(model, pool_loader, active_batch_size)\n",
    "\n",
    "    global_query_indices = [ pool_indices[idx] for idx in query_indices]\n",
    "\n",
    "    for idx in global_query_indices:\n",
    "        image, label = pool_dataset[idx]\n",
    "        adapt_dataset.add_data( [image], [label] )\n",
    "\n",
    "    # Remove queried instances from the pool\n",
    "    pool_indices = [idx for idx in pool_indices if idx not in global_query_indices]\n",
    "    print( len(pool_indices) )\n",
    "\n",
    "    ################# Update the ResNet through low rank matrices:\n",
    "    print('start training process')\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    adapt_loader = DataLoader(adapt_dataset, batch_size=128, shuffle=True)\n",
    "    for _ in range(10):\n",
    "        print('epoch {}'.format(_) )\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(adapt_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # loss = trades.trades_loss(model=model, x_natural=data, y=target, optimizer=optimizer,)\n",
    "            loss = nn.CrossEntropyLoss()( model(data) ,target)\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([50])\n",
      "torch.Size([50]) torch.Size([50]) torch.Size([2, 50])\n",
      "torch.Size([50])\n",
      "torch.Size([50]) torch.Size([50]) torch.Size([2, 50])\n",
      "torch.Size([50])\n",
      "torch.Size([50]) torch.Size([50]) torch.Size([2, 50])\n",
      "torch.Size([50])\n",
      "torch.Size([50]) torch.Size([50]) torch.Size([2, 50])\n",
      "torch.Size([50])\n",
      "torch.Size([50]) torch.Size([50]) torch.Size([2, 50])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import models \n",
    "import utils\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch.nn.utils.parametrize as parametrize\n",
    "import lora \n",
    "\n",
    "from torch.utils.data import Subset\n",
    "import trades\n",
    "import active\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import pickle as pkl\n",
    "import gzip\n",
    "\n",
    "import random \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mean_cifar10 = (0.4914, 0.4822, 0.4465)\n",
    "std_cifar10 = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10, std_cifar10)  ])\n",
    "\n",
    "\n",
    "# Uncertainty sampling function\n",
    "# def uncertainty_sampling(model, loader, n_instances=10):\n",
    "#     device = 'cuda'\n",
    "#     model.eval()\n",
    "#     all_indices = list( range(len(loader.dataset)) )\n",
    "#     uncertainties = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, _ in loader:\n",
    "#             images = images.to(device)\n",
    "#             outputs = torch.softmax( model(images), dim=1)\n",
    "#             uncertainty = 1 - torch.max(outputs, dim=1)[0]\n",
    "#             uncertainties.extend( uncertainty.tolist() )\n",
    "    \n",
    "#     # Select the indices of the top uncertain instances\n",
    "#     uncertainty_indices = np.argsort(uncertainties)[-n_instances:]\n",
    "\n",
    "#     return [all_indices[i] for i in uncertainty_indices]\n",
    "\n",
    "def attack_uncertainty_sampling(model, loader, n_instances=10):\n",
    "    device = 'cuda'\n",
    "    \n",
    "    result = []\n",
    "    epsilon = 8/255\n",
    "    n_restarts = 2\n",
    "    attack_iters = 10\n",
    "    alpha = epsilon/4\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        model.train()\n",
    "        print(labels.shape)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        delta = utils.attack_pgd_restart(model, images, labels, epsilon, alpha, attack_iters, n_restarts, rs=True, verbose=False,\n",
    "                    linf_proj=True, l2_proj=False, l2_grad_update=False, cuda=True)\n",
    "        \n",
    "        x_adv = images + delta\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        outputs = torch.softmax( model(images), dim=1)\n",
    "        uncertainties = 1 - torch.max(outputs, dim=1)[0]\n",
    "\n",
    "        outputs = model( x_adv )\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)\n",
    "        attack_success = predicted_labels != labels\n",
    "\n",
    "        merge = torch.vstack( [uncertainties, attack_success] )\n",
    "        print(uncertainties.shape, attack_success.shape, merge.shape)\n",
    "        \n",
    "        result.append(merge)\n",
    "\n",
    "    result = torch.hstack(result).T\n",
    "    result = result[ result[:, 1] == 1]\n",
    "    val, idx = result[:, 0].sort(descending=True)\n",
    "    indices = idx[:n_instances]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "n_rounds = 1\n",
    "\n",
    "pool_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "pool_loader = DataLoader(pool_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "adapt_dataset = active.EmptyDataset()\n",
    "pool_indices = list( range(250) ) #range(len(pool_loader.dataset ) ) \n",
    "\n",
    "epoch_counter = 0\n",
    "\n",
    "for i in range(n_rounds):\n",
    "\n",
    "    pool_loader = DataLoader( Subset(pool_dataset, pool_indices), batch_size=50, shuffle=False)\n",
    "\n",
    "    query_indices = attack_uncertainty_sampling( model, pool_loader, None)\n",
    "    \n",
    "    # _ = utils.add_data(query_indices, pool_indices, pool_dataset, adapt_dataset)\n",
    "\n",
    "    # adapt_loader = DataLoader(adapt_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([109, 122,  24,  17,  73,  40, 106, 129, 121,  11,  90, 125,  78,  53,\n",
       "         64,  23, 123,  54,  85, 105,  39,  29,  42, 120,  55,   8,  45,  63,\n",
       "        126,  34, 116, 111,  87,  76,  75,  25,  94,  43,   9,  37, 119,  79,\n",
       "         58,  44,  52,  98,  67,  68,  56,  57,  60,  89,  62,  22,  69,   4,\n",
       "         48,  92,  31,  66,  95,  13,  28,  12,  49,  27,  26,  51,  30,  21,\n",
       "        118,  16,  47,  65,  71,   0, 100, 107, 117,  10,  20, 115, 102,  83,\n",
       "        101,  97,   2,  91,  32,  59,  82,  38,  46,  33, 128,  19,  50,   7,\n",
       "         74,  18,  84,   6,  86,  77,  35,  15, 108,  61,  72,  36,   1, 124,\n",
       "         81, 112,  96,   5, 103,  14,  93,  88, 104, 113,  99,  41,  80, 110,\n",
       "         70, 114,   3, 127], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
