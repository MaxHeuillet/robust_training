{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_uc-merced-land-use-dataset_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coat_tiny.in1k_stanford_cars_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coat_tiny.in1k_caltech101_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coat_tiny.in1k_fgvc-aircraft-2013b_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coat_tiny.in1k_flowers-102_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coat_tiny.in1k_oxford-iiit-pet_TRADES_v2.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_uc-merced-land-use-dataset_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_stanford_cars_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coat_tiny.in1k_caltech101_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_fgvc-aircraft-2013b_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_flowers-102_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/CLIP-convnext_base_w-laion2B-s13B-b82K_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/deit_small_patch16_224.fb_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_resnet50_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/resnet50.a1_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_vit_base_patch16_224_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.mae_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.dino_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_base_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_convnext_tiny_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/robust_deit_small_patch16_224_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_clip_224.laion2b_ft_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_base_patch16_224.augreg_in21k_ft_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/vit_small_patch16_224.augreg_in21k_ft_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_base_patch14_224.mim_in22k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/eva02_tiny_patch14_224.mim_in22k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_base_patch4_window7_224.ms_in22k_ft_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/swin_tiny_patch4_window7_224.ms_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.clip_laion2b_augreg_ft_in12k_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_base.fb_in22k_ft_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/convnext_tiny.fb_in22k_ft_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_0_rw_224.sw_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_ft_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_paper_final2/coatnet_2_rw_224.sw_in12k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/regnetx_004.pycls_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/efficientnet-b0_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/deit_tiny_patch16_224.fb_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilevit-small_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/mobilenetv3_large_100.ra_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/edgenext_small.usi_in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n",
      "../results/full_fine_tuning_50epochs_edge_paper_final2/coat_tiny.in1k_oxford-iiit-pet_CLASSIC_AT.pkl\n"
     ]
    }
   ],
   "source": [
    "from load_results import load_result_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pn1 = 'full_fine_tuning_50epochs_edge_paper_final2'\n",
    "pn2 = 'full_fine_tuning_50epochs_paper_final2'\n",
    "\n",
    "final_data = load_result_dataset(pn1, pn2)\n",
    "final_data = [{**d, 'ft_strategy': 'FFT (50 epochs)'} for d in final_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaN values, for small architecture: 3.125\n",
      "Percentage of NaN values, for medium architecture: 0.0\n",
      "Percentage of NaN values, for large architecture: 3.3854166666666665\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(final_data)\n",
    "for arch_size in ('small', 'medium', 'large'):\n",
    "    df_curr = df[ df.model_size == arch_size ]\n",
    "    nan_percentage = (df_curr.isna().sum().sum() / df_curr.size) * 100\n",
    "    print(\"Percentage of NaN values, for {} architecture: {}\".format(arch_size, nan_percentage) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backbone</th>\n",
       "      <th>dataset</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>restart_from</th>\n",
       "      <th>model_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>coatnet_2_rw_224.sw_in12k_ft_in1k</td>\n",
       "      <td>stanford_cars</td>\n",
       "      <td>TRADES_v2</td>\n",
       "      <td>job1_hpo.sh</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>coatnet_2_rw_224.sw_in12k</td>\n",
       "      <td>stanford_cars</td>\n",
       "      <td>TRADES_v2</td>\n",
       "      <td>job1_hpo.sh</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>coat_tiny.in1k</td>\n",
       "      <td>stanford_cars</td>\n",
       "      <td>TRADES_v2</td>\n",
       "      <td>job1_hpo.sh</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>coat_tiny.in1k</td>\n",
       "      <td>caltech101</td>\n",
       "      <td>TRADES_v2</td>\n",
       "      <td>job1_hpo.sh</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              backbone        dataset loss_function  \\\n",
       "71   coatnet_2_rw_224.sw_in12k_ft_in1k  stanford_cars     TRADES_v2   \n",
       "72           coatnet_2_rw_224.sw_in12k  stanford_cars     TRADES_v2   \n",
       "79                      coat_tiny.in1k  stanford_cars     TRADES_v2   \n",
       "119                     coat_tiny.in1k     caltech101     TRADES_v2   \n",
       "\n",
       "    restart_from model_size  \n",
       "71   job1_hpo.sh      large  \n",
       "72   job1_hpo.sh      large  \n",
       "79   job1_hpo.sh      small  \n",
       "119  job1_hpo.sh      small  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "df = pd.DataFrame(final_data)\n",
    "# df = df[df[\"model_size\"].isin([  ])] # 'medium', 'large'\n",
    "# df.to_csv(\"./{}.csv\".format(\"result_with_missing_values_max\"))\n",
    "\n",
    "def get_restart_job(row):\n",
    "    is_missing = lambda col: pd.isna(row[col])\n",
    "\n",
    "    missing = {col: is_missing(col) for col in ['clean_acc', 'Linf_acc', 'L1_acc', 'L2_acc', 'common_acc']}\n",
    "\n",
    "    if all(missing.values()):\n",
    "        return \"job1_hpo.sh\"\n",
    "    elif not missing['clean_acc'] and not missing['Linf_acc'] and missing['L1_acc']:\n",
    "        return \"job4_test_l1.sh\"\n",
    "    elif not missing['clean_acc'] and not missing['Linf_acc'] and not missing['L1_acc'] and missing['L2_acc']:\n",
    "        return \"job5_test_l2.sh\"\n",
    "    elif not missing['clean_acc'] and not missing['Linf_acc'] and not missing['L1_acc'] and not missing['L2_acc'] and missing['common_acc']:\n",
    "        return \"job6_test_common.sh\"\n",
    "    else:\n",
    "        return None  # means no job needs to be restarted\n",
    "\n",
    "\n",
    "df['restart_from'] = df.apply(get_restart_job, axis=1)\n",
    "\n",
    "to_restart = df[df['restart_from'].notna()][['backbone', 'dataset', 'loss_function', 'restart_from', 'model_size']]\n",
    "to_restart = to_restart[ to_restart.restart_from == \"job1_hpo.sh\" ]\n",
    "to_restart.to_csv(\"./to_relaunch_rishika.csv\")\n",
    "to_restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaN values: 2.07%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"8\" halign=\"left\">TOTAL</th>\n",
       "      <th colspan=\"2\" halign=\"left\">caltech101</th>\n",
       "      <th>...</th>\n",
       "      <th>stanford_cars</th>\n",
       "      <th colspan=\"9\" halign=\"left\">uc-merced-land-use-dataset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>borda</th>\n",
       "      <th>nan_geom_cnt</th>\n",
       "      <th>nan_sum_cnt</th>\n",
       "      <th>rank_borda</th>\n",
       "      <th>rank_geom</th>\n",
       "      <th>rank_sum</th>\n",
       "      <th>score_geom</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>L1_acc</th>\n",
       "      <th>L2_acc</th>\n",
       "      <th>...</th>\n",
       "      <th>volume_pre_training_data</th>\n",
       "      <th>L1_acc</th>\n",
       "      <th>L2_acc</th>\n",
       "      <th>Linf_acc</th>\n",
       "      <th>borda</th>\n",
       "      <th>clean_acc</th>\n",
       "      <th>common_acc</th>\n",
       "      <th>geom</th>\n",
       "      <th>sum</th>\n",
       "      <th>volume_pre_training_data</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backbone</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>pre_training_strategy</th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_size</th>\n",
       "      <th>ft_strategy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>convnext_base.fb_in22k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>630.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.464580</td>\n",
       "      <td>18.114302</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>...</td>\n",
       "      <td>14197122</td>\n",
       "      <td>0.823810</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.863492</td>\n",
       "      <td>4.951017e-01</td>\n",
       "      <td>4.353968</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLIP-convnext_base_w-laion2B-s13B-b82K</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>self-supervised (multimodal)</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>590.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.118427</td>\n",
       "      <td>17.824968</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>...</td>\n",
       "      <td>2320000000</td>\n",
       "      <td>0.749206</td>\n",
       "      <td>0.888095</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.847619</td>\n",
       "      <td>4.296971e-01</td>\n",
       "      <td>4.237302</td>\n",
       "      <td>2320000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLIP-convnext_base_w-laion_aesthetic-s13B-b82K</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>self-supervised (multimodal)</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>669.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.729894</td>\n",
       "      <td>17.385889</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>...</td>\n",
       "      <td>900000000</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.193651</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>3.917079e-02</td>\n",
       "      <td>2.972222</td>\n",
       "      <td>900000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swin_base_patch4_window7_224.ms_in22k_ft_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>639.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.118414</td>\n",
       "      <td>17.202905</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>...</td>\n",
       "      <td>15478289</td>\n",
       "      <td>0.854762</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.721429</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.954762</td>\n",
       "      <td>0.838095</td>\n",
       "      <td>4.323401e-01</td>\n",
       "      <td>4.245238</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eva02_base_patch14_224.mim_in22k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>self-supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>640.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.735869</td>\n",
       "      <td>16.743524</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>...</td>\n",
       "      <td>14197122</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>3.576140e-02</td>\n",
       "      <td>3.426190</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robust_convnext_base</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised (robust)</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>627.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.005985</td>\n",
       "      <td>15.721302</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.861905</td>\n",
       "      <td>4.044271e-01</td>\n",
       "      <td>4.203968</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convnext_base.fb_in22k_ft_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>498.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.955705</td>\n",
       "      <td>15.341397</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>...</td>\n",
       "      <td>15478289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.806349</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.758730</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_clip_224.laion2b_ft_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>529.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.642220</td>\n",
       "      <td>15.240730</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>...</td>\n",
       "      <td>2321281167</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>2.575628e-01</td>\n",
       "      <td>3.858730</td>\n",
       "      <td>2321281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convnext_base.fb_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>506.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.739440</td>\n",
       "      <td>14.841429</td>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.102381</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.921429</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convnext_base.clip_laion2b_augreg_ft_in12k_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>523.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.651426</td>\n",
       "      <td>13.797873</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>...</td>\n",
       "      <td>2330281167</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.415873</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.569048</td>\n",
       "      <td>0.421429</td>\n",
       "      <td>1.147706e-02</td>\n",
       "      <td>2.099206</td>\n",
       "      <td>2330281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_clip_224.laion2b</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>self-supervised (multimodal)</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>492.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.428039</td>\n",
       "      <td>12.831397</td>\n",
       "      <td>0.757333</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>...</td>\n",
       "      <td>2320000000</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.088095</td>\n",
       "      <td>0.082540</td>\n",
       "      <td>2.679353e-07</td>\n",
       "      <td>0.275397</td>\n",
       "      <td>2320000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swin_base_patch4_window7_224.ms_in22k_ft_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>552.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.533941</td>\n",
       "      <td>12.544365</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15478289</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.879365</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.802381</td>\n",
       "      <td>4.091918e-01</td>\n",
       "      <td>4.196032</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">vit_base_patch16_224.augreg_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>394.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.526426</td>\n",
       "      <td>12.497778</td>\n",
       "      <td>0.717333</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.680952</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.634921</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>2.562525e-01</td>\n",
       "      <td>3.844444</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>325.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.530873</td>\n",
       "      <td>11.306857</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.692857</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>2.827642e-01</td>\n",
       "      <td>3.909524</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLIP-convnext_base_w-laion2B-s13B-b82K</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>self-supervised (multimodal)</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>508.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.395996</td>\n",
       "      <td>11.303238</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2320000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.945238</td>\n",
       "      <td>0.795238</td>\n",
       "      <td>3.368357e-01</td>\n",
       "      <td>4.045238</td>\n",
       "      <td>2320000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_224.mae</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>self-supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>363.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.053784</td>\n",
       "      <td>10.922762</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.645238</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.788095</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convnext_base.fb_in22k_ft_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>543.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.510607</td>\n",
       "      <td>10.703683</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15478289</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.892063</td>\n",
       "      <td>0.788095</td>\n",
       "      <td>169.0</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>4.758292e-01</td>\n",
       "      <td>4.323016</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_224.mae</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>self-supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>305.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.325365</td>\n",
       "      <td>10.652857</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>1.127445e-01</td>\n",
       "      <td>3.376190</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLIP-convnext_base_w-laion_aesthetic-s13B-b82K</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>self-supervised (multimodal)</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>446.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.405468</td>\n",
       "      <td>10.542286</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>900000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.759524</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3.622695e-01</td>\n",
       "      <td>4.097619</td>\n",
       "      <td>900000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robust_convnext_base</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised (robust)</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>489.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.076615</td>\n",
       "      <td>9.427698</td>\n",
       "      <td>0.736000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>0.121429</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.164286</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>4.296681e-05</td>\n",
       "      <td>0.679365</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robust_vit_base_patch16_224</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised (robust)</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>340.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.438403</td>\n",
       "      <td>9.284619</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.180952</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_224.augreg_in21k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>259.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.314362</td>\n",
       "      <td>9.204524</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>...</td>\n",
       "      <td>14197122</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.447619</td>\n",
       "      <td>4.640042e-04</td>\n",
       "      <td>1.609524</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eva02_base_patch14_224.mim_in22k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>self-supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>398.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.435495</td>\n",
       "      <td>9.000381</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14197122</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.869841</td>\n",
       "      <td>0.759524</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.796825</td>\n",
       "      <td>3.715125e-01</td>\n",
       "      <td>4.119048</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coatnet_2_rw_224.sw_in12k_ft_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>294.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.667381</td>\n",
       "      <td>8.783238</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10281167</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>0.802381</td>\n",
       "      <td>167.0</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>4.898830e-01</td>\n",
       "      <td>4.345238</td>\n",
       "      <td>10281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_224.augreg_in21k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>228.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.268741</td>\n",
       "      <td>8.744079</td>\n",
       "      <td>0.658667</td>\n",
       "      <td>0.709333</td>\n",
       "      <td>...</td>\n",
       "      <td>14197122</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.669841</td>\n",
       "      <td>0.530952</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>7.852726e-02</td>\n",
       "      <td>3.115079</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_clip_224.laion2b</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>self-supervised (multimodal)</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>215.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.289556</td>\n",
       "      <td>8.554016</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>...</td>\n",
       "      <td>2320000000</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.680952</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.707937</td>\n",
       "      <td>1.647349e-01</td>\n",
       "      <td>3.539683</td>\n",
       "      <td>2320000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convnext_base.fb_in22k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>473.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.048231</td>\n",
       "      <td>7.811889</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14197122</td>\n",
       "      <td>0.159524</td>\n",
       "      <td>0.136508</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.188095</td>\n",
       "      <td>7.690471e-05</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_224.augreg_in21k_ft_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>225.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.304043</td>\n",
       "      <td>7.766048</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>...</td>\n",
       "      <td>15478289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.092857</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.135714</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robust_vit_base_patch16_224</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised (robust)</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>311.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.475919</td>\n",
       "      <td>7.738222</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.174603</td>\n",
       "      <td>0.121429</td>\n",
       "      <td>1.236257e-07</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_224.augreg_in21k_ft_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>231.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.282617</td>\n",
       "      <td>7.513095</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>...</td>\n",
       "      <td>15478289</td>\n",
       "      <td>0.061905</td>\n",
       "      <td>0.097619</td>\n",
       "      <td>0.140476</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>2.879747e-05</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convnext_base.fb_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>389.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>7.086190</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1281167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.378571</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.859524</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">coatnet_2_rw_224.sw_in12k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>149.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.615903</td>\n",
       "      <td>6.933762</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9000000</td>\n",
       "      <td>0.792857</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>149.0</td>\n",
       "      <td>0.959524</td>\n",
       "      <td>0.861905</td>\n",
       "      <td>4.207975e-01</td>\n",
       "      <td>4.221429</td>\n",
       "      <td>9000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>254.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.155520</td>\n",
       "      <td>6.701143</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9000000</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.695238</td>\n",
       "      <td>1.513687e-01</td>\n",
       "      <td>3.457143</td>\n",
       "      <td>9000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vit_base_patch16_clip_224.laion2b_ft_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>fully attention</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>157.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.053082</td>\n",
       "      <td>5.820127</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>...</td>\n",
       "      <td>2321281167</td>\n",
       "      <td>0.230952</td>\n",
       "      <td>0.152381</td>\n",
       "      <td>0.098413</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.482540</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>6.737992e-04</td>\n",
       "      <td>1.367460</td>\n",
       "      <td>2321281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coatnet_2_rw_224.sw_in12k_ft_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>200.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>5.698333</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10281167</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>2.722446e-01</td>\n",
       "      <td>3.883333</td>\n",
       "      <td>10281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convnext_base.clip_laion2b_augreg_ft_in12k_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>hybrid</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <th>large</th>\n",
       "      <th>FFT (50 epochs)</th>\n",
       "      <td>275.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.032844</td>\n",
       "      <td>5.644952</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2330281167</td>\n",
       "      <td>0.419048</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>3.092505e-02</td>\n",
       "      <td>2.530952</td>\n",
       "      <td>2330281167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                                                                                                                                    TOTAL  \\\n",
       "metric                                                                                                                                     borda   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy              \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  630.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  590.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  669.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  639.0   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  640.0   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  627.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  498.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  529.0   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  506.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  523.0   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  492.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  552.0   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  394.0   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  325.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  508.0   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  363.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  543.0   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  305.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  446.0   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)  489.0   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)  340.0   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  259.0   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  398.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  294.0   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  228.0   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)  215.0   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  473.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  225.0   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)  311.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  231.0   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  389.0   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  149.0   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  254.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)  157.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  200.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)  275.0   \n",
       "\n",
       "dataset                                                                                                                                                 \\\n",
       "metric                                                                                                                                    nan_geom_cnt   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                    \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)            0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)            0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)            0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)            0   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)            0   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)            0   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)            0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)            0   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)            0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)            0   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)            0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)            1   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)            0   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)            0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)            1   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)            0   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)            2   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)            0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)            1   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)            1   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)            0   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)            0   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)            1   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)            3   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)            0   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)            0   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)            2   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)            0   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)            0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)            0   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)            2   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)            4   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)            2   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)            0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)            3   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)            1   \n",
       "\n",
       "dataset                                                                                                                                                \\\n",
       "metric                                                                                                                                    nan_sum_cnt   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                   \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)           0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)           0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)           0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)           0   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)           0   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)           0   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)           0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)           0   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)           0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)           0   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)           0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)           1   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)           0   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)           0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)           1   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)           0   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)           2   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)           0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)           1   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)           1   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)           0   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)           0   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)           1   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)           3   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)           0   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)           0   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)           2   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)           0   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)           0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)           0   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)           2   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)           4   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)           2   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)           0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)           3   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)           1   \n",
       "\n",
       "dataset                                                                                                                                               \\\n",
       "metric                                                                                                                                    rank_borda   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                  \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)        4.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)        6.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)        1.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)        3.0   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)        2.0   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)        5.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)       13.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)        9.0   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)       12.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)       10.0   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)       14.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)        7.0   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)       19.0   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)       23.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)       11.0   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)       21.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)        8.0   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)       25.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)       17.0   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)       15.0   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)       22.0   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)       28.0   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)       18.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)       26.0   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)       31.0   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)       33.0   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)       16.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)       32.0   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)       24.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)       30.0   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)       20.0   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)       36.0   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)       29.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)       35.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)       34.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)       27.0   \n",
       "\n",
       "dataset                                                                                                                                              \\\n",
       "metric                                                                                                                                    rank_geom   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                 \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)       1.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)       2.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)       8.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)       3.0   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)       7.0   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)       4.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)       5.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)      11.0   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)       6.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)      10.0   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)      20.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)      13.0   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)      15.0   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)      14.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)      22.0   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)      32.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)      16.0   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)      23.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)      21.0   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)      31.0   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)      18.0   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)      24.0   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)      19.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)       9.0   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)      29.0   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)      26.0   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)      34.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)      25.0   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)      17.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)      27.0   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)      35.0   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)      12.0   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)      30.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)      33.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)      28.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)      36.0   \n",
       "\n",
       "dataset                                                                                                                                             \\\n",
       "metric                                                                                                                                    rank_sum   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)      1.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)      2.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)      3.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)      4.0   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)      5.0   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)      6.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)      7.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)      8.0   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)      9.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)     10.0   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)     11.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)     12.0   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)     13.0   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)     14.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)     15.0   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)     16.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)     17.0   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)     18.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)     19.0   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)     20.0   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)     21.0   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)     22.0   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)     23.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)     24.0   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)     25.0   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)     26.0   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)     27.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)     28.0   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)     29.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)     30.0   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)     31.0   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)     32.0   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)     33.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)     34.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)     35.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)     36.0   \n",
       "\n",
       "dataset                                                                                                                                               \\\n",
       "metric                                                                                                                                    score_geom   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                  \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   1.464580   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   1.118427   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.729894   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   1.118414   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)   0.735869   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)   1.005985   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   0.955705   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)   0.642220   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   0.739440   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)   0.651426   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)   0.428039   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.533941   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.526426   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.530873   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.395996   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)   0.053784   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.510607   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)   0.325365   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.405468   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)   0.076615   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)   0.438403   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.314362   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)   0.435495   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)   0.667381   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.268741   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)   0.289556   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.048231   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.304043   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)   0.475919   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.282617   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.035156   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)   0.615903   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   0.155520   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)   0.053082   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   0.272500   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)   0.032844   \n",
       "\n",
       "dataset                                                                                                                                               \\\n",
       "metric                                                                                                                                     score_sum   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                  \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  18.114302   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  17.824968   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  17.385889   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  17.202905   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  16.743524   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  15.721302   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  15.341397   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  15.240730   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  14.841429   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  13.797873   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  12.831397   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  12.544365   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  12.497778   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  11.306857   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  11.303238   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  10.922762   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  10.703683   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  10.652857   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  10.542286   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)   9.427698   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)   9.284619   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   9.204524   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)   9.000381   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)   8.783238   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   8.744079   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)   8.554016   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   7.811889   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   7.766048   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)   7.738222   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   7.513095   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   7.086190   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)   6.933762   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   6.701143   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)   5.820127   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   5.698333   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)   5.644952   \n",
       "\n",
       "dataset                                                                                                                                   caltech101  \\\n",
       "metric                                                                                                                                        L1_acc   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                  \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   0.860000   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.803000   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.794000   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.836000   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)   0.844000   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)   0.748000   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   0.857000   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)   0.776000   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   0.838000   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)   0.812000   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)   0.757333   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.816000   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.717333   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.681000   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.784000   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)   0.155000   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.842000   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)   0.587000   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.770000   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)   0.736000   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)   0.755000   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.733000   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)   0.810000   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)   0.800000   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.658667   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)   0.609000   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.844000   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.666000   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)   0.760000   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.730000   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.825000   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)   0.816000   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   0.832000   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)   0.460000   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   0.782000   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)   0.794000   \n",
       "\n",
       "dataset                                                                                                                                              \\\n",
       "metric                                                                                                                                       L2_acc   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                 \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.888000   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.854667   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.856000   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.866667   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.890000   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  0.892000   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.898000   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  0.820000   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.871000   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  0.843000   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.829000   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)       NaN   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.748000   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.756000   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)       NaN   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.600000   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)       NaN   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  0.767000   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)       NaN   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)       NaN   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)  0.855000   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.802000   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)       NaN   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)       NaN   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.709333   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.652000   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)       NaN   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.811000   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)  0.872000   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.783000   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)       NaN   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)       NaN   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)       NaN   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)  0.558000   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)       NaN   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)       NaN   \n",
       "\n",
       "dataset                                                                                                                                    ...  \\\n",
       "metric                                                                                                                                     ...   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy      ...   \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  ...   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  ...   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  ...   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  ...   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  ...   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  ...   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  ...   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  ...   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  ...   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  ...   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  ...   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  ...   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  ...   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  ...   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)  ...   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  ...   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  ...   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)  ...   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  ...   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  ...   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  ...   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  ...   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  ...   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)  ...   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  ...   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)  ...   \n",
       "\n",
       "dataset                                                                                                                                              stanford_cars  \\\n",
       "metric                                                                                                                                    volume_pre_training_data   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                                \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                 14197122   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)               2320000000   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)                900000000   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                 15478289   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)                 14197122   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)                  1281167   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                 15478289   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)               2321281167   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                  1281167   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)               2330281167   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)               2320000000   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                 15478289   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                  1281167   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                  1281167   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)               2320000000   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)                  1281167   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                 15478289   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)                  1281167   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)                900000000   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)                  1281167   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)                  1281167   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                 14197122   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)                 14197122   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)                 10281167   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                 14197122   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)               2320000000   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                 14197122   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                 15478289   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)                  1281167   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                 15478289   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                  1281167   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)                  9000000   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)                  9000000   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)               2321281167   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)                 10281167   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)               2330281167   \n",
       "\n",
       "dataset                                                                                                                                   uc-merced-land-use-dataset  \\\n",
       "metric                                                                                                                                                        L1_acc   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                                  \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                   0.823810   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)                   0.749206   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)                   0.519048   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                   0.854762   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)                   0.073810   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)                   0.664286   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                   0.000000   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)                   0.585714   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                   0.102381   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)                   0.416667   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)                   0.023810   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                   0.833333   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                   0.680952   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                   0.685714   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)                   0.690476   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)                   0.000000   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                   0.800000   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)                   0.366667   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)                   0.733333   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)                   0.104762   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)                   0.000000   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                   0.028571   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)                   0.742857   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)                   0.819048   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                   0.380952   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)                   0.652381   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                   0.159524   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                   0.000000   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)                   0.009524   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                   0.061905   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                   0.000000   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)                   0.792857   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)                   0.609524   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)                   0.230952   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)                   0.650000   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)                   0.419048   \n",
       "\n",
       "dataset                                                                                                                                              \\\n",
       "metric                                                                                                                                       L2_acc   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                 \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.900000   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.888095   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.520635   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.876190   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.800000   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  0.880952   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.000000   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  0.809524   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.009524   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  0.415873   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.030952   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.879365   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.795238   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.809524   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.857143   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.000000   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.892063   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  0.704762   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.866667   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)  0.161905   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)  0.000000   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.238095   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  0.869841   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  0.895238   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.669841   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.680952   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.136508   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.000000   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)  0.021429   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.097619   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.000000   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  0.866667   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  0.676190   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)  0.152381   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  0.814286   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)  0.495238   \n",
       "\n",
       "dataset                                                                                                                                              \\\n",
       "metric                                                                                                                                     Linf_acc   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                 \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.800000   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.800000   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.193651   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.721429   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.761905   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  0.825397   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.000000   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  0.733333   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.000000   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  0.276190   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.050000   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.738095   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.634921   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.692857   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.757143   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.269841   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.788095   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  0.633333   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.759524   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)  0.121429   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)  0.000000   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.228571   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  0.759524   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  0.802381   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.530952   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.555556   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.085714   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.000000   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)  0.028571   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.140476   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.000000   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  0.740476   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  0.609524   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)  0.098413   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  0.707143   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)  0.414286   \n",
       "\n",
       "dataset                                                                                                                                           \\\n",
       "metric                                                                                                                                     borda   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy              \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  170.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  151.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   96.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  148.0   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  126.0   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  158.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   55.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  111.0   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   74.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)   66.0   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)   26.0   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  138.0   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  109.0   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  110.0   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  127.0   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)   44.0   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  169.0   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)   89.0   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  128.0   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)   43.0   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)    7.0   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   57.0   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  135.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  167.0   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   77.0   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)   96.0   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   45.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)    7.0   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)   28.0   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   41.0   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   19.0   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  149.0   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   87.0   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)   52.0   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  105.0   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)   72.0   \n",
       "\n",
       "dataset                                                                                                                                              \\\n",
       "metric                                                                                                                                    clean_acc   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                 \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.966667   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.952381   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.955556   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.954762   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.976190   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  0.971429   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.952381   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  0.952381   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.952381   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  0.569048   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.088095   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.942857   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.942857   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.935714   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.945238   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.873016   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.976190   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  0.933333   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  0.938095   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)  0.164286   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)  0.107143   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.666667   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  0.950000   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  0.971429   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.857143   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.942857   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.219048   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.042857   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)  0.174603   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.209524   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.480952   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  0.959524   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  0.866667   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)  0.482540   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  0.928571   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)  0.642857   \n",
       "\n",
       "dataset                                                                                                                                               \\\n",
       "metric                                                                                                                                    common_acc   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                  \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   0.863492   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.847619   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.783333   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.838095   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)   0.814286   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)   0.861905   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   0.806349   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)   0.777778   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)   0.857143   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)   0.421429   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)   0.082540   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.802381   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.790476   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.785714   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.795238   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)   0.645238   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.866667   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)   0.738095   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)   0.800000   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)   0.126984   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)   0.073810   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.447619   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)   0.796825   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)   0.857143   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.676190   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)   0.707937   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.188095   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)   0.092857   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)   0.121429   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)   0.161905   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)   0.378571   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)   0.861905   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   0.695238   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)   0.403175   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)   0.783333   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)   0.559524   \n",
       "\n",
       "dataset                                                                                                                                                  \\\n",
       "metric                                                                                                                                             geom   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                     \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  4.951017e-01   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  4.296971e-01   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  3.917079e-02   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  4.323401e-01   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  3.576140e-02   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  4.044271e-01   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.000000e+00   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  2.575628e-01   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  0.000000e+00   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  1.147706e-02   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  2.679353e-07   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  4.091918e-01   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  2.562525e-01   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  2.827642e-01   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  3.368357e-01   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  0.000000e+00   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  4.758292e-01   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  1.127445e-01   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  3.622695e-01   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)  4.296681e-05   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)  0.000000e+00   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  4.640042e-04   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  3.715125e-01   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  4.898830e-01   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  7.852726e-02   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)  1.647349e-01   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  7.690471e-05   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.000000e+00   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)  1.236257e-07   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  2.879747e-05   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.000000e+00   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  4.207975e-01   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  1.513687e-01   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)  6.737992e-04   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  2.722446e-01   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)  3.092505e-02   \n",
       "\n",
       "dataset                                                                                                                                              \\\n",
       "metric                                                                                                                                          sum   \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                 \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  4.353968   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  4.237302   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  2.972222   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  4.245238   \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  3.426190   \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)  4.203968   \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  1.758730   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)  3.858730   \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)  1.921429   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)  2.099206   \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)  0.275397   \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  4.196032   \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  3.844444   \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  3.909524   \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  4.045238   \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)  1.788095   \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  4.323016   \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  3.376190   \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)  4.097619   \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)  0.679365   \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)  0.180952   \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  1.609524   \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)  4.119048   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  4.345238   \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  3.115079   \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)  3.539683   \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.788889   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)  0.135714   \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)  0.355556   \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)  0.671429   \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)  0.859524   \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)  4.221429   \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  3.457143   \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)  1.367460   \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)  3.883333   \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)  2.530952   \n",
       "\n",
       "dataset                                                                                                                                                             \n",
       "metric                                                                                                                                    volume_pre_training_data  \n",
       "backbone                                        loss_function pre_training_strategy        model_type          model_size ft_strategy                               \n",
       "convnext_base.fb_in22k                          TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                 14197122  \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)               2320000000  \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  TRADES_v2     self-supervised (multimodal) fully convolutional large      FFT (50 epochs)                900000000  \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                 15478289  \n",
       "eva02_base_patch14_224.mim_in22k                TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)                 14197122  \n",
       "robust_convnext_base                            TRADES_v2     supervised (robust)          fully convolutional large      FFT (50 epochs)                  1281167  \n",
       "convnext_base.fb_in22k_ft_in1k                  TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                 15478289  \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       TRADES_v2     hybrid                       fully attention     large      FFT (50 epochs)               2321281167  \n",
       "convnext_base.fb_in1k                           TRADES_v2     supervised                   fully convolutional large      FFT (50 epochs)                  1281167  \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k TRADES_v2     hybrid                       fully convolutional large      FFT (50 epochs)               2330281167  \n",
       "vit_base_patch16_clip_224.laion2b               TRADES_v2     self-supervised (multimodal) fully attention     large      FFT (50 epochs)               2320000000  \n",
       "swin_base_patch4_window7_224.ms_in22k_ft_in1k   CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                 15478289  \n",
       "vit_base_patch16_224.augreg_in1k                TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                  1281167  \n",
       "                                                CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                  1281167  \n",
       "CLIP-convnext_base_w-laion2B-s13B-b82K          CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)               2320000000  \n",
       "vit_base_patch16_224.mae                        TRADES_v2     self-supervised              fully attention     large      FFT (50 epochs)                  1281167  \n",
       "convnext_base.fb_in22k_ft_in1k                  CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                 15478289  \n",
       "vit_base_patch16_224.mae                        CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)                  1281167  \n",
       "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K  CLASSIC_AT    self-supervised (multimodal) fully convolutional large      FFT (50 epochs)                900000000  \n",
       "robust_convnext_base                            CLASSIC_AT    supervised (robust)          fully convolutional large      FFT (50 epochs)                  1281167  \n",
       "robust_vit_base_patch16_224                     TRADES_v2     supervised (robust)          fully attention     large      FFT (50 epochs)                  1281167  \n",
       "vit_base_patch16_224.augreg_in21k               TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                 14197122  \n",
       "eva02_base_patch14_224.mim_in22k                CLASSIC_AT    self-supervised              fully attention     large      FFT (50 epochs)                 14197122  \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)                 10281167  \n",
       "vit_base_patch16_224.augreg_in21k               CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                 14197122  \n",
       "vit_base_patch16_clip_224.laion2b               CLASSIC_AT    self-supervised (multimodal) fully attention     large      FFT (50 epochs)               2320000000  \n",
       "convnext_base.fb_in22k                          CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                 14197122  \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       TRADES_v2     supervised                   fully attention     large      FFT (50 epochs)                 15478289  \n",
       "robust_vit_base_patch16_224                     CLASSIC_AT    supervised (robust)          fully attention     large      FFT (50 epochs)                  1281167  \n",
       "vit_base_patch16_224.augreg_in21k_ft_in1k       CLASSIC_AT    supervised                   fully attention     large      FFT (50 epochs)                 15478289  \n",
       "convnext_base.fb_in1k                           CLASSIC_AT    supervised                   fully convolutional large      FFT (50 epochs)                  1281167  \n",
       "coatnet_2_rw_224.sw_in12k                       TRADES_v2     supervised                   hybrid              large      FFT (50 epochs)                  9000000  \n",
       "                                                CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)                  9000000  \n",
       "vit_base_patch16_clip_224.laion2b_ft_in1k       CLASSIC_AT    hybrid                       fully attention     large      FFT (50 epochs)               2321281167  \n",
       "coatnet_2_rw_224.sw_in12k_ft_in1k               CLASSIC_AT    supervised                   hybrid              large      FFT (50 epochs)                 10281167  \n",
       "convnext_base.clip_laion2b_augreg_ft_in12k_in1k CLASSIC_AT    hybrid                       fully convolutional large      FFT (50 epochs)               2330281167  \n",
       "\n",
       "[36 rows x 62 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from process_database import process_grouped_df, process_rankings, compute_odds_ratio_by_group\n",
    "\n",
    "size = \"large\"\n",
    "grouped_df = process_grouped_df(final_data, size)\n",
    "grouped_df = process_rankings(grouped_df)\n",
    "grouped_df\n",
    "# grouped_df.sort_values(by=\"sum\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>metric</th>\n",
       "      <th>L1_acc</th>\n",
       "      <th>L2_acc</th>\n",
       "      <th>Linf_acc</th>\n",
       "      <th>borda</th>\n",
       "      <th>clean_acc</th>\n",
       "      <th>common_acc</th>\n",
       "      <th>geom</th>\n",
       "      <th>sum</th>\n",
       "      <th>volume_pre_training_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.841000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.615922</td>\n",
       "      <td>3.548000</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.854667</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.518307</td>\n",
       "      <td>3.400667</td>\n",
       "      <td>2320000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.856000</td>\n",
       "      <td>0.809333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523671</td>\n",
       "      <td>3.411333</td>\n",
       "      <td>900000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.541651</td>\n",
       "      <td>3.437667</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.949000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.587389</td>\n",
       "      <td>3.507000</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.892000</td>\n",
       "      <td>0.878000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.964000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.564726</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.620776</td>\n",
       "      <td>3.555000</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.919000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.376578</td>\n",
       "      <td>4.121000</td>\n",
       "      <td>2321281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.838000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.958000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.567785</td>\n",
       "      <td>3.479000</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.812000</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>0.802667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.505483</td>\n",
       "      <td>3.377667</td>\n",
       "      <td>2330281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.757333</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.921000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.364494</td>\n",
       "      <td>4.096333</td>\n",
       "      <td>2320000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.816000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.717333</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>0.268585</td>\n",
       "      <td>3.861333</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.681000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.683000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.240538</td>\n",
       "      <td>3.777000</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.784000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2320000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>3.228000</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.842000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.587000</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.781333</td>\n",
       "      <td>0.211785</td>\n",
       "      <td>3.697333</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.770000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.722667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>900000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.736000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.878667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.943000</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.438028</td>\n",
       "      <td>4.251000</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.313739</td>\n",
       "      <td>3.976000</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.810000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.841333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.658667</td>\n",
       "      <td>0.709333</td>\n",
       "      <td>0.642667</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.844000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.190066</td>\n",
       "      <td>3.604667</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.124638</td>\n",
       "      <td>3.312000</td>\n",
       "      <td>2320000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.844000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14197122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.666000</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.917000</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.303433</td>\n",
       "      <td>3.962000</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.834667</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.945333</td>\n",
       "      <td>0.888000</td>\n",
       "      <td>0.464345</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.282579</td>\n",
       "      <td>3.895000</td>\n",
       "      <td>15478289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.825000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.812000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.816000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.832000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.701000</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.052330</td>\n",
       "      <td>2.801000</td>\n",
       "      <td>2321281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.782000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.951000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10281167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.794000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2330281167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "metric    L1_acc    L2_acc  Linf_acc  borda  clean_acc  common_acc      geom  \\\n",
       "0       0.860000  0.888000  0.841000    NaN   0.959000         NaN  0.615922   \n",
       "1       0.803000  0.854667  0.806000    NaN   0.937000         NaN  0.518307   \n",
       "2       0.794000  0.856000  0.809333    NaN   0.952000         NaN  0.523671   \n",
       "3       0.836000  0.866667  0.797000    NaN   0.938000         NaN  0.541651   \n",
       "4       0.844000  0.890000  0.824000    NaN   0.949000         NaN  0.587389   \n",
       "5       0.748000  0.892000  0.878000    NaN   0.964000         NaN  0.564726   \n",
       "6       0.857000  0.898000  0.842000    NaN   0.958000         NaN  0.620776   \n",
       "7       0.776000  0.820000  0.774000   68.0   0.919000    0.832000  0.376578   \n",
       "8       0.838000  0.871000  0.812000    NaN   0.958000         NaN  0.567785   \n",
       "9       0.812000  0.843000  0.802667    NaN   0.920000         NaN  0.505483   \n",
       "10      0.757333  0.829000  0.765000   64.0   0.921000    0.824000  0.364494   \n",
       "11      0.816000       NaN  0.798000    NaN   0.936000         NaN       NaN   \n",
       "12      0.717333  0.748000  0.691000   35.0   0.901000    0.804000  0.268585   \n",
       "13      0.681000  0.756000  0.683000   26.0   0.877000    0.780000  0.240538   \n",
       "14      0.784000       NaN  0.770000    NaN   0.910000         NaN       NaN   \n",
       "15      0.155000  0.600000  0.759000   33.0   0.914000    0.800000  0.051613   \n",
       "16      0.842000       NaN  0.827000    NaN   0.950000         NaN       NaN   \n",
       "17      0.587000  0.767000  0.692000   25.0   0.870000    0.781333  0.211785   \n",
       "18      0.770000       NaN  0.722667    NaN   0.886667         NaN       NaN   \n",
       "19      0.736000       NaN  0.878667    NaN   0.957333         NaN       NaN   \n",
       "20      0.755000  0.855000  0.814000   87.0   0.943000    0.884000  0.438028   \n",
       "21      0.733000  0.802000  0.737000   45.0   0.894000    0.810000  0.313739   \n",
       "22      0.810000       NaN  0.783000    NaN   0.922000         NaN       NaN   \n",
       "23      0.800000       NaN  0.841333    NaN   0.960000         NaN       NaN   \n",
       "24      0.658667  0.709333  0.642667   18.0   0.844000    0.750000  0.190066   \n",
       "25      0.609000  0.652000  0.600000   13.0   0.782000    0.669000  0.124638   \n",
       "26      0.844000       NaN  0.826000    NaN   0.954000         NaN       NaN   \n",
       "27      0.666000  0.811000  0.739000   50.0   0.917000    0.829000  0.303433   \n",
       "28      0.760000  0.872000  0.834667  102.0   0.945333    0.888000  0.464345   \n",
       "29      0.730000  0.783000  0.709000   36.0   0.886000    0.787000  0.282579   \n",
       "30      0.825000       NaN  0.812000    NaN   0.940000         NaN       NaN   \n",
       "31      0.816000       NaN  0.828000    NaN   0.952000         NaN       NaN   \n",
       "32      0.832000       NaN  0.820000    NaN   0.950000         NaN       NaN   \n",
       "33      0.460000  0.558000  0.498000    6.0   0.701000    0.584000  0.052330   \n",
       "34      0.782000       NaN  0.819000    NaN   0.951000         NaN       NaN   \n",
       "35      0.794000       NaN  0.783000    NaN   0.913000         NaN       NaN   \n",
       "\n",
       "metric       sum  volume_pre_training_data  \n",
       "0       3.548000                  14197122  \n",
       "1       3.400667                2320000000  \n",
       "2       3.411333                 900000000  \n",
       "3       3.437667                  15478289  \n",
       "4       3.507000                  14197122  \n",
       "5       3.482000                   1281167  \n",
       "6       3.555000                  15478289  \n",
       "7       4.121000                2321281167  \n",
       "8       3.479000                   1281167  \n",
       "9       3.377667                2330281167  \n",
       "10      4.096333                2320000000  \n",
       "11           NaN                  15478289  \n",
       "12      3.861333                   1281167  \n",
       "13      3.777000                   1281167  \n",
       "14           NaN                2320000000  \n",
       "15      3.228000                   1281167  \n",
       "16           NaN                  15478289  \n",
       "17      3.697333                   1281167  \n",
       "18           NaN                 900000000  \n",
       "19           NaN                   1281167  \n",
       "20      4.251000                   1281167  \n",
       "21      3.976000                  14197122  \n",
       "22           NaN                  14197122  \n",
       "23           NaN                  10281167  \n",
       "24      3.604667                  14197122  \n",
       "25      3.312000                2320000000  \n",
       "26           NaN                  14197122  \n",
       "27      3.962000                  15478289  \n",
       "28      4.300000                   1281167  \n",
       "29      3.895000                  15478289  \n",
       "30           NaN                   1281167  \n",
       "31           NaN                   9000000  \n",
       "32           NaN                   9000000  \n",
       "33      2.801000                2321281167  \n",
       "34           NaN                  10281167  \n",
       "35           NaN                2330281167  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = grouped_df.reset_index()\n",
    "sub[ \"caltech101\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaN values: 3.75%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"8\" halign=\"left\">TOTAL</th>\n",
       "      <th colspan=\"2\" halign=\"left\">caltech101</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">stanford_cars</th>\n",
       "      <th colspan=\"8\" halign=\"left\">uc-merced-land-use-dataset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>borda</th>\n",
       "      <th>nan_geom_cnt</th>\n",
       "      <th>nan_sum_cnt</th>\n",
       "      <th>rank_borda</th>\n",
       "      <th>rank_geom</th>\n",
       "      <th>rank_sum</th>\n",
       "      <th>score_geom</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>L1_acc</th>\n",
       "      <th>L2_acc</th>\n",
       "      <th>...</th>\n",
       "      <th>geom</th>\n",
       "      <th>sum</th>\n",
       "      <th>L1_acc</th>\n",
       "      <th>L2_acc</th>\n",
       "      <th>Linf_acc</th>\n",
       "      <th>borda</th>\n",
       "      <th>clean_acc</th>\n",
       "      <th>common_acc</th>\n",
       "      <th>geom</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backbone</th>\n",
       "      <th>loss_function</th>\n",
       "      <th>pre_training_strategy</th>\n",
       "      <th>model_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>edgenext_small.usi_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>365.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495538</td>\n",
       "      <td>14.662857</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.811</td>\n",
       "      <td>...</td>\n",
       "      <td>1.208454e-03</td>\n",
       "      <td>1.637333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.361905</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.938095</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.129487</td>\n",
       "      <td>3.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deit_tiny_patch16_224.fb_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <td>256.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.325177</td>\n",
       "      <td>11.266238</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.731</td>\n",
       "      <td>...</td>\n",
       "      <td>7.641761e-06</td>\n",
       "      <td>1.147333</td>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.709524</td>\n",
       "      <td>0.561905</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.919048</td>\n",
       "      <td>0.692857</td>\n",
       "      <td>0.121495</td>\n",
       "      <td>3.361905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">regnetx_004.pycls_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <td>252.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.437787</td>\n",
       "      <td>10.845762</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.660</td>\n",
       "      <td>...</td>\n",
       "      <td>1.644276e-06</td>\n",
       "      <td>0.857000</td>\n",
       "      <td>0.692857</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.949206</td>\n",
       "      <td>0.759524</td>\n",
       "      <td>0.294316</td>\n",
       "      <td>3.938095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <td>158.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>9.000444</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.468</td>\n",
       "      <td>...</td>\n",
       "      <td>1.050168e-08</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.923810</td>\n",
       "      <td>0.663492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.761111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobilenetv3_large_100.ra_in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <td>177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.038294</td>\n",
       "      <td>8.869429</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.721429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edgenext_small.usi_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>275.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.291578</td>\n",
       "      <td>8.825714</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.811</td>\n",
       "      <td>...</td>\n",
       "      <td>8.139734e-04</td>\n",
       "      <td>1.433333</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.802381</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.935714</td>\n",
       "      <td>0.752381</td>\n",
       "      <td>0.284285</td>\n",
       "      <td>3.919048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobilenetv3_large_100.ra_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <td>206.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.141107</td>\n",
       "      <td>8.747540</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.646</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022437e-05</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.859524</td>\n",
       "      <td>0.463492</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>1.965873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deit_tiny_patch16_224.fb_in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully attention</th>\n",
       "      <td>167.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.280623</td>\n",
       "      <td>7.611524</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.721</td>\n",
       "      <td>...</td>\n",
       "      <td>4.840960e-08</td>\n",
       "      <td>0.209333</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.659524</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.878571</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.098777</td>\n",
       "      <td>3.209524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobilevit-small</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>177.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.035784</td>\n",
       "      <td>4.924286</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.515</td>\n",
       "      <td>...</td>\n",
       "      <td>8.733102e-08</td>\n",
       "      <td>0.231000</td>\n",
       "      <td>0.192857</td>\n",
       "      <td>0.280952</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.297619</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>1.414286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnet-b0</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <td>124.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.012939</td>\n",
       "      <td>3.946429</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.414</td>\n",
       "      <td>...</td>\n",
       "      <td>7.072560e-09</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.078571</td>\n",
       "      <td>0.161905</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.402381</td>\n",
       "      <td>0.292857</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>1.121429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coat_tiny.in1k</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>58.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.167673</td>\n",
       "      <td>3.573016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.625397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.747619</td>\n",
       "      <td>0.167673</td>\n",
       "      <td>3.573016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficientnet-b0</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>fully convolutional</th>\n",
       "      <td>91.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>3.465190</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.297</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421550e-10</td>\n",
       "      <td>0.159000</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.080952</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.319048</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.992857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobilevit-small</th>\n",
       "      <th>TRADES_v2</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>139.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>3.295667</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.121429</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.245238</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.645238</td>\n",
       "      <td>0.388095</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coat_tiny.in1k</th>\n",
       "      <th>CLASSIC_AT</th>\n",
       "      <th>supervised</th>\n",
       "      <th>hybrid</th>\n",
       "      <td>105.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>1.593095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.359524</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>1.104762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset                                                                                TOTAL  \\\n",
       "metric                                                                                 borda   \n",
       "backbone                      loss_function pre_training_strategy model_type                   \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               365.0   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      256.0   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  252.0   \n",
       "                              TRADES_v2     supervised            fully convolutional  158.0   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  177.0   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               275.0   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  206.0   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      167.0   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               177.0   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  124.0   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                58.0   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional   91.0   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               139.0   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid               105.0   \n",
       "\n",
       "dataset                                                                                             \\\n",
       "metric                                                                                nan_geom_cnt   \n",
       "backbone                      loss_function pre_training_strategy model_type                         \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                         0   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention                0   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional            0   \n",
       "                              TRADES_v2     supervised            fully convolutional            0   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional            0   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                         1   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional            0   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention                0   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                         0   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional            0   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                         5   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional            0   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                         1   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                         2   \n",
       "\n",
       "dataset                                                                                            \\\n",
       "metric                                                                                nan_sum_cnt   \n",
       "backbone                      loss_function pre_training_strategy model_type                        \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                        0   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention               0   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional           0   \n",
       "                              TRADES_v2     supervised            fully convolutional           0   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional           0   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                        1   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional           0   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention               0   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                        0   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional           0   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                        5   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional           0   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                        1   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                        2   \n",
       "\n",
       "dataset                                                                                           \\\n",
       "metric                                                                                rank_borda   \n",
       "backbone                      loss_function pre_training_strategy model_type                       \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                     1.0   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention            3.0   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional        4.0   \n",
       "                              TRADES_v2     supervised            fully convolutional        9.0   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional        6.0   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                     2.0   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional        5.0   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention            8.0   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                     6.0   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional       11.0   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                    14.0   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional       13.0   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                    10.0   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                    12.0   \n",
       "\n",
       "dataset                                                                                          \\\n",
       "metric                                                                                rank_geom   \n",
       "backbone                      loss_function pre_training_strategy model_type                      \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                    1.0   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention           3.0   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional       2.0   \n",
       "                              TRADES_v2     supervised            fully convolutional       8.0   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional       9.0   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                    4.0   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional       7.0   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention           5.0   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                   10.0   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional      11.0   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                    6.0   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional      12.0   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                   13.0   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                   14.0   \n",
       "\n",
       "dataset                                                                                         \\\n",
       "metric                                                                                rank_sum   \n",
       "backbone                      loss_function pre_training_strategy model_type                     \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                   1.0   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention          2.0   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional      3.0   \n",
       "                              TRADES_v2     supervised            fully convolutional      4.0   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional      5.0   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                   6.0   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional      7.0   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention          8.0   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                   9.0   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional     10.0   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                  11.0   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional     12.0   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                  13.0   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                  14.0   \n",
       "\n",
       "dataset                                                                                           \\\n",
       "metric                                                                                score_geom   \n",
       "backbone                      loss_function pre_training_strategy model_type                       \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                0.495538   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention       0.325177   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional   0.437787   \n",
       "                              TRADES_v2     supervised            fully convolutional   0.057308   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional   0.038294   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                0.291578   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional   0.141107   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention       0.280623   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                0.035784   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional   0.012939   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                0.167673   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional   0.002835   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                0.002004   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                0.000377   \n",
       "\n",
       "dataset                                                                                           \\\n",
       "metric                                                                                 score_sum   \n",
       "backbone                      loss_function pre_training_strategy model_type                       \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               14.662857   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      11.266238   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  10.845762   \n",
       "                              TRADES_v2     supervised            fully convolutional   9.000444   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional   8.869429   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                8.825714   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional   8.747540   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention       7.611524   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                4.924286   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional   3.946429   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                3.573016   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional   3.465190   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                3.295667   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                1.593095   \n",
       "\n",
       "dataset                                                                               caltech101  \\\n",
       "metric                                                                                    L1_acc   \n",
       "backbone                      loss_function pre_training_strategy model_type                       \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                   0.794   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention          0.656   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional      0.568   \n",
       "                              TRADES_v2     supervised            fully convolutional      0.536   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional      0.652   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                   0.764   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional      0.724   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention          0.646   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                   0.427   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional      0.343   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                     NaN   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional      0.246   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                   0.340   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                     NaN   \n",
       "\n",
       "dataset                                                                                       \\\n",
       "metric                                                                                L2_acc   \n",
       "backbone                      loss_function pre_training_strategy model_type                   \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               0.811   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      0.731   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  0.660   \n",
       "                              TRADES_v2     supervised            fully convolutional  0.468   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  0.481   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               0.811   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  0.646   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      0.721   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               0.515   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  0.414   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                 NaN   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  0.297   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               0.454   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                 NaN   \n",
       "\n",
       "dataset                                                                                ...  \\\n",
       "metric                                                                                 ...   \n",
       "backbone                      loss_function pre_training_strategy model_type           ...   \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               ...   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      ...   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  ...   \n",
       "                              TRADES_v2     supervised            fully convolutional  ...   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  ...   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               ...   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  ...   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      ...   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               ...   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  ...   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid               ...   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  ...   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               ...   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid               ...   \n",
       "\n",
       "dataset                                                                               stanford_cars  \\\n",
       "metric                                                                                         geom   \n",
       "backbone                      loss_function pre_training_strategy model_type                          \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               1.208454e-03   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      7.641761e-06   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  1.644276e-06   \n",
       "                              TRADES_v2     supervised            fully convolutional  1.050168e-08   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  0.000000e+00   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               8.139734e-04   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  1.022437e-05   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      4.840960e-08   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               8.733102e-08   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  7.072560e-09   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                        NaN   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  1.421550e-10   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               0.000000e+00   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                        NaN   \n",
       "\n",
       "dataset                                                                                          \\\n",
       "metric                                                                                      sum   \n",
       "backbone                      loss_function pre_training_strategy model_type                      \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               1.637333   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      1.147333   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  0.857000   \n",
       "                              TRADES_v2     supervised            fully convolutional  0.959000   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  0.875000   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               1.433333   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  0.803000   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      0.209333   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               0.231000   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  0.149000   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                    NaN   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  0.159000   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               0.195000   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                    NaN   \n",
       "\n",
       "dataset                                                                               uc-merced-land-use-dataset  \\\n",
       "metric                                                                                                    L1_acc   \n",
       "backbone                      loss_function pre_training_strategy model_type                                       \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                                0.733333   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention                       0.478571   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional                   0.692857   \n",
       "                              TRADES_v2     supervised            fully convolutional                   0.161905   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional                   0.219048   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                                0.797619   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional                   0.433333   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention                       0.495238   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                                0.192857   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional                   0.078571   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                                0.625397   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional                   0.030952   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                                0.121429   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                                0.142857   \n",
       "\n",
       "dataset                                                                                          \\\n",
       "metric                                                                                   L2_acc   \n",
       "backbone                      loss_function pre_training_strategy model_type                      \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               0.702381   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      0.709524   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  0.800000   \n",
       "                              TRADES_v2     supervised            fully convolutional  0.011905   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  0.023810   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               0.802381   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  0.178571   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      0.659524   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               0.280952   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  0.161905   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid               0.704762   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  0.095238   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               0.266667   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid               0.171429   \n",
       "\n",
       "dataset                                                                                          \\\n",
       "metric                                                                                 Linf_acc   \n",
       "backbone                      loss_function pre_training_strategy model_type                      \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               0.361905   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      0.561905   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  0.736508   \n",
       "                              TRADES_v2     supervised            fully convolutional  0.000000   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  0.000000   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               0.630952   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  0.030952   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      0.547619   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               0.261905   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  0.185714   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid               0.523810   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  0.080952   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               0.245238   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid               0.154762   \n",
       "\n",
       "dataset                                                                                      \\\n",
       "metric                                                                                borda   \n",
       "backbone                      loss_function pre_training_strategy model_type                  \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               54.0   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      51.0   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  65.0   \n",
       "                              TRADES_v2     supervised            fully convolutional  25.0   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  30.0   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               64.0   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  29.0   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      45.0   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               27.0   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  17.0   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid               58.0   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  16.0   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               27.0   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid               16.0   \n",
       "\n",
       "dataset                                                                                          \\\n",
       "metric                                                                                clean_acc   \n",
       "backbone                      loss_function pre_training_strategy model_type                      \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               0.938095   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      0.919048   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  0.949206   \n",
       "                              TRADES_v2     supervised            fully convolutional  0.923810   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  0.961905   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               0.935714   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  0.859524   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      0.878571   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               0.380952   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  0.402381   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid               0.971429   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  0.466667   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               0.645238   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid               0.359524   \n",
       "\n",
       "dataset                                                                                           \\\n",
       "metric                                                                                common_acc   \n",
       "backbone                      loss_function pre_training_strategy model_type                       \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid                0.740476   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention       0.692857   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional   0.759524   \n",
       "                              TRADES_v2     supervised            fully convolutional   0.663492   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional   0.516667   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid                0.752381   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional   0.463492   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention       0.628571   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid                0.297619   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional   0.292857   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid                0.747619   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional   0.319048   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid                0.388095   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid                0.276190   \n",
       "\n",
       "dataset                                                                                          \\\n",
       "metric                                                                                     geom   \n",
       "backbone                      loss_function pre_training_strategy model_type                      \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               0.129487   \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      0.121495   \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  0.294316   \n",
       "                              TRADES_v2     supervised            fully convolutional  0.000000   \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  0.000000   \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               0.284285   \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  0.000954   \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      0.098777   \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               0.001609   \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  0.000278   \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid               0.167673   \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  0.000036   \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               0.001989   \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid               0.000376   \n",
       "\n",
       "dataset                                                                                          \n",
       "metric                                                                                      sum  \n",
       "backbone                      loss_function pre_training_strategy model_type                     \n",
       "edgenext_small.usi_in1k       TRADES_v2     supervised            hybrid               3.476190  \n",
       "deit_tiny_patch16_224.fb_in1k TRADES_v2     supervised            fully attention      3.361905  \n",
       "regnetx_004.pycls_in1k        CLASSIC_AT    supervised            fully convolutional  3.938095  \n",
       "                              TRADES_v2     supervised            fully convolutional  1.761111  \n",
       "mobilenetv3_large_100.ra_in1k TRADES_v2     supervised            fully convolutional  1.721429  \n",
       "edgenext_small.usi_in1k       CLASSIC_AT    supervised            hybrid               3.919048  \n",
       "mobilenetv3_large_100.ra_in1k CLASSIC_AT    supervised            fully convolutional  1.965873  \n",
       "deit_tiny_patch16_224.fb_in1k CLASSIC_AT    supervised            fully attention      3.209524  \n",
       "mobilevit-small               CLASSIC_AT    supervised            hybrid               1.414286  \n",
       "efficientnet-b0               CLASSIC_AT    supervised            fully convolutional  1.121429  \n",
       "coat_tiny.in1k                TRADES_v2     supervised            hybrid               3.573016  \n",
       "efficientnet-b0               TRADES_v2     supervised            fully convolutional  0.992857  \n",
       "mobilevit-small               TRADES_v2     supervised            hybrid               1.666667  \n",
       "coat_tiny.in1k                CLASSIC_AT    supervised            hybrid               1.104762  \n",
       "\n",
       "[14 rows x 56 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from process_database import process_grouped_df, process_rankings, compute_odds_ratio_by_group\n",
    "\n",
    "grouped_df = process_grouped_df(final_data, size=\"small\")\n",
    "grouped_df = process_rankings(grouped_df)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NaN values: 3.75%\n",
      "Percentage of NaN values: 3.75%\n",
      "Percentage of NaN values: 3.75%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from process_database import process_grouped_df, process_rankings, compute_odds_ratio_by_group\n",
    "import pandas as pd\n",
    "\n",
    "odds_loss_list = []\n",
    "odds_architecture_list = []\n",
    "odds_pretraining_list = []\n",
    "\n",
    "for size_id,size in [ (0,'small'),(1,'medium'),(2,'large')]:\n",
    "    grouped_df = process_grouped_df(final_data, size)\n",
    "    grouped_df = process_rankings(grouped_df)\n",
    "\n",
    "    odds_df = grouped_df['TOTAL']\n",
    "    odds_df = odds_df.reset_index()\n",
    "    odds_df.to_csv(\"./odds_ratio.csv\")\n",
    "\n",
    "    # Define tier1 threshold based on the number of rows\n",
    "    n_rows = odds_df.shape[0]\n",
    "    top_k = int(np.ceil(0.33 * n_rows))  # top 10% as tier1\n",
    "\n",
    "    # Apply tier1 flag\n",
    "    odds_df[\"in_tier1\"] = 0\n",
    "    odds_df.loc[odds_df.sort_values(by=\"score_sum\", ascending=False).head(top_k).index, \"in_tier1\"] = 1\n",
    "\n",
    "    # Compute odds ratios for model_type and loss_function\n",
    "    odds_model_type = compute_odds_ratio_by_group(odds_df, \"model_type\")\n",
    "    odds_pretrain_type = compute_odds_ratio_by_group(odds_df, \"pre_training_strategy\")\n",
    "    odds_loss_function = compute_odds_ratio_by_group(odds_df, \"loss_function\")\n",
    "    odds_loss_function['size'] = size\n",
    "    odds_pretrain_type['size'] = size\n",
    "    odds_model_type['size'] = size\n",
    "\n",
    "    odds_loss_list.append(odds_loss_function)\n",
    "    odds_architecture_list.append(odds_model_type)\n",
    "    odds_pretraining_list.append(odds_pretrain_type)\n",
    "\n",
    "combined_odds_loss = pd.concat(odds_loss_list, axis=0)\n",
    "combined_odds_architecture = pd.concat(odds_architecture_list, axis=0)\n",
    "combined_odds_pretraining = pd.concat(odds_pretraining_list, axis=0)\n",
    "\n",
    "combined_odds_pretraining = combined_odds_pretraining.fillna(0)\n",
    "combined_odds_pretraining = combined_odds_pretraining[combined_odds_pretraining[\"size\"].isin([\"medium\", \"large\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "loss_function=TRADES_v2<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "TRADES_v2",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "TRADES_v2",
         "offsetgroup": "TRADES_v2",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "8<br>(p=0.116)",
          "1.83<br>(p=0.441)",
          "5<br>(p=0.041)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "small",
          "medium",
          "large"
         ],
         "xaxis": "x",
         "y": [
          8,
          1.8333333333333333,
          5
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "loss_function=CLASSIC_AT<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "CLASSIC_AT",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "CLASSIC_AT",
         "offsetgroup": "CLASSIC_AT",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "0.12<br>(p=0.116)",
          "0.55<br>(p=0.441)",
          "0.20<br>(p=0.041)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "small",
          "medium",
          "large"
         ],
         "xaxis": "x",
         "y": [
          0.125,
          0.5454545454545454,
          0.2
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "height": 400,
        "legend": {
         "bgcolor": "rgba(255,255,255,1)",
         "bordercolor": "lightgrey",
         "borderwidth": 1,
         "font": {
          "size": 12
         },
         "title": {
          "text": "loss_function"
         },
         "tracegroupgap": 0,
         "xanchor": "right",
         "yanchor": "top"
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 0
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "uniformtext": {
         "minsize": 11,
         "mode": "show"
        },
        "width": 450,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgrey",
         "showgrid": false,
         "title": {
          "text": "Model Size"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgrey",
         "range": [
          0,
          25
         ],
         "showgrid": true,
         "title": {
          "text": "Odds Ratio"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "model_type=hybrid<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "hybrid",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "hybrid",
         "offsetgroup": "hybrid",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "0.20<br>(p=0.217)",
          "2.11<br>(p=0.611)",
          "0.21<br>(p=0.310)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "small",
          "medium",
          "large"
         ],
         "xaxis": "x",
         "y": [
          0.2,
          2.111111111111111,
          0.20833333333333334
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "model_type=fully attention<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "fully attention",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "fully attention",
         "offsetgroup": "fully attention",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "2<br>(p=0.653)",
          "0.11<br>(p=0.016)",
          "1.65<br>(p=0.481)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "small",
          "medium",
          "large"
         ],
         "xaxis": "x",
         "y": [
          2,
          0.10714285714285714,
          1.6545454545454545
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "model_type=fully convolutional<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "fully convolutional",
         "marker": {
          "color": "#00cc96",
          "pattern": {
           "shape": ""
          }
         },
         "name": "fully convolutional",
         "offsetgroup": "fully convolutional",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "3<br>(p=0.341)",
          "7<br>(p=0.024)",
          "1.19<br>(p=0.809)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "small",
          "medium",
          "large"
         ],
         "xaxis": "x",
         "y": [
          3,
          7,
          1.1904761904761905
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "height": 400,
        "legend": {
         "bgcolor": "rgba(255,255,255,1)",
         "bordercolor": "lightgrey",
         "borderwidth": 1,
         "font": {
          "size": 12
         },
         "title": {
          "text": "model_type"
         },
         "tracegroupgap": 0,
         "xanchor": "right",
         "yanchor": "top"
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 0
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "uniformtext": {
         "minsize": 7,
         "mode": "show"
        },
        "width": 450,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgrey",
         "showgrid": false,
         "title": {
          "text": "Model Size"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgrey",
         "range": [
          0,
          25
         ],
         "showgrid": true,
         "title": {
          "text": "Odds Ratio"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "pre_training_strategy=supervised<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "supervised",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "supervised",
         "offsetgroup": "supervised",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "0.64<br>(p=0.585)",
          "0.60<br>(p=0.481)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "medium",
          "large"
         ],
         "xaxis": "x",
         "y": [
          0.6428571428571429,
          0.6043956043956044
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "pre_training_strategy=supervised (robust)<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "supervised (robust)",
         "marker": {
          "color": "#EF553B",
          "pattern": {
           "shape": ""
          }
         },
         "name": "supervised (robust)",
         "offsetgroup": "supervised (robust)",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "6<br>(p=0.069)",
          "0.64<br>(p=0.709)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "medium",
          "large"
         ],
         "xaxis": "x",
         "y": [
          6,
          0.6363636363636364
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "pre_training_strategy=self-supervised<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "self-supervised",
         "marker": {
          "color": "#00cc96",
          "pattern": {
           "shape": ""
          }
         },
         "name": "self-supervised",
         "offsetgroup": "self-supervised",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "0.20<br>(p=0.300)",
          "0.64<br>(p=0.709)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "medium",
          "large"
         ],
         "xaxis": "x",
         "y": [
          0.2,
          0.6363636363636364
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "pre_training_strategy=self-supervised (multimodal)<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "self-supervised (multimodal)",
         "marker": {
          "color": "#ab63fa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "self-supervised (multimodal)",
         "offsetgroup": "self-supervised (multimodal)",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "5<br>(p=0.059)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "large"
         ],
         "xaxis": "x",
         "y": [
          5
         ],
         "yaxis": "y"
        },
        {
         "alignmentgroup": "True",
         "cliponaxis": false,
         "hovertemplate": "pre_training_strategy=hybrid (multimodal)<br>size=%{x}<br>Odds Ratio=%{y}<br>text_label=%{text}<extra></extra>",
         "legendgroup": "hybrid (multimodal)",
         "marker": {
          "color": "#FFA15A",
          "pattern": {
           "shape": ""
          }
         },
         "name": "hybrid (multimodal)",
         "offsetgroup": "hybrid (multimodal)",
         "orientation": "v",
         "showlegend": true,
         "text": [
          "0.46<br>(p=0.630)"
         ],
         "textangle": -90,
         "textfont": {
          "color": "black",
          "family": "Arial"
         },
         "textposition": "outside",
         "type": "bar",
         "x": [
          "large"
         ],
         "xaxis": "x",
         "y": [
          0.4583333333333333
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "group",
        "height": 400,
        "legend": {
         "bgcolor": "rgba(255,255,255,1)",
         "bordercolor": "lightgrey",
         "borderwidth": 1,
         "font": {
          "size": 12
         },
         "title": {
          "text": "pre_training_strategy"
         },
         "tracegroupgap": 0,
         "xanchor": "right",
         "yanchor": "top"
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 0
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "uniformtext": {
         "minsize": 7,
         "mode": "show"
        },
        "width": 450,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgrey",
         "showgrid": false,
         "title": {
          "text": "Model Size"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "gridcolor": "lightgrey",
         "range": [
          0,
          25
         ],
         "showgrid": true,
         "title": {
          "text": "Odds Ratio"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Create mapping from numeric size to categorical label\n",
    "size_map = {\"small\":0, \"medium\":1, \"large\":2}\n",
    "\n",
    "# Convert numeric size columns to categorical labels\n",
    "combined_odds_loss[\"size_label\"] = combined_odds_loss[\"size\"].map(size_map)\n",
    "combined_odds_architecture[\"size_label\"] = combined_odds_architecture[\"size\"].map(size_map)\n",
    "combined_odds_pretraining[\"size_label\"] = combined_odds_pretraining[\"size\"].map(size_map)\n",
    "\n",
    "plots = [\n",
    "    (\"loss_function\", combined_odds_loss),\n",
    "    (\"model_type\", combined_odds_architecture),\n",
    "    (\"pre_training_strategy\", combined_odds_pretraining)\n",
    "]\n",
    "\n",
    "\n",
    "figs = []\n",
    "\n",
    "for var, plot_df in plots:\n",
    "\n",
    "    plot_df = plot_df.copy()\n",
    "    plot_df[\"odds_ratio\"] = plot_df[\"odds_ratio\"].replace(0, 0)\n",
    "\n",
    "    # plot_df[\"text_label\"] = plot_df.apply(\n",
    "    #     lambda row: (\n",
    "    #         f'{int(row[\"odds_ratio\"])}' if row[\"odds_ratio\"] == int(row[\"odds_ratio\"])\n",
    "    #         else f'{row[\"odds_ratio\"]:.2f}'\n",
    "    #     ) + f'  (p:{row[\"p_value\"]:.3f})',\n",
    "    #     axis=1\n",
    "    # )\n",
    "    plot_df[\"text_label\"] = plot_df.apply(\n",
    "        lambda row: f'{int(row[\"odds_ratio\"])}' if row[\"odds_ratio\"] == int(row[\"odds_ratio\"])\n",
    "        else f'{row[\"odds_ratio\"]:.2f}', axis=1 )\n",
    "    plot_df[\"text_label\"] = plot_df.apply(\n",
    "        lambda row: f'{row[\"text_label\"]}<br>(p={row[\"p_value\"]:.3f})', axis=1)\n",
    "            \n",
    "    fig = px.bar(\n",
    "        plot_df,\n",
    "        x=\"size\",\n",
    "        y=\"odds_ratio\",\n",
    "        color=var,\n",
    "        barmode=\"group\",\n",
    "        text=\"text_label\",\n",
    "        labels={\"size_label\": \"Model Size\", \"odds_ratio\": \"Odds Ratio\"},\n",
    "    )\n",
    "\n",
    "    fig.update_layout(yaxis=dict(range=[0, 25]),)\n",
    "\n",
    "    fig.update_traces(\n",
    "        textposition=\"outside\",\n",
    "        textangle=-90,  # Rotate text vertically\n",
    "        textfont=dict(\n",
    "            color=\"black\",\n",
    "            family=\"Arial\"\n",
    "        ),\n",
    "        cliponaxis=False\n",
    "    )\n",
    "\n",
    "    if var == 'loss_function':\n",
    "        fig.update_layout(\n",
    "        uniformtext_minsize=11,  # or your desired minimum font size\n",
    "        uniformtext_mode='show',  # force showing all text\n",
    "        )\n",
    "    else:\n",
    "        fig.update_layout(\n",
    "        uniformtext_minsize=7,  # or your desired minimum font size\n",
    "        uniformtext_mode='show',  # force showing all text\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "            margin=dict(l=0, r=0, t=0, b=0),  # remove all outer margins\n",
    "            width=450,    # in pixels (~2.5 inches at 96 DPI)\n",
    "            height=400,    # in pixels (~1.875 inches)\n",
    "            yaxis_title=\"Odds Ratio\",\n",
    "            xaxis_title=\"Model Size\",\n",
    "            legend_title_text=var,\n",
    "            \n",
    "            # White background\n",
    "            plot_bgcolor=\"white\",\n",
    "            paper_bgcolor=\"white\",\n",
    "            \n",
    "            # Light grey grid\n",
    "            xaxis=dict(showgrid=False, gridcolor=\"lightgrey\"),\n",
    "            yaxis=dict(showgrid=True, gridcolor=\"lightgrey\"),\n",
    "            \n",
    "            # Legend inside figure\n",
    "            legend=dict(    \n",
    "                font=dict(size=12),  # Adjust font size here \n",
    "                yanchor=\"top\",\n",
    "                xanchor=\"right\",\n",
    "                bgcolor=\"rgba(255,255,255,1)\",  # semi-transparent white box\n",
    "                bordercolor=\"lightgrey\",\n",
    "                borderwidth=1\n",
    "            )\n",
    "        )\n",
    "    fig.show()\n",
    "    fig.write_image(\"./paper_figures/oddsratio_{}_{}.png\".format(pn2,var), scale=3  )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "#9e3303",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_clip_224.laion2b_ft_in1k (TRADES_v2)",
         "r": [
          1.641714285714286,
          2.575857142857143,
          2.4633333333333334,
          4.657380952380953,
          3.902444444444444,
          1.641714285714286
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#8e8ac0",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K (TRADES_v2)",
         "r": [
          2.0030476190476194,
          2.5049682539682543,
          3.185984126984127,
          5.188555555555555,
          3.647333333333333,
          2.0030476190476194
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#f3701b",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "swin_base_patch4_window7_224.ms_in22k_ft_in1k (TRADES_v2)",
         "r": [
          2.6397619047619045,
          2.566190476190476,
          2.9914285714285715,
          4.830761904761904,
          3.308095238095238,
          2.6397619047619045
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#fd8c3b",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "eva02_base_patch14_224.mim_in22k (TRADES_v2)",
         "r": [
          1.7624761904761905,
          2.627,
          3.2612380952380953,
          4.864857142857143,
          3.3379523809523812,
          1.7624761904761905
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#b03903",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_clip_224.laion2b (TRADES_v2)",
         "r": [
          1.281142857142857,
          2.2386190476190477,
          2.226,
          3.8190952380952385,
          3.266539682539683,
          1.281142857142857
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#f3701b",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "swin_base_patch4_window7_224.ms_in22k_ft_in1k (CLASSIC_AT)",
         "r": [
          1.743666666666667,
          2.366365079365079,
          2.7670952380952385,
          4.457857142857143,
          2.943380952380952,
          1.743666666666667
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#ec620f",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_224.augreg_in1k (TRADES_v2)",
         "r": [
          1.5542857142857145,
          2.141238095238095,
          1.8439206349206347,
          3.7818571428571435,
          3.1764761904761905,
          1.5542857142857145
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#4f1f8b",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "robust_convnext_base (TRADES_v2)",
         "r": [
          1.7849523809523808,
          2.1596190476190475,
          3.2573968253968255,
          4.541428571428572,
          3.0859047619047617,
          1.7849523809523808
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#9e9ac8",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "CLIP-convnext_base_w-laion2B-s13B-b82K (TRADES_v2)",
         "r": [
          2.3152063492063495,
          2.1927619047619045,
          3.66,
          5.1943809523809525,
          2.9729523809523815,
          2.3152063492063495
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#674ba0",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "convnext_base.fb_in22k (TRADES_v2)",
         "r": [
          2.2261428571428574,
          2.18,
          3.5759999999999996,
          5.057666666666666,
          2.778492063492063,
          2.2261428571428574
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#ec620f",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_224.augreg_in1k (CLASSIC_AT)",
         "r": [
          1.6133809523809526,
          2.0508571428571427,
          1.874190476190476,
          3.127714285714286,
          2.640714285714286,
          1.6133809523809526
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#9e9ac8",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "CLIP-convnext_base_w-laion2B-s13B-b82K (CLASSIC_AT)",
         "r": [
          1.5124761904761905,
          2.137142857142857,
          2.767142857142857,
          4.016238095238095,
          2.5502380952380954,
          1.5124761904761905
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#c54102",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_224.mae (TRADES_v2)",
         "r": [
          0.244,
          1.3846666666666667,
          1.9228412698412698,
          4.0620158730158735,
          3.3092380952380953,
          0.244
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#5b3495",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "convnext_base.fb_in22k_ft_in1k (CLASSIC_AT)",
         "r": [
          1.3576666666666668,
          1.9860634920634923,
          2.681095238095238,
          3.9505238095238098,
          2.5333333333333337,
          1.3576666666666668
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#c54102",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_224.mae (CLASSIC_AT)",
         "r": [
          1.0303333333333333,
          1.8767619047619046,
          1.9003333333333332,
          3.168333333333333,
          2.677095238095238,
          1.0303333333333333
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#8e8ac0",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "CLIP-convnext_base_w-laion_aesthetic-s13B-b82K (CLASSIC_AT)",
         "r": [
          1.4209999999999998,
          1.994,
          2.5188571428571427,
          3.7970952380952383,
          2.4206666666666665,
          1.4209999999999998
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#7e79b8",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "convnext_base.clip_laion2b_augreg_ft_in12k_in1k (TRADES_v2)",
         "r": [
          1.9113333333333333,
          1.891873015873016,
          2.6211904761904763,
          3.952047619047619,
          2.5784285714285713,
          1.9113333333333333
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#4f1f8b",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "robust_convnext_base (CLASSIC_AT)",
         "r": [
          0.7857619047619047,
          1.665904761904762,
          2.6744285714285714,
          3.771952380952381,
          2.3656507936507936,
          0.7857619047619047
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#f87f2c",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "robust_vit_base_patch16_224 (TRADES_v2)",
         "r": [
          0.8570000000000001,
          1.4140000000000001,
          1.282,
          3.0621428571428573,
          2.669476190476191,
          0.8570000000000001
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#e25508",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_224.augreg_in21k (TRADES_v2)",
         "r": [
          0.7805714285714286,
          1.406095238095238,
          1.2805714285714287,
          3.1346666666666665,
          2.602619047619048,
          0.7805714285714286
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#fd8c3b",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "eva02_base_patch14_224.mim_in22k (CLASSIC_AT)",
         "r": [
          1.3278571428571428,
          1.7268412698412698,
          2.2768571428571427,
          3.319,
          2.0548253968253967,
          1.3278571428571428
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#7262ac",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "convnext_base.fb_in1k (TRADES_v2)",
         "r": [
          1.9413809523809524,
          1.2045238095238093,
          2.473,
          4.971380952380952,
          2.775142857142857,
          1.9413809523809524
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#5b3495",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "convnext_base.fb_in22k_ft_in1k (TRADES_v2)",
         "r": [
          1.3893333333333333,
          1.2389999999999999,
          2.707,
          5.011380952380952,
          2.7066825396825394,
          1.3893333333333333
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#e25508",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_224.augreg_in21k (CLASSIC_AT)",
         "r": [
          1.1779523809523809,
          1.6521746031746032,
          1.4322857142857144,
          2.4564761904761903,
          2.0251904761904767,
          1.1779523809523809
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#b03903",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_clip_224.laion2b (CLASSIC_AT)",
         "r": [
          1.3503809523809525,
          1.5679523809523812,
          1.4128888888888889,
          2.333190476190476,
          1.8896031746031747,
          1.3503809523809525
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#674ba0",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "convnext_base.fb_in22k (CLASSIC_AT)",
         "r": [
          0.7791904761904761,
          1.3965079365079365,
          2.7663809523809526,
          4.2587142857142855,
          1.979095238095238,
          0.7791904761904761
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#d84801",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_224.augreg_in21k_ft_in1k (TRADES_v2)",
         "r": [
          0.7693333333333334,
          1.237,
          1.115,
          2.495857142857143,
          2.148857142857143,
          0.7693333333333334
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#f87f2c",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "robust_vit_base_patch16_224 (CLASSIC_AT)",
         "r": [
          0.9258571428571428,
          1.4007619047619049,
          1.3442380952380952,
          2.1749365079365077,
          1.8924285714285716,
          0.9258571428571428
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#d84801",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_224.augreg_in21k_ft_in1k (CLASSIC_AT)",
         "r": [
          0.8602380952380952,
          1.065952380952381,
          0.9944761904761905,
          2.4435238095238097,
          2.148904761904762,
          0.8602380952380952
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#7262ac",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "convnext_base.fb_in1k (CLASSIC_AT)",
         "r": [
          0.5393333333333333,
          1.069,
          2.259666666666667,
          4.200619047619048,
          2.0175714285714283,
          0.5393333333333333
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#73c476",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "coatnet_2_rw_224.sw_in12k (CLASSIC_AT)",
         "r": [
          0.7731904761904762,
          1.002857142857143,
          2.4248571428571424,
          3.758,
          1.3952380952380952,
          0.7731904761904762
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#006428",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "coatnet_2_rw_224.sw_in12k_ft_in1k (TRADES_v2)",
         "r": [
          0.8670476190476191,
          1.1492380952380952,
          2.978714285714285,
          4.252428571428571,
          1.4051428571428572,
          0.8670476190476191
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#9e3303",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "vit_base_patch16_clip_224.laion2b_ft_in1k (CLASSIC_AT)",
         "r": [
          0.7599523809523809,
          0.9180476190476191,
          0.8100793650793651,
          1.8392063492063493,
          1.4928412698412696,
          0.7599523809523809
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#006428",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "coatnet_2_rw_224.sw_in12k_ft_in1k (CLASSIC_AT)",
         "r": [
          0.8240000000000001,
          1.0012857142857143,
          2.575142857142857,
          4.0525714285714285,
          1.3133333333333335,
          0.8240000000000001
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#7e79b8",
          "dash": "solid",
          "width": 2
         },
         "mode": "lines",
         "name": "convnext_base.clip_laion2b_augreg_ft_in12k_in1k (CLASSIC_AT)",
         "r": [
          0.726047619047619,
          1.0182380952380954,
          1.6612857142857145,
          2.5708571428571427,
          1.3645238095238095,
          0.726047619047619
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        },
        {
         "line": {
          "color": "#73c476",
          "dash": "dot",
          "width": 2
         },
         "mode": "lines",
         "name": "coatnet_2_rw_224.sw_in12k (TRADES_v2)",
         "r": [
          0.7928571428571428,
          0.8666666666666667,
          2.91747619047619,
          4.25052380952381,
          0.861904761904762,
          0.7928571428571428
         ],
         "theta": [
          "sum_L1_acc",
          "sum_L2_acc",
          "sum_Linf_acc",
          "sum_clean_acc",
          "sum_common_acc",
          "sum_L1_acc"
         ],
         "type": "scatterpolar"
        }
       ],
       "layout": {
        "font": {
         "color": "black"
        },
        "height": 500,
        "legend": {
         "bgcolor": "rgba(255,255,255,0.9)",
         "bordercolor": "lightgrey",
         "borderwidth": 1,
         "font": {
          "size": 11
         },
         "x": 0,
         "xanchor": "left",
         "y": 1,
         "yanchor": "top"
        },
        "margin": {
         "b": 25,
         "l": 25,
         "r": 25,
         "t": 25
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "polar": {
         "angularaxis": {
          "direction": "clockwise",
          "gridcolor": "lightgrey",
          "gridwidth": 0.5,
          "linecolor": "lightgrey",
          "linewidth": 1,
          "showline": true
         },
         "bgcolor": "white",
         "radialaxis": {
          "gridcolor": "lightgrey",
          "gridwidth": 0.5,
          "linecolor": "lightgrey",
          "linewidth": 1,
          "range": [
           0,
           6
          ],
          "showline": true,
          "visible": true
         }
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 500
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import to_hex\n",
    "import re\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1 = grouped_df.copy()\n",
    "\n",
    "metrics_to_sum = [\"L1_acc\", \"L2_acc\", \"Linf_acc\", \"clean_acc\", \"common_acc\"]\n",
    "\n",
    "for metric in metrics_to_sum:\n",
    "    metric_cols = [col for col in df1.columns if col[1] == metric]\n",
    "    df1[f'sum_{metric}'] = df1[metric_cols].sum(axis=1)\n",
    "\n",
    "df_per_row_sums = df1[[f'sum_{m}' for m in metrics_to_sum]]\n",
    "\n",
    "df_per_row_sums.columns = df_per_row_sums.columns.get_level_values(0)\n",
    "\n",
    "df_per_row_sums.columns.name = 'metric'\n",
    "\n",
    "df_per_row_sums\n",
    "\n",
    "\n",
    "# Your model type mapping\n",
    "model_type_map = {\n",
    "    'convnext_base': \"fully convolutional\",\n",
    "    'convnext_tiny': \"fully convolutional\",\n",
    "    'deit_small': \"fully attention\",\n",
    "    'vit_base': \"fully attention\",\n",
    "    'vit_small': \"fully attention\",\n",
    "    'resnet50': \"fully convolutional\",\n",
    "    'eva02_base': \"fully attention\",\n",
    "    'eva02_tiny': \"fully attention\",\n",
    "    'swin_base': \"fully attention\",\n",
    "    'swin_tiny': \"fully attention\",\n",
    "    'coatnet_0': \"hybrid\",\n",
    "    'coatnet_2': \"hybrid\",\n",
    "    'regnetx_004': \"fully convolutional\",\n",
    "    'efficientnet-b0': \"fully convolutional\", \n",
    "    'deit_tiny': \"fully attention\",\n",
    "    'mobilevit-small': \"hybrid\",\n",
    "    'mobilenetv3': \"fully convolutional\",\n",
    "    'edgenext_small': \"fully convolutional\",\n",
    "    'coat_tiny': \"hybrid\",\n",
    "}\n",
    "\n",
    "# Colormap for each model type\n",
    "type_to_cmap = {\n",
    "    'fully convolutional': cm.Purples,     # deep violet → lavender\n",
    "    'fully attention': cm.Oranges,         # strong orange → light peach\n",
    "    'hybrid': cm.Greens                    # forest → mint\n",
    "}\n",
    "\n",
    "# Extract base backbone name from full backbone string\n",
    "def extract_base_name(backbone_name):\n",
    "    for base in model_type_map:\n",
    "        if base in backbone_name:\n",
    "            return base\n",
    "    return 'unknown'\n",
    "\n",
    "# Get all unique backbones\n",
    "backbones = sorted(set(backbone for backbone, _, _, _ in df_per_row_sums.index))\n",
    "\n",
    "# Map backbones to model type\n",
    "backbone_model_type = {b: model_type_map.get(extract_base_name(b), 'unknown') for b in backbones}\n",
    "\n",
    "# Assign gradient color per backbone based on model type\n",
    "color_map = {}\n",
    "for model_type in ['fully convolutional', 'fully attention', 'hybrid']:\n",
    "    bks = [bk for bk in backbones if backbone_model_type[bk] == model_type]\n",
    "    n = len(bks)\n",
    "    for i, bk in enumerate(bks):\n",
    "        start, end = 0.5, 0.9  # avoid very light or very dark edges\n",
    "        position = start + (end - start) * (i / max(n - 1, 1))\n",
    "        color = to_hex(type_to_cmap[model_type](position))\n",
    "        color_map[bk] = color\n",
    "\n",
    "\n",
    "# Define metrics\n",
    "metrics = ['sum_L1_acc', 'sum_L2_acc', 'sum_Linf_acc', 'sum_clean_acc', 'sum_common_acc']\n",
    "\n",
    "# Create a list of unique backbones (regardless of loss)\n",
    "unique_backbones = sorted(set(backbone for backbone, _, _, _ in df_per_row_sums.index))\n",
    "\n",
    "# # Assign a unique color to each backbone\n",
    "# color_palette = px.colors.qualitative.Plotly\n",
    "# color_map = dict(zip(unique_backbones, color_palette * (len(unique_backbones) // len(color_palette) + 1)))\n",
    "\n",
    "# Define line styles per loss function\n",
    "line_styles = {\n",
    "    'CLASSIC_AT': 'solid',\n",
    "    'TRADES_v2': 'dot'\n",
    "}\n",
    "\n",
    "# Create the radar plot\n",
    "\n",
    "from plotly.colors import qualitative\n",
    "color_palette = qualitative.Bold\n",
    "\n",
    "# Get unique backbones\n",
    "unique_backbones = sorted(set(backbone for backbone, _, _, _ in df_per_row_sums.index))\n",
    "\n",
    "# Assign colors per backbone\n",
    "# color_map = dict(zip(unique_backbones, color_palette * (len(unique_backbones) // len(color_palette) + 1)))\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for (backbone, loss, _, _), row in df_per_row_sums.iterrows():\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=row.tolist() + [row.tolist()[0]],  # Close the loop\n",
    "        theta=metrics + [metrics[0]],\n",
    "        mode='lines',\n",
    "        name=f\"{backbone} ({loss})\",\n",
    "        line=dict(\n",
    "            width=2,\n",
    "            color=color_map[backbone],\n",
    "            dash=line_styles.get(loss, 'solid')\n",
    "        )\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=500, \n",
    "    height=500,\n",
    "    paper_bgcolor='white',\n",
    "    plot_bgcolor='white',\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 6],\n",
    "            showline=True,\n",
    "            linewidth=1,\n",
    "            linecolor=\"lightgrey\",\n",
    "            gridcolor=\"lightgrey\",\n",
    "            gridwidth=0.5,\n",
    "        ),\n",
    "        angularaxis=dict(\n",
    "            direction=\"clockwise\",\n",
    "            showline=True,\n",
    "            linewidth=1,\n",
    "            linecolor=\"lightgrey\",\n",
    "            gridcolor=\"lightgrey\",\n",
    "            gridwidth=0.5,\n",
    "        ),\n",
    "        bgcolor='white'\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1.0,\n",
    "        xanchor='left',\n",
    "        yanchor='top',\n",
    "        bgcolor='rgba(255,255,255,0.9)',\n",
    "        bordercolor='lightgrey',\n",
    "        borderwidth=1,\n",
    "        font=dict(size=11),\n",
    "    ),\n",
    "    showlegend=False,\n",
    "    font=dict(color='black'),\n",
    "    margin=dict(l=25, r=25, t=25, b=25)  # <<< reduced margins\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"./radar_plot_{}_{}.png\".format(size, pn2), scale=3  )                # upscale for higher DPI (1 = default)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
