{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset, load_data\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import DistributedCustomSampler\n",
    "from losses import trades_loss, apgd_loss\n",
    "from tqdm.notebook import tqdm\n",
    "from architectures import CustomModel, load_architecture, add_lora, set_lora_gradients #load_statedict\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "args.dataset = 'Flowers'\n",
    "args.selection_method = 'random'\n",
    "args.aug = 'aug'\n",
    "args.loss_function = 'APGD'\n",
    "\n",
    "args.iterations = 10\n",
    "args.pruning_ratio = 0\n",
    "args.delta = 1\n",
    "args.batch_size = 24\n",
    "args.init_lr = 0.001\n",
    "args.freeze_epochs = 5\n",
    "args.backbone = 'convnext_tiny' #deit_small_patch16_224.fb_in1k\n",
    "args.ft_type = 'full_fine_tuning'\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, N, train_transform, transform = load_data(args) \n",
    "# print(N)\n",
    "\n",
    "train_dataset = WeightedDataset(args, train_dataset, train_transform, N, prune_ratio = args.pruning_ratio,  )\n",
    "\n",
    "# # train_sampler = DistributedCustomSampler(args, train_dataset, num_replicas=2, rank=0, drop_last=True)\n",
    "train_sampler = DistributedSampler(train_dataset, num_replicas=2, rank=0, shuffle=True, drop_last=True)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=3, sampler = train_sampler, )\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "# rank = 0\n",
    "model = load_architecture(args, N= 100, rank=0 )\n",
    "# model = CustomModel(args, model)\n",
    "\n",
    "# model.set_fine_tuning_strategy()\n",
    "# model.to(rank)\n",
    "# model = DDP(model, device_ids=[rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up the distributed setup, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.2.2+cu121\n",
      "cuda 12.1\n",
      "cudnn 8902\n",
      " world size 1, rank 0\n",
      "set up the master adress and port\n",
      "init process group ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/maxheuillet/robust-training20/7a67287d12a24a43a51db5ade66a89e4\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize dataset 0\n",
      "initialize sampler 0\n",
      "initialize dataoader 0\n",
      "start the loop\n",
      "effective batch size 1024 per_gpu_batch_size 8 accumulation steps 128\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mheuillet/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [768, 1, 7, 7], strides() = [49, 1, 7, 1]\n",
      "bucket_view.sizes() = [768, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale loss and backward done\n",
      "gradient norm: 1.1932249135537547 rank: 0 loss tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8531544966043745 rank: 0 loss tensor(0.0693, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.6430655788701953 rank: 0 loss tensor(0.0681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.2362334252894214 rank: 0 loss tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.977043914232255 rank: 0 loss tensor(0.0670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.3370458159440215 rank: 0 loss tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.016453171714801 rank: 0 loss tensor(0.0709, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.4964987453784175 rank: 0 loss tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.824662076572852 rank: 0 loss tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.369381855116547 rank: 0 loss tensor(0.0708, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.906030429732099 rank: 0 loss tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.4008377993490555 rank: 0 loss tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.380047285123043 rank: 0 loss tensor(0.0670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.691356423047514 rank: 0 loss tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.256477029491789 rank: 0 loss tensor(0.0677, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.886834242632577 rank: 0 loss tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.218332490059568 rank: 0 loss tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.083486971554532 rank: 0 loss tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.55282573627841 rank: 0 loss tensor(0.0662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.343677918885092 rank: 0 loss tensor(0.0704, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.735135271357358 rank: 0 loss tensor(0.0672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.339321491898307 rank: 0 loss tensor(0.0671, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.790987332381397 rank: 0 loss tensor(0.0664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.422467503406683 rank: 0 loss tensor(0.0684, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.779891013027541 rank: 0 loss tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.347971729397171 rank: 0 loss tensor(0.0653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.671516021000624 rank: 0 loss tensor(0.0672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.132349749688814 rank: 0 loss tensor(0.0667, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.764300145891962 rank: 0 loss tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.412276546081106 rank: 0 loss tensor(0.0667, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.887816365719612 rank: 0 loss tensor(0.0670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 18.308122828698345 rank: 0 loss tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.08823975850005 rank: 0 loss tensor(0.0669, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.624588362571263 rank: 0 loss tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.150576767223352 rank: 0 loss tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.62895587862249 rank: 0 loss tensor(0.0685, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.308382635579765 rank: 0 loss tensor(0.0677, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.758723008274057 rank: 0 loss tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.12448009158734 rank: 0 loss tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.90204433568105 rank: 0 loss tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.193193466562278 rank: 0 loss tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.66945861851889 rank: 0 loss tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.267896945737256 rank: 0 loss tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.922668543436522 rank: 0 loss tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.486377195288664 rank: 0 loss tensor(0.0669, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.169090549347914 rank: 0 loss tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.91058405127194 rank: 0 loss tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.38468916669364 rank: 0 loss tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.683672794587178 rank: 0 loss tensor(0.0672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.10374088621334 rank: 0 loss tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.463382000867313 rank: 0 loss tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 29.05724094187933 rank: 0 loss tensor(0.0669, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 29.692577943254054 rank: 0 loss tensor(0.0688, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.476118161776537 rank: 0 loss tensor(0.0697, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.17442156346961 rank: 0 loss tensor(0.0658, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.879137939254473 rank: 0 loss tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.61582489062979 rank: 0 loss tensor(0.0693, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.97849149489879 rank: 0 loss tensor(0.0684, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 33.005756503735604 rank: 0 loss tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 33.58867650039302 rank: 0 loss tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.14214335288091 rank: 0 loss tensor(0.0681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.162543468994336 rank: 0 loss tensor(0.0690, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.50866757515683 rank: 0 loss tensor(0.0687, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.39403904646469 rank: 0 loss tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.924886306024874 rank: 0 loss tensor(0.0684, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 37.25555163548214 rank: 0 loss tensor(0.0690, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 37.94436984006675 rank: 0 loss tensor(0.0671, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 38.28961462897619 rank: 0 loss tensor(0.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 38.80199139024459 rank: 0 loss tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 39.68331781357958 rank: 0 loss tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 39.99872228063288 rank: 0 loss tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 40.4357663553372 rank: 0 loss tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 41.03683156320196 rank: 0 loss tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 41.58739614368586 rank: 0 loss tensor(0.0671, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 42.27097606292336 rank: 0 loss tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 42.77790126620817 rank: 0 loss tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 43.24556230763736 rank: 0 loss tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 43.530201589674796 rank: 0 loss tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 43.732872538902036 rank: 0 loss tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 44.16347415817662 rank: 0 loss tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 44.83250798204959 rank: 0 loss tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 45.67227012869792 rank: 0 loss tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 46.21419531790408 rank: 0 loss tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 46.66011897924528 rank: 0 loss tensor(0.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 47.1381904237973 rank: 0 loss tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 47.56594798837103 rank: 0 loss tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 48.24269838947729 rank: 0 loss tensor(0.0697, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 48.59293966521845 rank: 0 loss tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 48.84011475174171 rank: 0 loss tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 49.17689486912972 rank: 0 loss tensor(0.0714, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 49.72409188114175 rank: 0 loss tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 50.16614795206366 rank: 0 loss tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 50.51691555662529 rank: 0 loss tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 50.93218641464069 rank: 0 loss tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 51.372360220669556 rank: 0 loss tensor(0.0665, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 51.97380398434283 rank: 0 loss tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 52.66338268866201 rank: 0 loss tensor(0.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 53.06565690448016 rank: 0 loss tensor(0.0701, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 53.31051810911985 rank: 0 loss tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 54.14876677047259 rank: 0 loss tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 54.96225337695413 rank: 0 loss tensor(0.0677, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 55.53111254089651 rank: 0 loss tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 55.897365053409125 rank: 0 loss tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 56.44062680377728 rank: 0 loss tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 56.948239444068655 rank: 0 loss tensor(0.0664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 57.7462462640728 rank: 0 loss tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 58.25175392360239 rank: 0 loss tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 58.81588891858311 rank: 0 loss tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 59.27256372973995 rank: 0 loss tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 59.70136574003552 rank: 0 loss tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 60.11612604246855 rank: 0 loss tensor(0.0664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 61.28178393301059 rank: 0 loss tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 61.80540499317654 rank: 0 loss tensor(0.0681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 62.661668449441976 rank: 0 loss tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 63.63751988362085 rank: 0 loss tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 64.38887447537532 rank: 0 loss tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 64.6614863917989 rank: 0 loss tensor(0.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 65.44880032331366 rank: 0 loss tensor(0.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 65.88651565848339 rank: 0 loss tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 66.4051795760424 rank: 0 loss tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 66.76943382740686 rank: 0 loss tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 67.31687193869264 rank: 0 loss tensor(0.0708, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 68.05109466194287 rank: 0 loss tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 68.34931033766728 rank: 0 loss tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 68.96193443167661 rank: 0 loss tensor(0.0671, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 70.1094159010405 rank: 0 loss tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 70.54406746950507 rank: 0 loss tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 70.95018732657498 rank: 0 loss tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "start validation\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "Rank 0, Iteration 0,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.2541555932315137 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5411966413483372 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.843753644860541 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1634678178479638 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.443635902288848 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.7106301690937735 rank: 0 loss tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.003592275236673 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.2675135665687165 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.57353116351774 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.8850670962713694 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.178744066845295 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.457029640319142 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.736119280463164 rank: 0 loss tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.020586589973291 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.314897396598659 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.592386043731535 rank: 0 loss tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.851712063783184 rank: 0 loss tensor(0.0078, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.1642009354956055 rank: 0 loss tensor(0.0085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.458770753764915 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.7501663058329955 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.045270547522871 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.287670992471925 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.545124504820354 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.837097739426493 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.10953132233944 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.395200692415142 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.649472342831715 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.917720260457012 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.190539336573812 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.514947188677242 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.760577044215083 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.023493716582479 rank: 0 loss tensor(0.0085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.296509293709473 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.535595488630932 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.813786091220123 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.087605448481213 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.374627835015824 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.637026240860232 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.927010491513014 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.189817964120003 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.475842821866976 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.751963135874169 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.012761844145269 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.268916180574614 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.597525600391776 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.92473461254188 rank: 0 loss tensor(0.0085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.194532462527604 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.443284173527369 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.752281275171764 rank: 0 loss tensor(0.0085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.054890755019452 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.380464269220944 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.660878307073942 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.929800309634686 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.224647639100793 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.533142065140677 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.846403442965233 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.146700830847248 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.496611590024514 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.78074210087413 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.083268108809666 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.359809051999143 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.655225572364856 rank: 0 loss tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.93155404975429 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 18.20915656426336 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 18.49123580699084 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 18.774185857100047 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.02832706970058 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.32287557877592 rank: 0 loss tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.611133261539635 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.87681432036448 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.188634919264302 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.48225324159757 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.769954720133228 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.071568408444207 rank: 0 loss tensor(0.0085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.40571515710526 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.670901720896282 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.9381621876754 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.187138332736502 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.468851573774916 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.74659513130224 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.97513116729997 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.251019880737026 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.50763625630614 rank: 0 loss tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.85393603958191 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.13947274416103 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.463835498957614 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.7324264032843 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.04552266744015 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.308234191439862 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.551333964737204 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.828997530123758 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.11243116886262 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.433735679079753 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.69437341091586 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.95579545670962 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.232134383890628 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.525204468549397 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.787368370973436 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.0507990510373 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.35943971313609 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.6600251595393 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.976761380620612 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 29.23454758205793 rank: 0 loss tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 29.524406944494622 rank: 0 loss tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 29.81382176003773 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.095472436394868 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.371634790655683 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.664278734953864 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.960345523987723 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.207692687570184 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.48768733382949 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.763541187234345 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.07127977639902 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.334151784660115 rank: 0 loss tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.631892775185264 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.91652342378669 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 33.18549393036396 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 33.47955163647645 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 33.739045712312006 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.03589599489819 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.29086348782266 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.59642183564233 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.889119636946084 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.18454067884766 rank: 0 loss tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.50737047618512 rank: 0 loss tensor(0.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.816288133822376 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.09181239503859 rank: 0 loss tensor(0.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.20983852254012 rank: 0 loss tensor(0.0150, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Rank 0, Iteration 1,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.13108145318863584 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.26245057714663067 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.39564635962254985 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5264890589687681 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.6583576750122688 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.790371252915032 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9213747706576001 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.0530834870657602 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1851028653362088 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.3155376290605831 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.447871720843072 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.5794868592185796 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.7093323412183559 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8407158266622357 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.9713388754752514 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.1030650321154982 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.2334103999910675 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.363652249007726 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.4960260132049257 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.6272668509271515 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.7586087575934957 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.891316020438728 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.022215726258325 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.15306118863503 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.2850265340713465 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.4147987237275714 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.5457942573619055 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.6774182317616546 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.809392045532437 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.940803030082969 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.0706977222057885 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.200838564730912 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.332276525813507 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.46188895891087 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.592839945729462 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.72434046815008 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.8557066894052126 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.985816961707651 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.11558993260005 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.246090722762081 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.376902306477923 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.507306237029269 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.637912130374643 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.769076616084551 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.901782557001364 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.03380776301432 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.164475512288656 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.297546900656633 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.428375331327795 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.558620842829953 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.687435700536793 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.817966148512105 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.94864821648383 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.080197176635994 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.2142334739908955 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.345198295955554 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.477423170954843 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.609653273092123 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.741047328970221 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.871707153373343 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.00217305640969 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.133089793575257 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.263363192541094 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.394181242401894 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.525503859946342 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.656047721062777 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.78678869257543 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.916211426552882 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.045907442716134 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.176737532121079 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.307555497077333 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.439376848888891 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.570208181563551 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.701170410450814 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.833969187699314 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.963435764446565 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.09568320400292 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.227347555748237 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.357520513932206 rank: 0 loss tensor(0.0065, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.487434553534463 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.616108994095057 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.747965831453357 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.879581582084153 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.01171643698606 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.141323697938358 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.271953312558384 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.404081146561975 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.534071592761654 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.664074993360874 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.792775190723983 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.92428347815451 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.052520911449815 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.183290748291908 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.315814392043139 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.448002724646514 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.578994047068782 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.710595795363298 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.842499265359972 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.971647891653848 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.103140623170985 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.233259826168686 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.364882561557419 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.496232779286656 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.626134670154114 rank: 0 loss tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.758275072118812 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.887862043433522 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.019314357882266 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.15110237934704 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.282664933998031 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.41208826556177 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.542903802835228 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.672983147989038 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.803266645097134 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.933252387405625 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.06439599847661 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.195756320408098 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.32725541248268 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.457979470141046 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.58978665003554 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.723092158272253 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.853296057804073 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.982337887971925 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.112666901316313 rank: 0 loss tensor(0.0062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.245391148352287 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.378605989121304 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.509670392615774 rank: 0 loss tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.64084710018105 rank: 0 loss tensor(0.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.905417352074227 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Rank 0, Iteration 2,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.6835173399420574 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.3675499041481303 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.051809849616833 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.7356965920824767 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.4200084969889177 rank: 0 loss tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.103797506288177 rank: 0 loss tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.788210610474956 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.471479298352471 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.155150648586976 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.838611822932453 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.52223858206813 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.20645428743321 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.890149741908724 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.57354371636879 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.257128773001947 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.941374701430998 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.62544754540117 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.309595579122934 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.993486153116143 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.677335222815918 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.360855208312834 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.043888356338243 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.727977346940825 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.412360662310846 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.095691881838594 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.779470325687548 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 18.462873206398882 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.1473895201033 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.83085438464685 rank: 0 loss tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.514934702441266 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.19890050344006 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.882869714018938 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.56749947887069 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.251092017746203 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.9356484318898 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.619678428218243 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.30399275429819 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.98818433936174 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.67269940696955 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.356334467870692 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.039397137925626 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.723513347882683 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 29.40710588195339 rank: 0 loss tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.090965179664078 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.77531666829653 rank: 0 loss tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.4590404507448 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.143285331985645 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.82763573516237 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 33.512068401316185 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.19580940113731 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.879133129794816 rank: 0 loss tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.563958824299796 rank: 0 loss tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.24794980733908 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.932336561380296 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 37.616821270114805 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 38.30167027693747 rank: 0 loss tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 38.985975366651026 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 39.67017097559439 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 40.35407223148675 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 41.03727040662573 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 41.72157348697096 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 42.40522615004553 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 43.08867335959627 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 43.77294673667268 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 44.457006504530305 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 45.14105374662625 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 45.82461009827248 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 46.50867212228903 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 47.19342507945225 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 47.87775951067147 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 48.5620796183032 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 49.24641821095547 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 49.93039758861475 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 50.61405658023066 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 51.297807278144354 rank: 0 loss tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 51.98156921211498 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 52.66555508752472 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 53.34935295478951 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 54.03285116269934 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 54.7166450263687 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 55.400819041587674 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 56.0843791099131 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 56.76874386533019 rank: 0 loss tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 57.45200185563761 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 58.1359840879149 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 58.81995413636996 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 59.50381192858129 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 60.187750928396106 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 60.87162057755025 rank: 0 loss tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 61.555517714218674 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 62.24018197188843 rank: 0 loss tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 62.924178610957526 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 63.608024628602855 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 64.29128758466682 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 64.97509634329005 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 65.6589992754236 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 66.3423514731094 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 67.02631858265231 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 67.71062870198047 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 68.39473006370056 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 69.07912989610702 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 69.76287524815658 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 70.44660400696425 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 71.131210193677 rank: 0 loss tensor(0.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 71.81541697475994 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 72.49901035801052 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 73.18276688468573 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 73.8668723043869 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 74.55003168999839 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 75.23446457718462 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 75.91897956958155 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 76.60354022344633 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 77.2867623545839 rank: 0 loss tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 77.96980464236049 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 78.65348367805929 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 79.33721781775544 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 80.02181494134285 rank: 0 loss tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 80.70586531759237 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 81.38982622484896 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 82.07382101281001 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 82.75757781940258 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 83.44187904330501 rank: 0 loss tensor(0.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 84.12553993263339 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 84.81069900089413 rank: 0 loss tensor(0.0736, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 85.4942708048976 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 86.1784782696904 rank: 0 loss tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 86.86215620230737 rank: 0 loss tensor(0.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 87.44580765550384 rank: 0 loss tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Rank 0, Iteration 3,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.36175549468047313 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.7235622307263984 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.085222811895467 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.4470164560258447 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.808791408738143 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.170589907905767 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.5322634020231005 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.8940793445952706 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.2556447657654366 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.617356874575145 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.9789498811485067 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.340571079791289 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.702305317725292 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.064165869170987 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.42597406639035 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.787834980357746 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.149537895366419 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.5112182885913645 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.8729934245083495 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.234754313505448 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.5962959324159325 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.958095737278021 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.319692313107556 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.681491268879675 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.043325154699696 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.405116502554174 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.76699528522258 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.128702876720837 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.490543344849433 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 10.852336208268792 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.214180210557634 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.575949034897691 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 11.93778495290749 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.299632543295026 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 12.66139436206516 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.023312348999182 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.384992365994556 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 13.746837881850986 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.108568996365193 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.470465932725824 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 14.832035906062263 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.193856048001493 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.55568294071782 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 15.917313288577235 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.279089465575897 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 16.640946953648765 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.00272719149613 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.36466459883198 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 17.726402782276416 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 18.088074168837387 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 18.44995341162752 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 18.81194873420821 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.173543355417745 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.53532425471075 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 19.896844531444998 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.258504643943425 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.620293392109108 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 20.982269048249634 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.344123652588 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 21.70583842162299 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.067380541824733 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.429128045007683 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 22.790752903435667 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.152474004886805 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.514133711541714 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 23.875886936731305 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.237846393265183 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.599671318756766 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 24.961516759163192 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.32329225645963 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 25.68502953528176 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.046736262416125 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.408381296025052 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 26.770189942681668 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.13192008765702 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.49381336228425 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 27.855602545966633 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.21722083313546 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.578851529207654 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 28.940596451222273 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 29.302202397858608 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 29.66412105298081 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.025826570633015 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.387503682624697 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 30.749451719116983 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.111179983932864 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.472975387729967 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 31.83453230132266 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.196282955750014 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.558003642719164 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 32.92002061562218 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 33.28183011595529 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 33.6434057853941 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.005195334382606 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.36688758749105 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 34.72856693817794 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.0904080366639 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.45222131837884 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 35.81412546155162 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.17586724341309 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.537507019804586 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 36.89900692927264 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 37.26075978866253 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 37.6224499315342 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 37.98418501712313 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 38.34591949956176 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 38.707580253815145 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 39.06953786165632 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 39.43128674691687 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 39.79327678363316 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 40.15490440754143 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 40.51675135533339 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 40.87849142011484 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 41.240143328884585 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 41.60193475953721 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 41.96390315520298 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 42.32573366438192 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 42.68750685843916 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 43.04916193349219 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 43.41094529166368 rank: 0 loss tensor(0.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 43.772659515582376 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 44.134406078772045 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 44.49634641114359 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 44.85832438567332 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 45.22019860542492 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 45.58196082167292 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 45.94353720706529 rank: 0 loss tensor(0.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 46.26286886133756 rank: 0 loss tensor(0.0147, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Rank 0, Iteration 4,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.07375004737191718 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.14747658213311668 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.22123542352350542 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.29497470392891184 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.3687385080909641 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.44248908125754616 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5162512593363824 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5900094004728068 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.6637677479877313 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.7375222403089441 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.8112626443874811 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.8849959621150216 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9587636112953188 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.0325265870607796 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1062842507253088 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1800428398573286 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.2537743267886765 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.327543488637645 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.401300566278717 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.4750724671882363 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.5488285738084464 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.622592454363873 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.6963544667227732 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.7701165266617411 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8438791040531008 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.9176326021427899 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.9913770575303333 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.0651363635722806 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.1389000976802284 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.2126361512554618 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.28637341828088 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.360123591265265 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.4338871945954197 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.507646297429567 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.5814097142490118 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.6551738384547696 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.7289360288662223 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.802700152675643 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.8764534529793293 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.9502059764461315 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.023962539705009 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.0977252601049026 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.1714934533383414 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.2452539420237816 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.3189993676006284 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.3927558200354193 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.4665228146033975 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.540284455958796 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.6140243762433126 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.687772848973431 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.7615255305565363 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.835277034054285 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.9090375556572705 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.982771294121321 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.056524331489438 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.130294178657703 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.204050471294749 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.277811641803687 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.351576812102042 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.425334601718018 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.499092945477558 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.572863846330021 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.646629313896305 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.720393343500226 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.7941570520799095 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.8679167817671 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.941688856741604 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.015439660612936 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.089207896306404 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.162953326084743 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.236722116664681 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.3104963657614475 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.384253133059189 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.458011466346991 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.531772344936687 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.6055346608867485 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.679277169630235 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.753041754896405 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.826798989079979 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.900551955917697 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.974302921939845 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.048058000296446 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.121822666682243 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.195592683162685 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.2693442395684595 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.343116473528046 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.416882607619723 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.49064545942432 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.564405843686003 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.638161762111801 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.711903463687141 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.785669801423446 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.859416898966226 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.933166855773112 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.006924831333961 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.080675721451631 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.154417965101785 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.228184789065745 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.3019245860351205 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.3756676346858505 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.4494331326240815 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.523196922608513 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.596962397550797 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.670721166345703 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.7444948135999265 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.818250240178947 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.892021260292709 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.965786366695872 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.039515838615078 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.113282976059097 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.187054667850495 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.260826306020748 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.334588130356842 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.408331968963973 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.48209323912443 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.55583542444925 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.629593123145895 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.703339541791939 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.777091206510697 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.850840600378596 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.924592703449697 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.998358240364405 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.072127602608921 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.145881097864413 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.21964221972855 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.293396950562144 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.367168977371161 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.514792429307622 rank: 0 loss tensor(0.0183, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "start validation\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "torch.Size([6, 1000]) torch.Size([6])\n",
      "Rank 0, Iteration 5,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.0721642196689999 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.14431180642864974 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.2164861939643526 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.2886597908809483 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.36082670591271465 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.4330003386818488 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5051693837533886 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5773490124840934 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.649513359532751 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.721683838830892 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.7938512363498764 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.8660184032574166 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9381988273491045 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.0103713735780135 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.082544896568517 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1546911443242447 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.2268567494220768 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.2990317601899048 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.3711843655889 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.443352209314141 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.5155093692331418 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.5876562096910052 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.6598229325878149 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.7320051488597712 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8041848092536685 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8763401078568072 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.94850917892957 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.020681145654519 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.0928534008501063 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.165017252509393 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.237196376397181 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.3093726768830334 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.3815288416097338 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.453698211224246 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.5258660054859328 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.5980358973476356 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.6702026860848487 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.7423686035845445 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.8145298534599323 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.886695526167939 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.9588611958155697 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.0310287313438677 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.1031863087244123 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.175350432461437 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.247512583276458 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.3196765245260313 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.3918567891026594 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.4640259291843134 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.536174085588508 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.6083386089176197 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.6805067334677304 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.752663072294695 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.8248335528440407 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.8970086170433715 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.9691717131680346 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.041341198606996 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.113517868734221 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.18568784081654 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.257846502722342 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.330026435632723 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.402189869169514 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.47435794726267 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.546529528319086 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.618678174073761 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.690861976368029 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.763042118069091 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.8352091048077845 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.907374477031802 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.979543952944995 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.0517235153938005 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.123876686522542 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.196035228953663 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.2681897964702324 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.340357881038745 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.412535413375039 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.484708855012665 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.556875078530842 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.629045631149373 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.701214583942981 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.773382270078825 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.845535853540181 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.917708220715574 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.989869310431381 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.0620365942276555 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.134195733260795 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.206363523952717 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.278505722945721 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.350661263578363 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.422840004347208 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.495018148077462 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.567191280973579 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.63936336054832 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.71152247163316 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.783686249353077 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.855831179168932 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.9279990712856385 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.00017181564075 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.072341705008061 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.144503225351787 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.2166871608160035 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.288860576403989 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.361035136820961 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.433189578100049 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.505363302876263 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.577535762270103 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.6497095604334 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.7218796787543855 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.794046843239719 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.866222213904404 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.938404662369764 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.010579180735085 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.08275063382797 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.154925311673738 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.22709021439106 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.29927566501581 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.371445077253911 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.44361637555625 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.515773668260792 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.587946663158663 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.660112926231337 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.732270564138236 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.80442569527364 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.876595732049344 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.948755331281362 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.020914143334224 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.093072636248868 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.165245083024027 rank: 0 loss tensor(0.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.309608375765889 rank: 0 loss tensor(0.0204, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Rank 0, Iteration 6,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.07105816303569121 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.14210517293123726 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.21314401485128065 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.2841894150457299 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.3552458338552134 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.42629798362402543 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.49732391540159826 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5683759997190836 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.6394113987978723 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.7104477520077445 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.7814792191790669 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.8525272261119946 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9235674619966101 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9946137433532565 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.065659876491228 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1366975500981775 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.2077311182308281 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.278776832904848 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.349826495427749 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.4208695871420698 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.4918999483495785 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.5629291014858653 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.6339547771353988 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.7049944672374326 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.7760345982110546 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8470671679904551 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.9181005629592314 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.9891516257910151 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.0602069163099266 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.1312546145259517 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.2022999928068447 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.2733495731887077 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.3443961780117184 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.4154467572964027 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.4864948518144137 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.5575278629963103 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.628580205540086 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.6996147757794073 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.77066728333276 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.84170265683239 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.912740751137639 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.9837977280145878 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.054852702878274 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.1258918316244504 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.196934037703901 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.2679895268421535 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.3390275729912333 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.410080989786778 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.4811198640603727 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.552166811904898 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.623216872005349 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.6942648212198694 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.765303700583376 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.8363480004115034 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.907397351215876 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.9784345597175244 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.049466100106826 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.120524631406122 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.191568908145371 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.262613686046009 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.3336614192253196 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.4047062009423135 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.475764396069559 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.546795193564753 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.617842150854502 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.68888708719085 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.759913026984503 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.830962770163319 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.901999730778478 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.973051704068246 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.044084596221302 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.1151230660147196 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.186165198586594 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.2571898271938435 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.32823202371258 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.399267504450285 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.470305384754745 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.541349029820328 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.61240013457169 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.683450282080242 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.754509233263074 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.825549743010908 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.896589368695033 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.9676312999862615 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.0386832685784695 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.1097348045784905 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.180777959181901 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.251826860459077 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.322851958687179 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.393893453383962 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.46493720168832 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.535972032730031 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.607015293648021 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.678062637635073 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.749119489286952 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.820181953996192 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.891216430998206 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.9622593346853545 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.033310867269611 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.104360456211545 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.175414243445277 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.246440564492572 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.317484702299232 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.388532987909543 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.459567380282497 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.53060087218322 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.601631290676232 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.672670898741583 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.743711765873 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.814751811626784 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.885787693692703 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.956836235870575 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.02786814691864 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.098917233266535 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.169965085561204 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.24101093193882 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.312057228955846 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.38310831805293 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.454162005129282 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.525188088310983 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.596223848931272 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.667279526065979 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.738322784009648 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.809380960583411 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.880408134232862 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.95143248047091 rank: 0 loss tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.022488643895246 rank: 0 loss tensor(0.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.164573493623324 rank: 0 loss tensor(0.0195, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Rank 0, Iteration 7,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.07031935027166568 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.14060764744761445 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.2109165381617432 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.28121078232900404 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.35152135165360615 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.4218343042843855 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.4921272880121385 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5624342740906337 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.6327558709598669 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.7030814438643688 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.7733839316204745 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.8437102661203787 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9140160439250068 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9843304245232879 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.0546352536363512 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1249651801376543 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1952747960655807 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.265578410022145 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.3358872144942704 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.406200854017621 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.4765137573091003 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.5468415340278325 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.6171624729552276 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.6874797359458418 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.7577864559889789 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8281116794404277 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8984169100830388 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.9687363971557243 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.0390542885749094 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.109360179062338 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.1796825181966857 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.250000972757986 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.3203041263056012 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.390599205957907 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.4609121464263715 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.53120684443572 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.6015081365958066 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.6718272256538484 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.742138596736742 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.812458285276955 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.8827690701542052 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.953070985184634 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.0233864737421277 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.0936916795985367 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.164007360729779 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.234317524447746 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.3046384745698543 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.3749623861743085 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.445283383197429 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.5155758330179117 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.585882984408798 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.656185995242054 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.726490400058196 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.7968197528036205 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.867133101961864 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.9374525764474924 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.007760263342608 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.078072021161623 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.148366984893821 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.2186784816613185 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.289010197662782 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.359325747809702 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.429634462149332 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.499953965021217 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.570263282973448 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.640571130205959 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.71088504456326 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.781199482967577 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.851511891073341 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.921818965237534 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.992129743346568 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.062446032122811 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.132764383865585 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.20307051410907 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.273392512960005 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.34370909046652 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.414013024418383 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.4843268649185735 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.55463574414316 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.6249474883931 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.695255472793838 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.765572942364512 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.835879163477264 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.906182396440397 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.976474564871783 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.0467947614321025 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.117114467028053 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.187429181828346 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.257757199150921 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.328062290827621 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.398378445025988 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.468697669150057 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.539022146016812 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.609329668605385 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.679648182173323 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.749960674167553 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.820272269370847 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.8905810600365385 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.960895676121943 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.031198132322687 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.101487140125178 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.171795320134385 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.242095941055682 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.312410251079579 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.382727172482625 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.453038365822246 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.523366596893876 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.593683066191817 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.664006405776308 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.734321181216322 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.804633411506454 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.874952558911735 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.945256025788347 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.015566111169626 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.085887998206152 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.15619467817368 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.226503943142097 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.296800747227396 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.367118364263877 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.437436421886261 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.507753778598152 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.57805224254973 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.648360120058818 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.718667778234126 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.788993789646982 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.859284657955019 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.929598228585279 rank: 0 loss tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.070295205701829 rank: 0 loss tensor(0.0182, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Rank 0, Iteration 8,\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.06997922581440406 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.13995004525668647 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.20994010360383727 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.27991615355755056 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.34990844220865264 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.41989263056269654 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.4898901683667912 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.5598807417846923 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.629873303372922 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.6998640866671874 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.7698386874768322 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.8398218414890638 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9098230615185686 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 0.9798173288845177 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.0498141874614564 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1197943478443533 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.1897911281413862 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.2597882622253624 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.3297756971193104 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.3997602604300552 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.4697500929806313 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.5397419236010697 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.6097266094825664 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.6797183610932185 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.7497052719870263 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8196989268369397 rank: 0 loss tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.8896887850998467 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 1.9596825899739558 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.0296861391757623 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.0996616495568468 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.1696553997147103 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.239630269245119 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.3096208367569706 rank: 0 loss tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.379613176941907 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.4495845120613224 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.5195759522097787 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.5895683637780134 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.659558616938399 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.729551581809741 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.799531472189275 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.8695216863586173 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 2.9395065889226366 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.009491635073071 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.0794792181243795 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.1494650850750774 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.21945218972132 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.2894339189430055 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.3594267198178014 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.4294180503191294 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.4994067740184156 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.569389293383551 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.6393778853007372 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.7093682386501325 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.779350604373681 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.8493357699658683 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.9193166804929915 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 3.989312479167021 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.059297535499205 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.129286026887512 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.199271172665052 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.2692603753348175 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.339245678525439 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.409236890643785 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.4792123496576 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.549200898497668 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.619189818353938 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.689176722166771 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.759159500147864 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.829157254851837 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.89914451038411 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 4.969143056347204 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.0391199140065615 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.109100640904161 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.179091362418718 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.249080692183455 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.319061891221663 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.389050114225671 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.459046023884121 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.529036925700536 rank: 0 loss tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.59902584338985 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.669016402523179 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.738997378796037 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.808993291365417 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.87897266744431 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 5.948957009516876 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.018946283402569 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.0889190232643084 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.158900456401411 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.228886782915671 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.298872102354167 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.368864534544545 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.438857077384287 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.508858003878493 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.578848348146821 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.648842708129125 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.718827633048915 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.7888256290605025 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.858816297710288 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.928811055729292 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 6.99880551983788 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.068806952973053 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.138785953073441 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.208774666431232 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.278766634120865 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.348756130595002 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.418760825696621 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.488752338974523 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.5587047960665785 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.628694336606108 rank: 0 loss tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.698683795840351 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.76866713446134 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.838650930903567 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.90863991956025 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 7.978622379249828 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.048621999397502 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.118608726531658 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.188601046641859 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.258596078115207 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.328571168936264 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.398557316526592 rank: 0 loss tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.46855219846349 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.538532199416222 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.608525287216446 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.678522662368785 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.748508002992548 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.818496733881368 rank: 0 loss tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 8.88847642912612 rank: 0 loss tensor(0.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 9.028543147655787 rank: 0 loss tensor(0.0173, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Rank 0, Iteration 9,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : APGD_0.001_sched_Flowers_convnext_tiny_random_0_random_10_0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/maxheuillet/robust-training20/7a67287d12a24a43a51db5ade66a89e4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     global_step [1280]   : (1, 1280)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gradient_norm [1280] : (0.06997922581440406, 87.44580765550384)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_value [1280]    : (0.7767572999000549, 9.421733856201172)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_schedule [1280]   : (2.4471741852423235e-05, 0.001)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_clean_accuracy   : 0.00980392156862745\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [2]         : (8.185188802083333, 17.933517156862745)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_robust_accuracy  : 0.00980392156862745\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     Name : APGD_0.001_sched_Flowers_convnext_tiny_random_0_random_10_0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     aug                 : aug\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     backbone            : convnext_tiny\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size          : 24\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_strategy      : random\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     beta                : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     data_dir            : ./data\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dataset             : Flowers\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     delta               : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     distance            : Linf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epsilon             : 0.01568627450980392\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     exp                 : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freeze_epochs       : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ft_type             : full_fine_tuning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     global_process_rank : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     init_lr             : 0.001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     iterations          : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     log_dir             : ./logs\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss_function       : APGD\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum            : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     num_workers         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     perturb_steps       : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pruning_ratio       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     pruning_strategy    : random\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     run_id              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     sample_size         : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     sched               : sched\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     seed                : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     selection_method    : random\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     statedict_dir       : ./state_dicts\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     step_size           : 0.00784313725490196\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     task                : train\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     unsup_fraction      : 0.3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay        : 0.0001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata             : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed) : 1 (59.46 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages       : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved by rank 0\n",
      "start the loop 3\n"
     ]
    }
   ],
   "source": [
    "from distributed_experiment1 import BaseExperiment\n",
    "\n",
    "world_size = torch.cuda.device_count()\n",
    "# set_seeds(args.seed)\n",
    "\n",
    "experiment = BaseExperiment(args, world_size)\n",
    "\n",
    "experiment.training(rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664424c9577f4bafa794a5eb04263e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 224, 224]) torch.Size([3]) tensor([44,  9, 38])\n",
      "torch.Size([3]) torch.Size([3, 3, 224, 224])\n",
      "tensor([44,  9, 38], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from losses import get_loss, get_eval_loss\n",
    "import numpy as np\n",
    "from losses import apgd_loss\n",
    "\n",
    "rank = 'cuda'\n",
    "\n",
    "optimizer = torch.optim.SGD( model.parameters(),lr=args.init_lr, weight_decay=args.weight_decay, momentum=args.momentum, nesterov=True, )\n",
    "\n",
    "from autoattack import autopgd_base\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "cutmix = v2.CutMix(num_classes=N)\n",
    "mixup = v2.MixUp(num_classes=N)\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
    "\n",
    "for iteration in range(args.iterations):\n",
    "\n",
    "    model.eval()\n",
    "    train_sampler.set_epoch(iteration)\n",
    "\n",
    "    # apgd = autopgd_base.APGDAttack(model, n_restarts=5, n_iter=args.perturb_steps, verbose=False,\n",
    "    #             eps=args.epsilon, norm=args.distance, eot_iter=1, rho=.75, seed=args.seed,\n",
    "    #             device='cuda', logger=None).perturb(data, target)\n",
    "    \n",
    "    for batch_id, batch in tqdm(enumerate( trainloader ) ):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, target, idxs = batch\n",
    "\n",
    "        print(data.shape, target.shape, target)\n",
    "\n",
    "        data, target_one_hot = cutmix_or_mixup(data, target)\n",
    "\n",
    "        print(target.shape, data.shape)\n",
    "\n",
    "        # print(data,target, idxs)\n",
    "\n",
    "        data, target = data.to(rank), target.to(rank) \n",
    "\n",
    "        print(target,)\n",
    "\n",
    "        break\n",
    "\n",
    "    break\n",
    "\n",
    "        # data, target_one_hot = cutmix_or_mixup(data, target)\n",
    "\n",
    "        # adv_data = apgd\n",
    "        # print('data attacked')\n",
    "\n",
    "        # loss_values, logits = get_loss(args, model, data, target, optimizer)\n",
    "\n",
    "        # loss = loss_values.mean()\n",
    "        # #loss = #train_dataset.compute_loss(idxs, loss_values)\n",
    "        # print(loss)\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # break\n",
    "    \n",
    "    # model.update_fine_tuning_strategy(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract labels from the dataset\n",
    "labels = [ label for _, label in dataset ]\n",
    "\n",
    "# Split the dataset into train+val and test, keeping stratification\n",
    "train_val_indices, test_indices = train_test_split( range(len(labels)), test_size=0.2, stratify=labels, random_state=42 )\n",
    "\n",
    "# Extract labels for the train+val set for further stratification\n",
    "train_val_labels = [labels[i] for i in train_val_indices]\n",
    "\n",
    "# Split the train+val set into train and validation, keeping stratification\n",
    "train_indices, val_indices = train_test_split( train_val_indices, test_size=0.15, stratify=train_val_labels, random_state=42 )  # 0.25 * 0.8 = 0.2 of the dataset\n",
    "\n",
    "# Create subsets for train, validation, and test\n",
    "\n",
    "N = 10\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://madm.dfki.de/files/sentinel/EuroSAT.zip to ./data/eurosat/EuroSAT.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 94280567/94280567 [00:15<00:00, 6066822.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/eurosat/EuroSAT.zip to ./data/eurosat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset EuroSAT\n",
       "    Number of datapoints: 27000\n",
       "    Root location: ./data"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat_resnext26ts.ch_in1k\n",
      "beit_base_patch16_224.in22k_ft_in22k\n",
      "beit_base_patch16_224.in22k_ft_in22k_in1k\n",
      "beit_base_patch16_384.in22k_ft_in22k_in1k\n",
      "beit_large_patch16_224.in22k_ft_in22k\n",
      "beit_large_patch16_224.in22k_ft_in22k_in1k\n",
      "beit_large_patch16_384.in22k_ft_in22k_in1k\n",
      "beit_large_patch16_512.in22k_ft_in22k_in1k\n",
      "beitv2_base_patch16_224.in1k_ft_in1k\n",
      "beitv2_base_patch16_224.in1k_ft_in22k\n",
      "beitv2_base_patch16_224.in1k_ft_in22k_in1k\n",
      "beitv2_large_patch16_224.in1k_ft_in1k\n",
      "beitv2_large_patch16_224.in1k_ft_in22k\n",
      "beitv2_large_patch16_224.in1k_ft_in22k_in1k\n",
      "botnet26t_256.c1_in1k\n",
      "caformer_b36.sail_in1k\n",
      "caformer_b36.sail_in1k_384\n",
      "caformer_b36.sail_in22k\n",
      "caformer_b36.sail_in22k_ft_in1k\n",
      "caformer_b36.sail_in22k_ft_in1k_384\n",
      "caformer_m36.sail_in1k\n",
      "caformer_m36.sail_in1k_384\n",
      "caformer_m36.sail_in22k\n",
      "caformer_m36.sail_in22k_ft_in1k\n",
      "caformer_m36.sail_in22k_ft_in1k_384\n",
      "caformer_s18.sail_in1k\n",
      "caformer_s18.sail_in1k_384\n",
      "caformer_s18.sail_in22k\n",
      "caformer_s18.sail_in22k_ft_in1k\n",
      "caformer_s18.sail_in22k_ft_in1k_384\n",
      "caformer_s36.sail_in1k\n",
      "caformer_s36.sail_in1k_384\n",
      "caformer_s36.sail_in22k\n",
      "caformer_s36.sail_in22k_ft_in1k\n",
      "caformer_s36.sail_in22k_ft_in1k_384\n",
      "cait_m36_384.fb_dist_in1k\n",
      "cait_m48_448.fb_dist_in1k\n",
      "cait_s24_224.fb_dist_in1k\n",
      "cait_s24_384.fb_dist_in1k\n",
      "cait_s36_384.fb_dist_in1k\n",
      "cait_xs24_384.fb_dist_in1k\n",
      "cait_xxs24_224.fb_dist_in1k\n",
      "cait_xxs24_384.fb_dist_in1k\n",
      "cait_xxs36_224.fb_dist_in1k\n",
      "cait_xxs36_384.fb_dist_in1k\n",
      "coat_lite_medium.in1k\n",
      "coat_lite_medium_384.in1k\n",
      "coat_lite_mini.in1k\n",
      "coat_lite_small.in1k\n",
      "coat_lite_tiny.in1k\n",
      "coat_mini.in1k\n",
      "coat_small.in1k\n",
      "coat_tiny.in1k\n",
      "coatnet_0_rw_224.sw_in1k\n",
      "coatnet_1_rw_224.sw_in1k\n",
      "coatnet_2_rw_224.sw_in12k\n",
      "coatnet_2_rw_224.sw_in12k_ft_in1k\n",
      "coatnet_3_rw_224.sw_in12k\n",
      "coatnet_bn_0_rw_224.sw_in1k\n",
      "coatnet_nano_rw_224.sw_in1k\n",
      "coatnet_rmlp_1_rw2_224.sw_in12k\n",
      "coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k\n",
      "coatnet_rmlp_1_rw_224.sw_in1k\n",
      "coatnet_rmlp_2_rw_224.sw_in1k\n",
      "coatnet_rmlp_2_rw_224.sw_in12k\n",
      "coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k\n",
      "coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k\n",
      "coatnet_rmlp_nano_rw_224.sw_in1k\n",
      "coatnext_nano_rw_224.sw_in1k\n",
      "convformer_b36.sail_in1k\n",
      "convformer_b36.sail_in1k_384\n",
      "convformer_b36.sail_in22k\n",
      "convformer_b36.sail_in22k_ft_in1k\n",
      "convformer_b36.sail_in22k_ft_in1k_384\n",
      "convformer_m36.sail_in1k\n",
      "convformer_m36.sail_in1k_384\n",
      "convformer_m36.sail_in22k\n",
      "convformer_m36.sail_in22k_ft_in1k\n",
      "convformer_m36.sail_in22k_ft_in1k_384\n",
      "convformer_s18.sail_in1k\n",
      "convformer_s18.sail_in1k_384\n",
      "convformer_s18.sail_in22k\n",
      "convformer_s18.sail_in22k_ft_in1k\n",
      "convformer_s18.sail_in22k_ft_in1k_384\n",
      "convformer_s36.sail_in1k\n",
      "convformer_s36.sail_in1k_384\n",
      "convformer_s36.sail_in22k\n",
      "convformer_s36.sail_in22k_ft_in1k\n",
      "convformer_s36.sail_in22k_ft_in1k_384\n",
      "convit_base.fb_in1k\n",
      "convit_small.fb_in1k\n",
      "convit_tiny.fb_in1k\n",
      "convmixer_768_32.in1k\n",
      "convmixer_1024_20_ks9_p14.in1k\n",
      "convmixer_1536_20.in1k\n",
      "convnext_atto.d2_in1k\n",
      "convnext_atto_ols.a2_in1k\n",
      "convnext_base.clip_laion2b\n",
      "convnext_base.clip_laion2b_augreg\n",
      "convnext_base.clip_laion2b_augreg_ft_in1k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k_in1k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384\n",
      "convnext_base.clip_laiona\n",
      "convnext_base.clip_laiona_320\n",
      "convnext_base.clip_laiona_augreg_320\n",
      "convnext_base.clip_laiona_augreg_ft_in1k_384\n",
      "convnext_base.fb_in1k\n",
      "convnext_base.fb_in22k\n",
      "convnext_base.fb_in22k_ft_in1k\n",
      "convnext_base.fb_in22k_ft_in1k_384\n",
      "convnext_femto.d1_in1k\n",
      "convnext_femto_ols.d1_in1k\n",
      "convnext_large.fb_in1k\n",
      "convnext_large.fb_in22k\n",
      "convnext_large.fb_in22k_ft_in1k\n",
      "convnext_large.fb_in22k_ft_in1k_384\n",
      "convnext_large_mlp.clip_laion2b_augreg\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in1k\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384\n",
      "convnext_large_mlp.clip_laion2b_ft_320\n",
      "convnext_large_mlp.clip_laion2b_ft_soup_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_384\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384\n",
      "convnext_nano.d1h_in1k\n",
      "convnext_nano.in12k\n",
      "convnext_nano.in12k_ft_in1k\n",
      "convnext_nano_ols.d1h_in1k\n",
      "convnext_pico.d1_in1k\n",
      "convnext_pico_ols.d1_in1k\n",
      "convnext_small.fb_in1k\n",
      "convnext_small.fb_in22k\n",
      "convnext_small.fb_in22k_ft_in1k\n",
      "convnext_small.fb_in22k_ft_in1k_384\n",
      "convnext_small.in12k\n",
      "convnext_small.in12k_ft_in1k\n",
      "convnext_small.in12k_ft_in1k_384\n",
      "convnext_tiny.fb_in1k\n",
      "convnext_tiny.fb_in22k\n",
      "convnext_tiny.fb_in22k_ft_in1k\n",
      "convnext_tiny.fb_in22k_ft_in1k_384\n",
      "convnext_tiny.in12k\n",
      "convnext_tiny.in12k_ft_in1k\n",
      "convnext_tiny.in12k_ft_in1k_384\n",
      "convnext_tiny_hnf.a2h_in1k\n",
      "convnext_xlarge.fb_in22k\n",
      "convnext_xlarge.fb_in22k_ft_in1k\n",
      "convnext_xlarge.fb_in22k_ft_in1k_384\n",
      "convnext_xxlarge.clip_laion2b_rewind\n",
      "convnext_xxlarge.clip_laion2b_soup\n",
      "convnext_xxlarge.clip_laion2b_soup_ft_in1k\n",
      "convnext_xxlarge.clip_laion2b_soup_ft_in12k\n",
      "convnextv2_atto.fcmae\n",
      "convnextv2_atto.fcmae_ft_in1k\n",
      "convnextv2_base.fcmae\n",
      "convnextv2_base.fcmae_ft_in1k\n",
      "convnextv2_base.fcmae_ft_in22k_in1k\n",
      "convnextv2_base.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_femto.fcmae\n",
      "convnextv2_femto.fcmae_ft_in1k\n",
      "convnextv2_huge.fcmae\n",
      "convnextv2_huge.fcmae_ft_in1k\n",
      "convnextv2_huge.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_huge.fcmae_ft_in22k_in1k_512\n",
      "convnextv2_large.fcmae\n",
      "convnextv2_large.fcmae_ft_in1k\n",
      "convnextv2_large.fcmae_ft_in22k_in1k\n",
      "convnextv2_large.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_nano.fcmae\n",
      "convnextv2_nano.fcmae_ft_in1k\n",
      "convnextv2_nano.fcmae_ft_in22k_in1k\n",
      "convnextv2_nano.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_pico.fcmae\n",
      "convnextv2_pico.fcmae_ft_in1k\n",
      "convnextv2_tiny.fcmae\n",
      "convnextv2_tiny.fcmae_ft_in1k\n",
      "convnextv2_tiny.fcmae_ft_in22k_in1k\n",
      "convnextv2_tiny.fcmae_ft_in22k_in1k_384\n",
      "crossvit_9_240.in1k\n",
      "crossvit_9_dagger_240.in1k\n",
      "crossvit_15_240.in1k\n",
      "crossvit_15_dagger_240.in1k\n",
      "crossvit_15_dagger_408.in1k\n",
      "crossvit_18_240.in1k\n",
      "crossvit_18_dagger_240.in1k\n",
      "crossvit_18_dagger_408.in1k\n",
      "crossvit_base_240.in1k\n",
      "crossvit_small_240.in1k\n",
      "crossvit_tiny_240.in1k\n",
      "cs3darknet_focus_l.c2ns_in1k\n",
      "cs3darknet_focus_m.c2ns_in1k\n",
      "cs3darknet_l.c2ns_in1k\n",
      "cs3darknet_m.c2ns_in1k\n",
      "cs3darknet_x.c2ns_in1k\n",
      "cs3edgenet_x.c2_in1k\n",
      "cs3se_edgenet_x.c2ns_in1k\n",
      "cs3sedarknet_l.c2ns_in1k\n",
      "cs3sedarknet_x.c2ns_in1k\n",
      "cspdarknet53.ra_in1k\n",
      "cspresnet50.ra_in1k\n",
      "cspresnext50.ra_in1k\n",
      "darknet53.c2ns_in1k\n",
      "darknetaa53.c2ns_in1k\n",
      "davit_base.msft_in1k\n",
      "davit_small.msft_in1k\n",
      "davit_tiny.msft_in1k\n",
      "deit3_base_patch16_224.fb_in1k\n",
      "deit3_base_patch16_224.fb_in22k_ft_in1k\n",
      "deit3_base_patch16_384.fb_in1k\n",
      "deit3_base_patch16_384.fb_in22k_ft_in1k\n",
      "deit3_huge_patch14_224.fb_in1k\n",
      "deit3_huge_patch14_224.fb_in22k_ft_in1k\n",
      "deit3_large_patch16_224.fb_in1k\n",
      "deit3_large_patch16_224.fb_in22k_ft_in1k\n",
      "deit3_large_patch16_384.fb_in1k\n",
      "deit3_large_patch16_384.fb_in22k_ft_in1k\n",
      "deit3_medium_patch16_224.fb_in1k\n",
      "deit3_medium_patch16_224.fb_in22k_ft_in1k\n",
      "deit3_small_patch16_224.fb_in1k\n",
      "deit3_small_patch16_224.fb_in22k_ft_in1k\n",
      "deit3_small_patch16_384.fb_in1k\n",
      "deit3_small_patch16_384.fb_in22k_ft_in1k\n",
      "deit_base_distilled_patch16_224.fb_in1k\n",
      "deit_base_distilled_patch16_384.fb_in1k\n",
      "deit_base_patch16_224.fb_in1k\n",
      "deit_base_patch16_384.fb_in1k\n",
      "deit_small_distilled_patch16_224.fb_in1k\n",
      "deit_small_patch16_224.fb_in1k\n",
      "deit_tiny_distilled_patch16_224.fb_in1k\n",
      "deit_tiny_patch16_224.fb_in1k\n",
      "densenet121.ra_in1k\n",
      "densenet121.tv_in1k\n",
      "densenet161.tv_in1k\n",
      "densenet169.tv_in1k\n",
      "densenet201.tv_in1k\n",
      "densenetblur121d.ra_in1k\n",
      "dla34.in1k\n",
      "dla46_c.in1k\n",
      "dla46x_c.in1k\n",
      "dla60.in1k\n",
      "dla60_res2net.in1k\n",
      "dla60_res2next.in1k\n",
      "dla60x.in1k\n",
      "dla60x_c.in1k\n",
      "dla102.in1k\n",
      "dla102x2.in1k\n",
      "dla102x.in1k\n",
      "dla169.in1k\n",
      "dm_nfnet_f0.dm_in1k\n",
      "dm_nfnet_f1.dm_in1k\n",
      "dm_nfnet_f2.dm_in1k\n",
      "dm_nfnet_f3.dm_in1k\n",
      "dm_nfnet_f4.dm_in1k\n",
      "dm_nfnet_f5.dm_in1k\n",
      "dm_nfnet_f6.dm_in1k\n",
      "dpn68.mx_in1k\n",
      "dpn68b.mx_in1k\n",
      "dpn68b.ra_in1k\n",
      "dpn92.mx_in1k\n",
      "dpn98.mx_in1k\n",
      "dpn107.mx_in1k\n",
      "dpn131.mx_in1k\n",
      "eca_botnext26ts_256.c1_in1k\n",
      "eca_halonext26ts.c1_in1k\n",
      "eca_nfnet_l0.ra2_in1k\n",
      "eca_nfnet_l1.ra2_in1k\n",
      "eca_nfnet_l2.ra3_in1k\n",
      "eca_resnet33ts.ra2_in1k\n",
      "eca_resnext26ts.ch_in1k\n",
      "ecaresnet26t.ra2_in1k\n",
      "ecaresnet50d.miil_in1k\n",
      "ecaresnet50d_pruned.miil_in1k\n",
      "ecaresnet50t.a1_in1k\n",
      "ecaresnet50t.a2_in1k\n",
      "ecaresnet50t.a3_in1k\n",
      "ecaresnet50t.ra2_in1k\n",
      "ecaresnet101d.miil_in1k\n",
      "ecaresnet101d_pruned.miil_in1k\n",
      "ecaresnet269d.ra2_in1k\n",
      "ecaresnetlight.miil_in1k\n",
      "edgenext_base.in21k_ft_in1k\n",
      "edgenext_base.usi_in1k\n",
      "edgenext_small.usi_in1k\n",
      "edgenext_small_rw.sw_in1k\n",
      "edgenext_x_small.in1k\n",
      "edgenext_xx_small.in1k\n",
      "efficientformer_l1.snap_dist_in1k\n",
      "efficientformer_l3.snap_dist_in1k\n",
      "efficientformer_l7.snap_dist_in1k\n",
      "efficientformerv2_l.snap_dist_in1k\n",
      "efficientformerv2_s0.snap_dist_in1k\n",
      "efficientformerv2_s1.snap_dist_in1k\n",
      "efficientformerv2_s2.snap_dist_in1k\n",
      "efficientnet_b0.ra_in1k\n",
      "efficientnet_b1.ft_in1k\n",
      "efficientnet_b1_pruned.in1k\n",
      "efficientnet_b2.ra_in1k\n",
      "efficientnet_b2_pruned.in1k\n",
      "efficientnet_b3.ra2_in1k\n",
      "efficientnet_b3_pruned.in1k\n",
      "efficientnet_b4.ra2_in1k\n",
      "efficientnet_b5.sw_in12k\n",
      "efficientnet_b5.sw_in12k_ft_in1k\n",
      "efficientnet_el.ra_in1k\n",
      "efficientnet_el_pruned.in1k\n",
      "efficientnet_em.ra2_in1k\n",
      "efficientnet_es.ra_in1k\n",
      "efficientnet_es_pruned.in1k\n",
      "efficientnet_lite0.ra_in1k\n",
      "efficientnetv2_rw_m.agc_in1k\n",
      "efficientnetv2_rw_s.ra2_in1k\n",
      "efficientnetv2_rw_t.ra2_in1k\n",
      "efficientvit_b0.r224_in1k\n",
      "efficientvit_b1.r224_in1k\n",
      "efficientvit_b1.r256_in1k\n",
      "efficientvit_b1.r288_in1k\n",
      "efficientvit_b2.r224_in1k\n",
      "efficientvit_b2.r256_in1k\n",
      "efficientvit_b2.r288_in1k\n",
      "efficientvit_b3.r224_in1k\n",
      "efficientvit_b3.r256_in1k\n",
      "efficientvit_b3.r288_in1k\n",
      "efficientvit_l1.r224_in1k\n",
      "efficientvit_l2.r224_in1k\n",
      "efficientvit_l2.r256_in1k\n",
      "efficientvit_l2.r288_in1k\n",
      "efficientvit_l2.r384_in1k\n",
      "efficientvit_l3.r224_in1k\n",
      "efficientvit_l3.r256_in1k\n",
      "efficientvit_l3.r320_in1k\n",
      "efficientvit_l3.r384_in1k\n",
      "efficientvit_m0.r224_in1k\n",
      "efficientvit_m1.r224_in1k\n",
      "efficientvit_m2.r224_in1k\n",
      "efficientvit_m3.r224_in1k\n",
      "efficientvit_m4.r224_in1k\n",
      "efficientvit_m5.r224_in1k\n",
      "ese_vovnet19b_dw.ra_in1k\n",
      "ese_vovnet39b.ra_in1k\n",
      "eva02_base_patch14_224.mim_in22k\n",
      "eva02_base_patch14_448.mim_in22k_ft_in1k\n",
      "eva02_base_patch14_448.mim_in22k_ft_in22k\n",
      "eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\n",
      "eva02_base_patch16_clip_224.merged2b\n",
      "eva02_enormous_patch14_clip_224.laion2b\n",
      "eva02_enormous_patch14_clip_224.laion2b_plus\n",
      "eva02_large_patch14_224.mim_in22k\n",
      "eva02_large_patch14_224.mim_m38m\n",
      "eva02_large_patch14_448.mim_in22k_ft_in1k\n",
      "eva02_large_patch14_448.mim_in22k_ft_in22k\n",
      "eva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n",
      "eva02_large_patch14_448.mim_m38m_ft_in1k\n",
      "eva02_large_patch14_448.mim_m38m_ft_in22k\n",
      "eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n",
      "eva02_large_patch14_clip_224.merged2b\n",
      "eva02_large_patch14_clip_336.merged2b\n",
      "eva02_small_patch14_224.mim_in22k\n",
      "eva02_small_patch14_336.mim_in22k_ft_in1k\n",
      "eva02_tiny_patch14_224.mim_in22k\n",
      "eva02_tiny_patch14_336.mim_in22k_ft_in1k\n",
      "eva_giant_patch14_224.clip_ft_in1k\n",
      "eva_giant_patch14_336.clip_ft_in1k\n",
      "eva_giant_patch14_336.m30m_ft_in22k_in1k\n",
      "eva_giant_patch14_560.m30m_ft_in22k_in1k\n",
      "eva_giant_patch14_clip_224.laion400m\n",
      "eva_giant_patch14_clip_224.merged2b\n",
      "eva_large_patch14_196.in22k_ft_in1k\n",
      "eva_large_patch14_196.in22k_ft_in22k_in1k\n",
      "eva_large_patch14_336.in22k_ft_in1k\n",
      "eva_large_patch14_336.in22k_ft_in22k_in1k\n",
      "fastvit_ma36.apple_dist_in1k\n",
      "fastvit_ma36.apple_in1k\n",
      "fastvit_s12.apple_dist_in1k\n",
      "fastvit_s12.apple_in1k\n",
      "fastvit_sa12.apple_dist_in1k\n",
      "fastvit_sa12.apple_in1k\n",
      "fastvit_sa24.apple_dist_in1k\n",
      "fastvit_sa24.apple_in1k\n",
      "fastvit_sa36.apple_dist_in1k\n",
      "fastvit_sa36.apple_in1k\n",
      "fastvit_t8.apple_dist_in1k\n",
      "fastvit_t8.apple_in1k\n",
      "fastvit_t12.apple_dist_in1k\n",
      "fastvit_t12.apple_in1k\n",
      "fbnetc_100.rmsp_in1k\n",
      "fbnetv3_b.ra2_in1k\n",
      "fbnetv3_d.ra2_in1k\n",
      "fbnetv3_g.ra2_in1k\n",
      "flexivit_base.300ep_in1k\n",
      "flexivit_base.300ep_in21k\n",
      "flexivit_base.600ep_in1k\n",
      "flexivit_base.1000ep_in21k\n",
      "flexivit_base.1200ep_in1k\n",
      "flexivit_base.patch16_in21k\n",
      "flexivit_base.patch30_in21k\n",
      "flexivit_large.300ep_in1k\n",
      "flexivit_large.600ep_in1k\n",
      "flexivit_large.1200ep_in1k\n",
      "flexivit_small.300ep_in1k\n",
      "flexivit_small.600ep_in1k\n",
      "flexivit_small.1200ep_in1k\n",
      "focalnet_base_lrf.ms_in1k\n",
      "focalnet_base_srf.ms_in1k\n",
      "focalnet_huge_fl3.ms_in22k\n",
      "focalnet_huge_fl4.ms_in22k\n",
      "focalnet_large_fl3.ms_in22k\n",
      "focalnet_large_fl4.ms_in22k\n",
      "focalnet_small_lrf.ms_in1k\n",
      "focalnet_small_srf.ms_in1k\n",
      "focalnet_tiny_lrf.ms_in1k\n",
      "focalnet_tiny_srf.ms_in1k\n",
      "focalnet_xlarge_fl3.ms_in22k\n",
      "focalnet_xlarge_fl4.ms_in22k\n",
      "gc_efficientnetv2_rw_t.agc_in1k\n",
      "gcresnet33ts.ra2_in1k\n",
      "gcresnet50t.ra2_in1k\n",
      "gcresnext26ts.ch_in1k\n",
      "gcresnext50ts.ch_in1k\n",
      "gcvit_base.in1k\n",
      "gcvit_small.in1k\n",
      "gcvit_tiny.in1k\n",
      "gcvit_xtiny.in1k\n",
      "gcvit_xxtiny.in1k\n",
      "gernet_l.idstcv_in1k\n",
      "gernet_m.idstcv_in1k\n",
      "gernet_s.idstcv_in1k\n",
      "ghostnet_100.in1k\n",
      "ghostnetv2_100.in1k\n",
      "ghostnetv2_130.in1k\n",
      "ghostnetv2_160.in1k\n",
      "gmixer_24_224.ra3_in1k\n",
      "gmlp_s16_224.ra3_in1k\n",
      "halo2botnet50ts_256.a1h_in1k\n",
      "halonet26t.a1h_in1k\n",
      "halonet50ts.a1h_in1k\n",
      "haloregnetz_b.ra3_in1k\n",
      "hardcorenas_a.miil_green_in1k\n",
      "hardcorenas_b.miil_green_in1k\n",
      "hardcorenas_c.miil_green_in1k\n",
      "hardcorenas_d.miil_green_in1k\n",
      "hardcorenas_e.miil_green_in1k\n",
      "hardcorenas_f.miil_green_in1k\n",
      "hgnet_base.ssld_in1k\n",
      "hgnet_small.paddle_in1k\n",
      "hgnet_small.ssld_in1k\n",
      "hgnet_tiny.paddle_in1k\n",
      "hgnet_tiny.ssld_in1k\n",
      "hgnetv2_b0.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b0.ssld_stage2_ft_in1k\n",
      "hgnetv2_b1.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b1.ssld_stage2_ft_in1k\n",
      "hgnetv2_b2.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b2.ssld_stage2_ft_in1k\n",
      "hgnetv2_b3.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b3.ssld_stage2_ft_in1k\n",
      "hgnetv2_b4.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b4.ssld_stage2_ft_in1k\n",
      "hgnetv2_b5.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b5.ssld_stage2_ft_in1k\n",
      "hgnetv2_b6.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b6.ssld_stage2_ft_in1k\n",
      "hiera_base_224.mae\n",
      "hiera_base_224.mae_in1k_ft_in1k\n",
      "hiera_base_plus_224.mae\n",
      "hiera_base_plus_224.mae_in1k_ft_in1k\n",
      "hiera_huge_224.mae\n",
      "hiera_huge_224.mae_in1k_ft_in1k\n",
      "hiera_large_224.mae\n",
      "hiera_large_224.mae_in1k_ft_in1k\n",
      "hiera_small_224.mae\n",
      "hiera_small_224.mae_in1k_ft_in1k\n",
      "hiera_tiny_224.mae\n",
      "hiera_tiny_224.mae_in1k_ft_in1k\n",
      "hrnet_w18.ms_aug_in1k\n",
      "hrnet_w18.ms_in1k\n",
      "hrnet_w18_small.gluon_in1k\n",
      "hrnet_w18_small.ms_in1k\n",
      "hrnet_w18_small_v2.gluon_in1k\n",
      "hrnet_w18_small_v2.ms_in1k\n",
      "hrnet_w18_ssld.paddle_in1k\n",
      "hrnet_w30.ms_in1k\n",
      "hrnet_w32.ms_in1k\n",
      "hrnet_w40.ms_in1k\n",
      "hrnet_w44.ms_in1k\n",
      "hrnet_w48.ms_in1k\n",
      "hrnet_w48_ssld.paddle_in1k\n",
      "hrnet_w64.ms_in1k\n",
      "inception_next_base.sail_in1k\n",
      "inception_next_base.sail_in1k_384\n",
      "inception_next_small.sail_in1k\n",
      "inception_next_tiny.sail_in1k\n",
      "inception_resnet_v2.tf_ens_adv_in1k\n",
      "inception_resnet_v2.tf_in1k\n",
      "inception_v3.gluon_in1k\n",
      "inception_v3.tf_adv_in1k\n",
      "inception_v3.tf_in1k\n",
      "inception_v3.tv_in1k\n",
      "inception_v4.tf_in1k\n",
      "lambda_resnet26rpt_256.c1_in1k\n",
      "lambda_resnet26t.c1_in1k\n",
      "lambda_resnet50ts.a1h_in1k\n",
      "lamhalobotnet50ts_256.a1h_in1k\n",
      "lcnet_050.ra2_in1k\n",
      "lcnet_075.ra2_in1k\n",
      "lcnet_100.ra2_in1k\n",
      "legacy_senet154.in1k\n",
      "legacy_seresnet18.in1k\n",
      "legacy_seresnet34.in1k\n",
      "legacy_seresnet50.in1k\n",
      "legacy_seresnet101.in1k\n",
      "legacy_seresnet152.in1k\n",
      "legacy_seresnext26_32x4d.in1k\n",
      "legacy_seresnext50_32x4d.in1k\n",
      "legacy_seresnext101_32x4d.in1k\n",
      "legacy_xception.tf_in1k\n",
      "levit_128.fb_dist_in1k\n",
      "levit_128s.fb_dist_in1k\n",
      "levit_192.fb_dist_in1k\n",
      "levit_256.fb_dist_in1k\n",
      "levit_384.fb_dist_in1k\n",
      "levit_conv_128.fb_dist_in1k\n",
      "levit_conv_128s.fb_dist_in1k\n",
      "levit_conv_192.fb_dist_in1k\n",
      "levit_conv_256.fb_dist_in1k\n",
      "levit_conv_384.fb_dist_in1k\n",
      "maxvit_base_tf_224.in1k\n",
      "maxvit_base_tf_224.in21k\n",
      "maxvit_base_tf_384.in1k\n",
      "maxvit_base_tf_384.in21k_ft_in1k\n",
      "maxvit_base_tf_512.in1k\n",
      "maxvit_base_tf_512.in21k_ft_in1k\n",
      "maxvit_large_tf_224.in1k\n",
      "maxvit_large_tf_224.in21k\n",
      "maxvit_large_tf_384.in1k\n",
      "maxvit_large_tf_384.in21k_ft_in1k\n",
      "maxvit_large_tf_512.in1k\n",
      "maxvit_large_tf_512.in21k_ft_in1k\n",
      "maxvit_nano_rw_256.sw_in1k\n",
      "maxvit_rmlp_base_rw_224.sw_in12k\n",
      "maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k\n",
      "maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k\n",
      "maxvit_rmlp_nano_rw_256.sw_in1k\n",
      "maxvit_rmlp_pico_rw_256.sw_in1k\n",
      "maxvit_rmlp_small_rw_224.sw_in1k\n",
      "maxvit_rmlp_tiny_rw_256.sw_in1k\n",
      "maxvit_small_tf_224.in1k\n",
      "maxvit_small_tf_384.in1k\n",
      "maxvit_small_tf_512.in1k\n",
      "maxvit_tiny_rw_224.sw_in1k\n",
      "maxvit_tiny_tf_224.in1k\n",
      "maxvit_tiny_tf_384.in1k\n",
      "maxvit_tiny_tf_512.in1k\n",
      "maxvit_xlarge_tf_224.in21k\n",
      "maxvit_xlarge_tf_384.in21k_ft_in1k\n",
      "maxvit_xlarge_tf_512.in21k_ft_in1k\n",
      "maxxvit_rmlp_nano_rw_256.sw_in1k\n",
      "maxxvit_rmlp_small_rw_256.sw_in1k\n",
      "maxxvitv2_nano_rw_256.sw_in1k\n",
      "maxxvitv2_rmlp_base_rw_224.sw_in12k\n",
      "maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k\n",
      "maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k\n",
      "mixer_b16_224.goog_in21k\n",
      "mixer_b16_224.goog_in21k_ft_in1k\n",
      "mixer_b16_224.miil_in21k\n",
      "mixer_b16_224.miil_in21k_ft_in1k\n",
      "mixer_l16_224.goog_in21k\n",
      "mixer_l16_224.goog_in21k_ft_in1k\n",
      "mixnet_l.ft_in1k\n",
      "mixnet_m.ft_in1k\n",
      "mixnet_s.ft_in1k\n",
      "mixnet_xl.ra_in1k\n",
      "mnasnet_100.rmsp_in1k\n",
      "mnasnet_small.lamb_in1k\n",
      "mobilenetv2_050.lamb_in1k\n",
      "mobilenetv2_100.ra_in1k\n",
      "mobilenetv2_110d.ra_in1k\n",
      "mobilenetv2_120d.ra_in1k\n",
      "mobilenetv2_140.ra_in1k\n",
      "mobilenetv3_large_100.miil_in21k\n",
      "mobilenetv3_large_100.miil_in21k_ft_in1k\n",
      "mobilenetv3_large_100.ra_in1k\n",
      "mobilenetv3_rw.rmsp_in1k\n",
      "mobilenetv3_small_050.lamb_in1k\n",
      "mobilenetv3_small_075.lamb_in1k\n",
      "mobilenetv3_small_100.lamb_in1k\n",
      "mobileone_s0.apple_in1k\n",
      "mobileone_s1.apple_in1k\n",
      "mobileone_s2.apple_in1k\n",
      "mobileone_s3.apple_in1k\n",
      "mobileone_s4.apple_in1k\n",
      "mobilevit_s.cvnets_in1k\n",
      "mobilevit_xs.cvnets_in1k\n",
      "mobilevit_xxs.cvnets_in1k\n",
      "mobilevitv2_050.cvnets_in1k\n",
      "mobilevitv2_075.cvnets_in1k\n",
      "mobilevitv2_100.cvnets_in1k\n",
      "mobilevitv2_125.cvnets_in1k\n",
      "mobilevitv2_150.cvnets_in1k\n",
      "mobilevitv2_150.cvnets_in22k_ft_in1k\n",
      "mobilevitv2_150.cvnets_in22k_ft_in1k_384\n",
      "mobilevitv2_175.cvnets_in1k\n",
      "mobilevitv2_175.cvnets_in22k_ft_in1k\n",
      "mobilevitv2_175.cvnets_in22k_ft_in1k_384\n",
      "mobilevitv2_200.cvnets_in1k\n",
      "mobilevitv2_200.cvnets_in22k_ft_in1k\n",
      "mobilevitv2_200.cvnets_in22k_ft_in1k_384\n",
      "mvitv2_base.fb_in1k\n",
      "mvitv2_base_cls.fb_inw21k\n",
      "mvitv2_huge_cls.fb_inw21k\n",
      "mvitv2_large.fb_in1k\n",
      "mvitv2_large_cls.fb_inw21k\n",
      "mvitv2_small.fb_in1k\n",
      "mvitv2_tiny.fb_in1k\n",
      "nasnetalarge.tf_in1k\n",
      "nest_base_jx.goog_in1k\n",
      "nest_small_jx.goog_in1k\n",
      "nest_tiny_jx.goog_in1k\n",
      "nextvit_base.bd_in1k\n",
      "nextvit_base.bd_in1k_384\n",
      "nextvit_base.bd_ssld_6m_in1k\n",
      "nextvit_base.bd_ssld_6m_in1k_384\n",
      "nextvit_large.bd_in1k\n",
      "nextvit_large.bd_in1k_384\n",
      "nextvit_large.bd_ssld_6m_in1k\n",
      "nextvit_large.bd_ssld_6m_in1k_384\n",
      "nextvit_small.bd_in1k\n",
      "nextvit_small.bd_in1k_384\n",
      "nextvit_small.bd_ssld_6m_in1k\n",
      "nextvit_small.bd_ssld_6m_in1k_384\n",
      "nf_regnet_b1.ra2_in1k\n",
      "nf_resnet50.ra2_in1k\n",
      "nfnet_l0.ra2_in1k\n",
      "pit_b_224.in1k\n",
      "pit_b_distilled_224.in1k\n",
      "pit_s_224.in1k\n",
      "pit_s_distilled_224.in1k\n",
      "pit_ti_224.in1k\n",
      "pit_ti_distilled_224.in1k\n",
      "pit_xs_224.in1k\n",
      "pit_xs_distilled_224.in1k\n",
      "pnasnet5large.tf_in1k\n",
      "poolformer_m36.sail_in1k\n",
      "poolformer_m48.sail_in1k\n",
      "poolformer_s12.sail_in1k\n",
      "poolformer_s24.sail_in1k\n",
      "poolformer_s36.sail_in1k\n",
      "poolformerv2_m36.sail_in1k\n",
      "poolformerv2_m48.sail_in1k\n",
      "poolformerv2_s12.sail_in1k\n",
      "poolformerv2_s24.sail_in1k\n",
      "poolformerv2_s36.sail_in1k\n",
      "pvt_v2_b0.in1k\n",
      "pvt_v2_b1.in1k\n",
      "pvt_v2_b2.in1k\n",
      "pvt_v2_b2_li.in1k\n",
      "pvt_v2_b3.in1k\n",
      "pvt_v2_b4.in1k\n",
      "pvt_v2_b5.in1k\n",
      "regnetv_040.ra3_in1k\n",
      "regnetv_064.ra3_in1k\n",
      "regnetx_002.pycls_in1k\n",
      "regnetx_004.pycls_in1k\n",
      "regnetx_004_tv.tv2_in1k\n",
      "regnetx_006.pycls_in1k\n",
      "regnetx_008.pycls_in1k\n",
      "regnetx_008.tv2_in1k\n",
      "regnetx_016.pycls_in1k\n",
      "regnetx_016.tv2_in1k\n",
      "regnetx_032.pycls_in1k\n",
      "regnetx_032.tv2_in1k\n",
      "regnetx_040.pycls_in1k\n",
      "regnetx_064.pycls_in1k\n",
      "regnetx_080.pycls_in1k\n",
      "regnetx_080.tv2_in1k\n",
      "regnetx_120.pycls_in1k\n",
      "regnetx_160.pycls_in1k\n",
      "regnetx_160.tv2_in1k\n",
      "regnetx_320.pycls_in1k\n",
      "regnetx_320.tv2_in1k\n",
      "regnety_002.pycls_in1k\n",
      "regnety_004.pycls_in1k\n",
      "regnety_004.tv2_in1k\n",
      "regnety_006.pycls_in1k\n",
      "regnety_008.pycls_in1k\n",
      "regnety_008_tv.tv2_in1k\n",
      "regnety_016.pycls_in1k\n",
      "regnety_016.tv2_in1k\n",
      "regnety_032.pycls_in1k\n",
      "regnety_032.ra_in1k\n",
      "regnety_032.tv2_in1k\n",
      "regnety_040.pycls_in1k\n",
      "regnety_040.ra3_in1k\n",
      "regnety_064.pycls_in1k\n",
      "regnety_064.ra3_in1k\n",
      "regnety_080.pycls_in1k\n",
      "regnety_080.ra3_in1k\n",
      "regnety_080_tv.tv2_in1k\n",
      "regnety_120.pycls_in1k\n",
      "regnety_120.sw_in12k\n",
      "regnety_120.sw_in12k_ft_in1k\n",
      "regnety_160.deit_in1k\n",
      "regnety_160.lion_in12k_ft_in1k\n",
      "regnety_160.pycls_in1k\n",
      "regnety_160.sw_in12k\n",
      "regnety_160.sw_in12k_ft_in1k\n",
      "regnety_160.swag_ft_in1k\n",
      "regnety_160.swag_lc_in1k\n",
      "regnety_160.tv2_in1k\n",
      "regnety_320.pycls_in1k\n",
      "regnety_320.seer\n",
      "regnety_320.seer_ft_in1k\n",
      "regnety_320.swag_ft_in1k\n",
      "regnety_320.swag_lc_in1k\n",
      "regnety_320.tv2_in1k\n",
      "regnety_640.seer\n",
      "regnety_640.seer_ft_in1k\n",
      "regnety_1280.seer\n",
      "regnety_1280.seer_ft_in1k\n",
      "regnety_1280.swag_ft_in1k\n",
      "regnety_1280.swag_lc_in1k\n",
      "regnety_2560.seer_ft_in1k\n",
      "regnetz_040.ra3_in1k\n",
      "regnetz_040_h.ra3_in1k\n",
      "regnetz_b16.ra3_in1k\n",
      "regnetz_c16.ra3_in1k\n",
      "regnetz_c16_evos.ch_in1k\n",
      "regnetz_d8.ra3_in1k\n",
      "regnetz_d8_evos.ch_in1k\n",
      "regnetz_d32.ra3_in1k\n",
      "regnetz_e8.ra3_in1k\n",
      "repghostnet_050.in1k\n",
      "repghostnet_058.in1k\n",
      "repghostnet_080.in1k\n",
      "repghostnet_100.in1k\n",
      "repghostnet_111.in1k\n",
      "repghostnet_130.in1k\n",
      "repghostnet_150.in1k\n",
      "repghostnet_200.in1k\n",
      "repvgg_a0.rvgg_in1k\n",
      "repvgg_a1.rvgg_in1k\n",
      "repvgg_a2.rvgg_in1k\n",
      "repvgg_b0.rvgg_in1k\n",
      "repvgg_b1.rvgg_in1k\n",
      "repvgg_b1g4.rvgg_in1k\n",
      "repvgg_b2.rvgg_in1k\n",
      "repvgg_b2g4.rvgg_in1k\n",
      "repvgg_b3.rvgg_in1k\n",
      "repvgg_b3g4.rvgg_in1k\n",
      "repvgg_d2se.rvgg_in1k\n",
      "repvit_m0_9.dist_300e_in1k\n",
      "repvit_m0_9.dist_450e_in1k\n",
      "repvit_m1.dist_in1k\n",
      "repvit_m1_0.dist_300e_in1k\n",
      "repvit_m1_0.dist_450e_in1k\n",
      "repvit_m1_1.dist_300e_in1k\n",
      "repvit_m1_1.dist_450e_in1k\n",
      "repvit_m1_5.dist_300e_in1k\n",
      "repvit_m1_5.dist_450e_in1k\n",
      "repvit_m2.dist_in1k\n",
      "repvit_m2_3.dist_300e_in1k\n",
      "repvit_m2_3.dist_450e_in1k\n",
      "repvit_m3.dist_in1k\n",
      "res2net50_14w_8s.in1k\n",
      "res2net50_26w_4s.in1k\n",
      "res2net50_26w_6s.in1k\n",
      "res2net50_26w_8s.in1k\n",
      "res2net50_48w_2s.in1k\n",
      "res2net50d.in1k\n",
      "res2net101_26w_4s.in1k\n",
      "res2net101d.in1k\n",
      "res2next50.in1k\n",
      "resmlp_12_224.fb_dino\n",
      "resmlp_12_224.fb_distilled_in1k\n",
      "resmlp_12_224.fb_in1k\n",
      "resmlp_24_224.fb_dino\n",
      "resmlp_24_224.fb_distilled_in1k\n",
      "resmlp_24_224.fb_in1k\n",
      "resmlp_36_224.fb_distilled_in1k\n",
      "resmlp_36_224.fb_in1k\n",
      "resmlp_big_24_224.fb_distilled_in1k\n",
      "resmlp_big_24_224.fb_in1k\n",
      "resmlp_big_24_224.fb_in22k_ft_in1k\n",
      "resnest14d.gluon_in1k\n",
      "resnest26d.gluon_in1k\n",
      "resnest50d.in1k\n",
      "resnest50d_1s4x24d.in1k\n",
      "resnest50d_4s2x40d.in1k\n",
      "resnest101e.in1k\n",
      "resnest200e.in1k\n",
      "resnest269e.in1k\n",
      "resnet10t.c3_in1k\n",
      "resnet14t.c3_in1k\n",
      "resnet18.a1_in1k\n",
      "resnet18.a2_in1k\n",
      "resnet18.a3_in1k\n",
      "resnet18.fb_ssl_yfcc100m_ft_in1k\n",
      "resnet18.fb_swsl_ig1b_ft_in1k\n",
      "resnet18.gluon_in1k\n",
      "resnet18.tv_in1k\n",
      "resnet18d.ra2_in1k\n",
      "resnet26.bt_in1k\n",
      "resnet26d.bt_in1k\n",
      "resnet26t.ra2_in1k\n",
      "resnet32ts.ra2_in1k\n",
      "resnet33ts.ra2_in1k\n",
      "resnet34.a1_in1k\n",
      "resnet34.a2_in1k\n",
      "resnet34.a3_in1k\n",
      "resnet34.bt_in1k\n",
      "resnet34.gluon_in1k\n",
      "resnet34.tv_in1k\n",
      "resnet34d.ra2_in1k\n",
      "resnet50.a1_in1k\n",
      "resnet50.a1h_in1k\n",
      "resnet50.a2_in1k\n",
      "resnet50.a3_in1k\n",
      "resnet50.am_in1k\n",
      "resnet50.b1k_in1k\n",
      "resnet50.b2k_in1k\n",
      "resnet50.bt_in1k\n",
      "resnet50.c1_in1k\n",
      "resnet50.c2_in1k\n",
      "resnet50.d_in1k\n",
      "resnet50.fb_ssl_yfcc100m_ft_in1k\n",
      "resnet50.fb_swsl_ig1b_ft_in1k\n",
      "resnet50.gluon_in1k\n",
      "resnet50.ra_in1k\n",
      "resnet50.ram_in1k\n",
      "resnet50.tv2_in1k\n",
      "resnet50.tv_in1k\n",
      "resnet50_gn.a1h_in1k\n",
      "resnet50c.gluon_in1k\n",
      "resnet50d.a1_in1k\n",
      "resnet50d.a2_in1k\n",
      "resnet50d.a3_in1k\n",
      "resnet50d.gluon_in1k\n",
      "resnet50d.ra2_in1k\n",
      "resnet50s.gluon_in1k\n",
      "resnet51q.ra2_in1k\n",
      "resnet61q.ra2_in1k\n",
      "resnet101.a1_in1k\n",
      "resnet101.a1h_in1k\n",
      "resnet101.a2_in1k\n",
      "resnet101.a3_in1k\n",
      "resnet101.gluon_in1k\n",
      "resnet101.tv2_in1k\n",
      "resnet101.tv_in1k\n",
      "resnet101c.gluon_in1k\n",
      "resnet101d.gluon_in1k\n",
      "resnet101d.ra2_in1k\n",
      "resnet101s.gluon_in1k\n",
      "resnet152.a1_in1k\n",
      "resnet152.a1h_in1k\n",
      "resnet152.a2_in1k\n",
      "resnet152.a3_in1k\n",
      "resnet152.gluon_in1k\n",
      "resnet152.tv2_in1k\n",
      "resnet152.tv_in1k\n",
      "resnet152c.gluon_in1k\n",
      "resnet152d.gluon_in1k\n",
      "resnet152d.ra2_in1k\n",
      "resnet152s.gluon_in1k\n",
      "resnet200d.ra2_in1k\n",
      "resnetaa50.a1h_in1k\n",
      "resnetaa50d.d_in12k\n",
      "resnetaa50d.sw_in12k\n",
      "resnetaa50d.sw_in12k_ft_in1k\n",
      "resnetaa101d.sw_in12k\n",
      "resnetaa101d.sw_in12k_ft_in1k\n",
      "resnetblur50.bt_in1k\n",
      "resnetrs50.tf_in1k\n",
      "resnetrs101.tf_in1k\n",
      "resnetrs152.tf_in1k\n",
      "resnetrs200.tf_in1k\n",
      "resnetrs270.tf_in1k\n",
      "resnetrs350.tf_in1k\n",
      "resnetrs420.tf_in1k\n",
      "resnetv2_50.a1h_in1k\n",
      "resnetv2_50d_evos.ah_in1k\n",
      "resnetv2_50d_gn.ah_in1k\n",
      "resnetv2_50x1_bit.goog_distilled_in1k\n",
      "resnetv2_50x1_bit.goog_in21k\n",
      "resnetv2_50x1_bit.goog_in21k_ft_in1k\n",
      "resnetv2_50x3_bit.goog_in21k\n",
      "resnetv2_50x3_bit.goog_in21k_ft_in1k\n",
      "resnetv2_101.a1h_in1k\n",
      "resnetv2_101x1_bit.goog_in21k\n",
      "resnetv2_101x1_bit.goog_in21k_ft_in1k\n",
      "resnetv2_101x3_bit.goog_in21k\n",
      "resnetv2_101x3_bit.goog_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_in21k\n",
      "resnetv2_152x2_bit.goog_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384\n",
      "resnetv2_152x4_bit.goog_in21k\n",
      "resnetv2_152x4_bit.goog_in21k_ft_in1k\n",
      "resnext26ts.ra2_in1k\n",
      "resnext50_32x4d.a1_in1k\n",
      "resnext50_32x4d.a1h_in1k\n",
      "resnext50_32x4d.a2_in1k\n",
      "resnext50_32x4d.a3_in1k\n",
      "resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k\n",
      "resnext50_32x4d.fb_swsl_ig1b_ft_in1k\n",
      "resnext50_32x4d.gluon_in1k\n",
      "resnext50_32x4d.ra_in1k\n",
      "resnext50_32x4d.tv2_in1k\n",
      "resnext50_32x4d.tv_in1k\n",
      "resnext50d_32x4d.bt_in1k\n",
      "resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k\n",
      "resnext101_32x4d.fb_swsl_ig1b_ft_in1k\n",
      "resnext101_32x4d.gluon_in1k\n",
      "resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k\n",
      "resnext101_32x8d.fb_swsl_ig1b_ft_in1k\n",
      "resnext101_32x8d.fb_wsl_ig1b_ft_in1k\n",
      "resnext101_32x8d.tv2_in1k\n",
      "resnext101_32x8d.tv_in1k\n",
      "resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k\n",
      "resnext101_32x16d.fb_swsl_ig1b_ft_in1k\n",
      "resnext101_32x16d.fb_wsl_ig1b_ft_in1k\n",
      "resnext101_32x32d.fb_wsl_ig1b_ft_in1k\n",
      "resnext101_64x4d.c1_in1k\n",
      "resnext101_64x4d.gluon_in1k\n",
      "resnext101_64x4d.tv_in1k\n",
      "rexnet_100.nav_in1k\n",
      "rexnet_130.nav_in1k\n",
      "rexnet_150.nav_in1k\n",
      "rexnet_200.nav_in1k\n",
      "rexnet_300.nav_in1k\n",
      "rexnetr_200.sw_in12k\n",
      "rexnetr_200.sw_in12k_ft_in1k\n",
      "rexnetr_300.sw_in12k\n",
      "rexnetr_300.sw_in12k_ft_in1k\n",
      "samvit_base_patch16.sa1b\n",
      "samvit_huge_patch16.sa1b\n",
      "samvit_large_patch16.sa1b\n",
      "sebotnet33ts_256.a1h_in1k\n",
      "sehalonet33ts.ra2_in1k\n",
      "selecsls42b.in1k\n",
      "selecsls60.in1k\n",
      "selecsls60b.in1k\n",
      "semnasnet_075.rmsp_in1k\n",
      "semnasnet_100.rmsp_in1k\n",
      "senet154.gluon_in1k\n",
      "sequencer2d_l.in1k\n",
      "sequencer2d_m.in1k\n",
      "sequencer2d_s.in1k\n",
      "seresnet33ts.ra2_in1k\n",
      "seresnet50.a1_in1k\n",
      "seresnet50.a2_in1k\n",
      "seresnet50.a3_in1k\n",
      "seresnet50.ra2_in1k\n",
      "seresnet152d.ra2_in1k\n",
      "seresnext26d_32x4d.bt_in1k\n",
      "seresnext26t_32x4d.bt_in1k\n",
      "seresnext26ts.ch_in1k\n",
      "seresnext50_32x4d.gluon_in1k\n",
      "seresnext50_32x4d.racm_in1k\n",
      "seresnext101_32x4d.gluon_in1k\n",
      "seresnext101_32x8d.ah_in1k\n",
      "seresnext101_64x4d.gluon_in1k\n",
      "seresnext101d_32x8d.ah_in1k\n",
      "seresnextaa101d_32x8d.ah_in1k\n",
      "seresnextaa101d_32x8d.sw_in12k\n",
      "seresnextaa101d_32x8d.sw_in12k_ft_in1k\n",
      "seresnextaa101d_32x8d.sw_in12k_ft_in1k_288\n",
      "seresnextaa201d_32x8d.sw_in12k\n",
      "seresnextaa201d_32x8d.sw_in12k_ft_in1k_384\n",
      "skresnet18.ra_in1k\n",
      "skresnet34.ra_in1k\n",
      "skresnext50_32x4d.ra_in1k\n",
      "spnasnet_100.rmsp_in1k\n",
      "swin_base_patch4_window7_224.ms_in1k\n",
      "swin_base_patch4_window7_224.ms_in22k\n",
      "swin_base_patch4_window7_224.ms_in22k_ft_in1k\n",
      "swin_base_patch4_window12_384.ms_in1k\n",
      "swin_base_patch4_window12_384.ms_in22k\n",
      "swin_base_patch4_window12_384.ms_in22k_ft_in1k\n",
      "swin_large_patch4_window7_224.ms_in22k\n",
      "swin_large_patch4_window7_224.ms_in22k_ft_in1k\n",
      "swin_large_patch4_window12_384.ms_in22k\n",
      "swin_large_patch4_window12_384.ms_in22k_ft_in1k\n",
      "swin_s3_base_224.ms_in1k\n",
      "swin_s3_small_224.ms_in1k\n",
      "swin_s3_tiny_224.ms_in1k\n",
      "swin_small_patch4_window7_224.ms_in1k\n",
      "swin_small_patch4_window7_224.ms_in22k\n",
      "swin_small_patch4_window7_224.ms_in22k_ft_in1k\n",
      "swin_tiny_patch4_window7_224.ms_in1k\n",
      "swin_tiny_patch4_window7_224.ms_in22k\n",
      "swin_tiny_patch4_window7_224.ms_in22k_ft_in1k\n",
      "swinv2_base_window8_256.ms_in1k\n",
      "swinv2_base_window12_192.ms_in22k\n",
      "swinv2_base_window12to16_192to256.ms_in22k_ft_in1k\n",
      "swinv2_base_window12to24_192to384.ms_in22k_ft_in1k\n",
      "swinv2_base_window16_256.ms_in1k\n",
      "swinv2_cr_small_224.sw_in1k\n",
      "swinv2_cr_small_ns_224.sw_in1k\n",
      "swinv2_cr_tiny_ns_224.sw_in1k\n",
      "swinv2_large_window12_192.ms_in22k\n",
      "swinv2_large_window12to16_192to256.ms_in22k_ft_in1k\n",
      "swinv2_large_window12to24_192to384.ms_in22k_ft_in1k\n",
      "swinv2_small_window8_256.ms_in1k\n",
      "swinv2_small_window16_256.ms_in1k\n",
      "swinv2_tiny_window8_256.ms_in1k\n",
      "swinv2_tiny_window16_256.ms_in1k\n",
      "tf_efficientnet_b0.aa_in1k\n",
      "tf_efficientnet_b0.ap_in1k\n",
      "tf_efficientnet_b0.in1k\n",
      "tf_efficientnet_b0.ns_jft_in1k\n",
      "tf_efficientnet_b1.aa_in1k\n",
      "tf_efficientnet_b1.ap_in1k\n",
      "tf_efficientnet_b1.in1k\n",
      "tf_efficientnet_b1.ns_jft_in1k\n",
      "tf_efficientnet_b2.aa_in1k\n",
      "tf_efficientnet_b2.ap_in1k\n",
      "tf_efficientnet_b2.in1k\n",
      "tf_efficientnet_b2.ns_jft_in1k\n",
      "tf_efficientnet_b3.aa_in1k\n",
      "tf_efficientnet_b3.ap_in1k\n",
      "tf_efficientnet_b3.in1k\n",
      "tf_efficientnet_b3.ns_jft_in1k\n",
      "tf_efficientnet_b4.aa_in1k\n",
      "tf_efficientnet_b4.ap_in1k\n",
      "tf_efficientnet_b4.in1k\n",
      "tf_efficientnet_b4.ns_jft_in1k\n",
      "tf_efficientnet_b5.aa_in1k\n",
      "tf_efficientnet_b5.ap_in1k\n",
      "tf_efficientnet_b5.in1k\n",
      "tf_efficientnet_b5.ns_jft_in1k\n",
      "tf_efficientnet_b5.ra_in1k\n",
      "tf_efficientnet_b6.aa_in1k\n",
      "tf_efficientnet_b6.ap_in1k\n",
      "tf_efficientnet_b6.ns_jft_in1k\n",
      "tf_efficientnet_b7.aa_in1k\n",
      "tf_efficientnet_b7.ap_in1k\n",
      "tf_efficientnet_b7.ns_jft_in1k\n",
      "tf_efficientnet_b7.ra_in1k\n",
      "tf_efficientnet_b8.ap_in1k\n",
      "tf_efficientnet_b8.ra_in1k\n",
      "tf_efficientnet_cc_b0_4e.in1k\n",
      "tf_efficientnet_cc_b0_8e.in1k\n",
      "tf_efficientnet_cc_b1_8e.in1k\n",
      "tf_efficientnet_el.in1k\n",
      "tf_efficientnet_em.in1k\n",
      "tf_efficientnet_es.in1k\n",
      "tf_efficientnet_l2.ns_jft_in1k\n",
      "tf_efficientnet_l2.ns_jft_in1k_475\n",
      "tf_efficientnet_lite0.in1k\n",
      "tf_efficientnet_lite1.in1k\n",
      "tf_efficientnet_lite2.in1k\n",
      "tf_efficientnet_lite3.in1k\n",
      "tf_efficientnet_lite4.in1k\n",
      "tf_efficientnetv2_b0.in1k\n",
      "tf_efficientnetv2_b1.in1k\n",
      "tf_efficientnetv2_b2.in1k\n",
      "tf_efficientnetv2_b3.in1k\n",
      "tf_efficientnetv2_b3.in21k\n",
      "tf_efficientnetv2_b3.in21k_ft_in1k\n",
      "tf_efficientnetv2_l.in1k\n",
      "tf_efficientnetv2_l.in21k\n",
      "tf_efficientnetv2_l.in21k_ft_in1k\n",
      "tf_efficientnetv2_m.in1k\n",
      "tf_efficientnetv2_m.in21k\n",
      "tf_efficientnetv2_m.in21k_ft_in1k\n",
      "tf_efficientnetv2_s.in1k\n",
      "tf_efficientnetv2_s.in21k\n",
      "tf_efficientnetv2_s.in21k_ft_in1k\n",
      "tf_efficientnetv2_xl.in21k\n",
      "tf_efficientnetv2_xl.in21k_ft_in1k\n",
      "tf_mixnet_l.in1k\n",
      "tf_mixnet_m.in1k\n",
      "tf_mixnet_s.in1k\n",
      "tf_mobilenetv3_large_075.in1k\n",
      "tf_mobilenetv3_large_100.in1k\n",
      "tf_mobilenetv3_large_minimal_100.in1k\n",
      "tf_mobilenetv3_small_075.in1k\n",
      "tf_mobilenetv3_small_100.in1k\n",
      "tf_mobilenetv3_small_minimal_100.in1k\n",
      "tiny_vit_5m_224.dist_in22k\n",
      "tiny_vit_5m_224.dist_in22k_ft_in1k\n",
      "tiny_vit_5m_224.in1k\n",
      "tiny_vit_11m_224.dist_in22k\n",
      "tiny_vit_11m_224.dist_in22k_ft_in1k\n",
      "tiny_vit_11m_224.in1k\n",
      "tiny_vit_21m_224.dist_in22k\n",
      "tiny_vit_21m_224.dist_in22k_ft_in1k\n",
      "tiny_vit_21m_224.in1k\n",
      "tiny_vit_21m_384.dist_in22k_ft_in1k\n",
      "tiny_vit_21m_512.dist_in22k_ft_in1k\n",
      "tinynet_a.in1k\n",
      "tinynet_b.in1k\n",
      "tinynet_c.in1k\n",
      "tinynet_d.in1k\n",
      "tinynet_e.in1k\n",
      "tnt_s_patch16_224\n",
      "tresnet_l.miil_in1k\n",
      "tresnet_l.miil_in1k_448\n",
      "tresnet_m.miil_in1k\n",
      "tresnet_m.miil_in1k_448\n",
      "tresnet_m.miil_in21k\n",
      "tresnet_m.miil_in21k_ft_in1k\n",
      "tresnet_v2_l.miil_in21k\n",
      "tresnet_v2_l.miil_in21k_ft_in1k\n",
      "tresnet_xl.miil_in1k\n",
      "tresnet_xl.miil_in1k_448\n",
      "twins_pcpvt_base.in1k\n",
      "twins_pcpvt_large.in1k\n",
      "twins_pcpvt_small.in1k\n",
      "twins_svt_base.in1k\n",
      "twins_svt_large.in1k\n",
      "twins_svt_small.in1k\n",
      "vgg11.tv_in1k\n",
      "vgg11_bn.tv_in1k\n",
      "vgg13.tv_in1k\n",
      "vgg13_bn.tv_in1k\n",
      "vgg16.tv_in1k\n",
      "vgg16_bn.tv_in1k\n",
      "vgg19.tv_in1k\n",
      "vgg19_bn.tv_in1k\n",
      "visformer_small.in1k\n",
      "visformer_tiny.in1k\n",
      "vit_base_patch8_224.augreg2_in21k_ft_in1k\n",
      "vit_base_patch8_224.augreg_in21k\n",
      "vit_base_patch8_224.augreg_in21k_ft_in1k\n",
      "vit_base_patch8_224.dino\n",
      "vit_base_patch14_dinov2.lvd142m\n",
      "vit_base_patch14_reg4_dinov2.lvd142m\n",
      "vit_base_patch16_224.augreg2_in21k_ft_in1k\n",
      "vit_base_patch16_224.augreg_in1k\n",
      "vit_base_patch16_224.augreg_in21k\n",
      "vit_base_patch16_224.augreg_in21k_ft_in1k\n",
      "vit_base_patch16_224.dino\n",
      "vit_base_patch16_224.mae\n",
      "vit_base_patch16_224.orig_in21k\n",
      "vit_base_patch16_224.orig_in21k_ft_in1k\n",
      "vit_base_patch16_224.sam_in1k\n",
      "vit_base_patch16_224_miil.in21k\n",
      "vit_base_patch16_224_miil.in21k_ft_in1k\n",
      "vit_base_patch16_384.augreg_in1k\n",
      "vit_base_patch16_384.augreg_in21k_ft_in1k\n",
      "vit_base_patch16_384.orig_in21k_ft_in1k\n",
      "vit_base_patch16_clip_224.datacompxl\n",
      "vit_base_patch16_clip_224.dfn2b\n",
      "vit_base_patch16_clip_224.laion2b\n",
      "vit_base_patch16_clip_224.laion2b_ft_in1k\n",
      "vit_base_patch16_clip_224.laion2b_ft_in12k\n",
      "vit_base_patch16_clip_224.laion2b_ft_in12k_in1k\n",
      "vit_base_patch16_clip_224.metaclip_2pt5b\n",
      "vit_base_patch16_clip_224.openai\n",
      "vit_base_patch16_clip_224.openai_ft_in1k\n",
      "vit_base_patch16_clip_224.openai_ft_in12k\n",
      "vit_base_patch16_clip_224.openai_ft_in12k_in1k\n",
      "vit_base_patch16_clip_384.laion2b_ft_in1k\n",
      "vit_base_patch16_clip_384.laion2b_ft_in12k_in1k\n",
      "vit_base_patch16_clip_384.openai_ft_in1k\n",
      "vit_base_patch16_clip_384.openai_ft_in12k_in1k\n",
      "vit_base_patch16_clip_quickgelu_224.metaclip_2pt5b\n",
      "vit_base_patch16_clip_quickgelu_224.openai\n",
      "vit_base_patch16_rope_reg1_gap_256.sbb_in1k\n",
      "vit_base_patch16_rpn_224.sw_in1k\n",
      "vit_base_patch16_siglip_224.webli\n",
      "vit_base_patch16_siglip_256.webli\n",
      "vit_base_patch16_siglip_384.webli\n",
      "vit_base_patch16_siglip_512.webli\n",
      "vit_base_patch16_siglip_gap_224.webli\n",
      "vit_base_patch16_siglip_gap_256.webli\n",
      "vit_base_patch16_siglip_gap_384.webli\n",
      "vit_base_patch16_siglip_gap_512.webli\n",
      "vit_base_patch32_224.augreg_in1k\n",
      "vit_base_patch32_224.augreg_in21k\n",
      "vit_base_patch32_224.augreg_in21k_ft_in1k\n",
      "vit_base_patch32_224.orig_in21k\n",
      "vit_base_patch32_224.sam_in1k\n",
      "vit_base_patch32_384.augreg_in1k\n",
      "vit_base_patch32_384.augreg_in21k_ft_in1k\n",
      "vit_base_patch32_clip_224.datacompxl\n",
      "vit_base_patch32_clip_224.laion2b\n",
      "vit_base_patch32_clip_224.laion2b_ft_in1k\n",
      "vit_base_patch32_clip_224.laion2b_ft_in12k_in1k\n",
      "vit_base_patch32_clip_224.metaclip_2pt5b\n",
      "vit_base_patch32_clip_224.openai\n",
      "vit_base_patch32_clip_224.openai_ft_in1k\n",
      "vit_base_patch32_clip_256.datacompxl\n",
      "vit_base_patch32_clip_384.laion2b_ft_in12k_in1k\n",
      "vit_base_patch32_clip_384.openai_ft_in12k_in1k\n",
      "vit_base_patch32_clip_448.laion2b_ft_in12k_in1k\n",
      "vit_base_patch32_clip_quickgelu_224.metaclip_2pt5b\n",
      "vit_base_patch32_clip_quickgelu_224.openai\n",
      "vit_base_r50_s16_224.orig_in21k\n",
      "vit_base_r50_s16_384.orig_in21k_ft_in1k\n",
      "vit_betwixt_patch16_reg1_gap_256.sbb_in1k\n",
      "vit_betwixt_patch16_reg4_gap_256.sbb_in1k\n",
      "vit_betwixt_patch16_reg4_gap_256.sbb_in12k\n",
      "vit_betwixt_patch16_reg4_gap_256.sbb_in12k_ft_in1k\n",
      "vit_betwixt_patch16_rope_reg4_gap_256.sbb_in1k\n",
      "vit_betwixt_patch32_clip_224.tinyclip_laion400m\n",
      "vit_giant_patch14_clip_224.laion2b\n",
      "vit_giant_patch14_dinov2.lvd142m\n",
      "vit_giant_patch14_reg4_dinov2.lvd142m\n",
      "vit_giant_patch16_gap_224.in22k_ijepa\n",
      "vit_gigantic_patch14_clip_224.laion2b\n",
      "vit_huge_patch14_224.mae\n",
      "vit_huge_patch14_224.orig_in21k\n",
      "vit_huge_patch14_clip_224.dfn5b\n",
      "vit_huge_patch14_clip_224.laion2b\n",
      "vit_huge_patch14_clip_224.laion2b_ft_in1k\n",
      "vit_huge_patch14_clip_224.laion2b_ft_in12k\n",
      "vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k\n",
      "vit_huge_patch14_clip_224.metaclip_2pt5b\n",
      "vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k\n",
      "vit_huge_patch14_clip_378.dfn5b\n",
      "vit_huge_patch14_clip_quickgelu_224.dfn5b\n",
      "vit_huge_patch14_clip_quickgelu_224.metaclip_2pt5b\n",
      "vit_huge_patch14_clip_quickgelu_378.dfn5b\n",
      "vit_huge_patch14_gap_224.in1k_ijepa\n",
      "vit_huge_patch14_gap_224.in22k_ijepa\n",
      "vit_huge_patch16_gap_448.in1k_ijepa\n",
      "vit_large_patch14_clip_224.datacompxl\n",
      "vit_large_patch14_clip_224.dfn2b\n",
      "vit_large_patch14_clip_224.laion2b\n",
      "vit_large_patch14_clip_224.laion2b_ft_in1k\n",
      "vit_large_patch14_clip_224.laion2b_ft_in12k\n",
      "vit_large_patch14_clip_224.laion2b_ft_in12k_in1k\n",
      "vit_large_patch14_clip_224.metaclip_2pt5b\n",
      "vit_large_patch14_clip_224.openai\n",
      "vit_large_patch14_clip_224.openai_ft_in1k\n",
      "vit_large_patch14_clip_224.openai_ft_in12k\n",
      "vit_large_patch14_clip_224.openai_ft_in12k_in1k\n",
      "vit_large_patch14_clip_336.laion2b_ft_in1k\n",
      "vit_large_patch14_clip_336.laion2b_ft_in12k_in1k\n",
      "vit_large_patch14_clip_336.openai\n",
      "vit_large_patch14_clip_336.openai_ft_in12k_in1k\n",
      "vit_large_patch14_clip_quickgelu_224.dfn2b\n",
      "vit_large_patch14_clip_quickgelu_224.metaclip_2pt5b\n",
      "vit_large_patch14_clip_quickgelu_224.openai\n",
      "vit_large_patch14_clip_quickgelu_336.openai\n",
      "vit_large_patch14_dinov2.lvd142m\n",
      "vit_large_patch14_reg4_dinov2.lvd142m\n",
      "vit_large_patch16_224.augreg_in21k\n",
      "vit_large_patch16_224.augreg_in21k_ft_in1k\n",
      "vit_large_patch16_224.mae\n",
      "vit_large_patch16_224.orig_in21k\n",
      "vit_large_patch16_384.augreg_in21k_ft_in1k\n",
      "vit_large_patch16_siglip_256.webli\n",
      "vit_large_patch16_siglip_384.webli\n",
      "vit_large_patch16_siglip_gap_256.webli\n",
      "vit_large_patch16_siglip_gap_384.webli\n",
      "vit_large_patch32_224.orig_in21k\n",
      "vit_large_patch32_384.orig_in21k_ft_in1k\n",
      "vit_large_r50_s32_224.augreg_in21k\n",
      "vit_large_r50_s32_224.augreg_in21k_ft_in1k\n",
      "vit_large_r50_s32_384.augreg_in21k_ft_in1k\n",
      "vit_little_patch16_reg4_gap_256.sbb_in1k\n",
      "vit_medium_patch16_clip_224.tinyclip_yfcc15m\n",
      "vit_medium_patch16_gap_240.sw_in12k\n",
      "vit_medium_patch16_gap_256.sw_in12k_ft_in1k\n",
      "vit_medium_patch16_gap_384.sw_in12k_ft_in1k\n",
      "vit_medium_patch16_reg1_gap_256.sbb_in1k\n",
      "vit_medium_patch16_reg4_gap_256.sbb_in1k\n",
      "vit_medium_patch16_rope_reg1_gap_256.sbb_in1k\n",
      "vit_medium_patch32_clip_224.tinyclip_laion400m\n",
      "vit_mediumd_patch16_reg4_gap_256.sbb_in12k\n",
      "vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k\n",
      "vit_mediumd_patch16_rope_reg1_gap_256.sbb_in1k\n",
      "vit_pwee_patch16_reg1_gap_256.sbb_in1k\n",
      "vit_relpos_base_patch16_224.sw_in1k\n",
      "vit_relpos_base_patch16_clsgap_224.sw_in1k\n",
      "vit_relpos_base_patch32_plus_rpn_256.sw_in1k\n",
      "vit_relpos_medium_patch16_224.sw_in1k\n",
      "vit_relpos_medium_patch16_cls_224.sw_in1k\n",
      "vit_relpos_medium_patch16_rpn_224.sw_in1k\n",
      "vit_relpos_small_patch16_224.sw_in1k\n",
      "vit_small_patch8_224.dino\n",
      "vit_small_patch14_dinov2.lvd142m\n",
      "vit_small_patch14_reg4_dinov2.lvd142m\n",
      "vit_small_patch16_224.augreg_in1k\n",
      "vit_small_patch16_224.augreg_in21k\n",
      "vit_small_patch16_224.augreg_in21k_ft_in1k\n",
      "vit_small_patch16_224.dino\n",
      "vit_small_patch16_384.augreg_in1k\n",
      "vit_small_patch16_384.augreg_in21k_ft_in1k\n",
      "vit_small_patch32_224.augreg_in21k\n",
      "vit_small_patch32_224.augreg_in21k_ft_in1k\n",
      "vit_small_patch32_384.augreg_in21k_ft_in1k\n",
      "vit_small_r26_s32_224.augreg_in21k\n",
      "vit_small_r26_s32_224.augreg_in21k_ft_in1k\n",
      "vit_small_r26_s32_384.augreg_in21k_ft_in1k\n",
      "vit_so400m_patch14_siglip_224.webli\n",
      "vit_so400m_patch14_siglip_384.webli\n",
      "vit_so400m_patch14_siglip_gap_224.pali_mix\n",
      "vit_so400m_patch14_siglip_gap_224.pali_pt\n",
      "vit_so400m_patch14_siglip_gap_224.webli\n",
      "vit_so400m_patch14_siglip_gap_384.webli\n",
      "vit_so400m_patch14_siglip_gap_448.pali_mix\n",
      "vit_so400m_patch14_siglip_gap_448.pali_pt\n",
      "vit_so400m_patch14_siglip_gap_896.pali_pt\n",
      "vit_srelpos_medium_patch16_224.sw_in1k\n",
      "vit_srelpos_small_patch16_224.sw_in1k\n",
      "vit_tiny_patch16_224.augreg_in21k\n",
      "vit_tiny_patch16_224.augreg_in21k_ft_in1k\n",
      "vit_tiny_patch16_384.augreg_in21k_ft_in1k\n",
      "vit_tiny_r_s16_p8_224.augreg_in21k\n",
      "vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k\n",
      "vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k\n",
      "vit_wee_patch16_reg1_gap_256.sbb_in1k\n",
      "vit_xsmall_patch16_clip_224.tinyclip_yfcc15m\n",
      "volo_d1_224.sail_in1k\n",
      "volo_d1_384.sail_in1k\n",
      "volo_d2_224.sail_in1k\n",
      "volo_d2_384.sail_in1k\n",
      "volo_d3_224.sail_in1k\n",
      "volo_d3_448.sail_in1k\n",
      "volo_d4_224.sail_in1k\n",
      "volo_d4_448.sail_in1k\n",
      "volo_d5_224.sail_in1k\n",
      "volo_d5_448.sail_in1k\n",
      "volo_d5_512.sail_in1k\n",
      "wide_resnet50_2.racm_in1k\n",
      "wide_resnet50_2.tv2_in1k\n",
      "wide_resnet50_2.tv_in1k\n",
      "wide_resnet101_2.tv2_in1k\n",
      "wide_resnet101_2.tv_in1k\n",
      "xception41.tf_in1k\n",
      "xception41p.ra3_in1k\n",
      "xception65.ra3_in1k\n",
      "xception65.tf_in1k\n",
      "xception65p.ra3_in1k\n",
      "xception71.tf_in1k\n",
      "xcit_large_24_p8_224.fb_dist_in1k\n",
      "xcit_large_24_p8_224.fb_in1k\n",
      "xcit_large_24_p8_384.fb_dist_in1k\n",
      "xcit_large_24_p16_224.fb_dist_in1k\n",
      "xcit_large_24_p16_224.fb_in1k\n",
      "xcit_large_24_p16_384.fb_dist_in1k\n",
      "xcit_medium_24_p8_224.fb_dist_in1k\n",
      "xcit_medium_24_p8_224.fb_in1k\n",
      "xcit_medium_24_p8_384.fb_dist_in1k\n",
      "xcit_medium_24_p16_224.fb_dist_in1k\n",
      "xcit_medium_24_p16_224.fb_in1k\n",
      "xcit_medium_24_p16_384.fb_dist_in1k\n",
      "xcit_nano_12_p8_224.fb_dist_in1k\n",
      "xcit_nano_12_p8_224.fb_in1k\n",
      "xcit_nano_12_p8_384.fb_dist_in1k\n",
      "xcit_nano_12_p16_224.fb_dist_in1k\n",
      "xcit_nano_12_p16_224.fb_in1k\n",
      "xcit_nano_12_p16_384.fb_dist_in1k\n",
      "xcit_small_12_p8_224.fb_dist_in1k\n",
      "xcit_small_12_p8_224.fb_in1k\n",
      "xcit_small_12_p8_384.fb_dist_in1k\n",
      "xcit_small_12_p16_224.fb_dist_in1k\n",
      "xcit_small_12_p16_224.fb_in1k\n",
      "xcit_small_12_p16_384.fb_dist_in1k\n",
      "xcit_small_24_p8_224.fb_dist_in1k\n",
      "xcit_small_24_p8_224.fb_in1k\n",
      "xcit_small_24_p8_384.fb_dist_in1k\n",
      "xcit_small_24_p16_224.fb_dist_in1k\n",
      "xcit_small_24_p16_224.fb_in1k\n",
      "xcit_small_24_p16_384.fb_dist_in1k\n",
      "xcit_tiny_12_p8_224.fb_dist_in1k\n",
      "xcit_tiny_12_p8_224.fb_in1k\n",
      "xcit_tiny_12_p8_384.fb_dist_in1k\n",
      "xcit_tiny_12_p16_224.fb_dist_in1k\n",
      "xcit_tiny_12_p16_224.fb_in1k\n",
      "xcit_tiny_12_p16_384.fb_dist_in1k\n",
      "xcit_tiny_24_p8_224.fb_dist_in1k\n",
      "xcit_tiny_24_p8_224.fb_in1k\n",
      "xcit_tiny_24_p8_384.fb_dist_in1k\n",
      "xcit_tiny_24_p16_224.fb_dist_in1k\n",
      "xcit_tiny_24_p16_224.fb_in1k\n",
      "xcit_tiny_24_p16_384.fb_dist_in1k\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "pretrained_models = timm.list_models(pretrained=True)\n",
    "\n",
    "for m in pretrained_models:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoattack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autopgd_base\n\u001b[0;32m----> 4\u001b[0m autopgd_base\u001b[38;5;241m.\u001b[39mAPGDAttack(\u001b[43mmodel\u001b[49m, n_restarts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mperturb_steps, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                 eps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepsilon, norm\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdistance, eot_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, rho\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.75\u001b[39m, seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed,\n\u001b[1;32m      6\u001b[0m                 device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1280 1024\n"
     ]
    }
   ],
   "source": [
    "effective_batch_size = 1024\n",
    "world_size = 4\n",
    "per_gpu_batch_size = 320\n",
    "\n",
    "print( effective_batch_size // (world_size * per_gpu_batch_size), (world_size * per_gpu_batch_size), effective_batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "from architectures.resnet_imagenet import ResNet_imagenet, Bottleneck_imagenet\n",
    "model = ResNet_imagenet(Bottleneck_imagenet, [3, 4, 6, 3], )\n",
    "num_features = model.fc.in_features\n",
    "print(num_features)\n",
    "model.fc = nn.Linear(num_features, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
