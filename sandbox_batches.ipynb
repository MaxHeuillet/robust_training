{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset\n",
    "from dataloader import IndexedDataLoader\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args, get_exp_name\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import InfoBatch , CustomSampler\n",
    "from losses import get_loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "args = get_args()\n",
    "# args.arch = 'LeNet5'\n",
    "# args.dataset = 'MNIST'\n",
    "# args.selection_method = 'none'\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "# model, target_layers = load_architecture(args)\n",
    "\n",
    "# get_exp_name(args)\n",
    "\n",
    "# args.iterations = 2\n",
    "# args.ratio = 0.5\n",
    "# args.delta = 1\n",
    "# args.batch_size = 128\n",
    "# args.pruner = 'score'\n",
    "# args.sample_size=256\n",
    "\n",
    "# # train_dataset = IndexedDataset()\n",
    "# print('init weighted dataset')\n",
    "# train_dataset = WeightedDataset(args, epochs = args.epochs, train=True, prune_ratio = args.ratio, delta = args.delta, )\n",
    "\n",
    "# print('init sampler')\n",
    "# sampler = CustomSampler(args, train_dataset)\n",
    "\n",
    "# # trainset = InfoBatch(dataset, args.epochs, args.ratio, args.delta)\n",
    "\n",
    "# # # sampler = trainset.sampler\n",
    "# # train_shuffle = False\n",
    "\n",
    "# print('init dataloder')\n",
    "# # trainloader = DataLoader(train_dataset, batch_size=None, num_workers=0, sampler=sampler) # \n",
    "# trainloader = DataLoader(train_dataset, batch_size=None, num_workers=0, sampler = sampler,  pin_memory=True) # \n",
    "\n",
    "# print('init optimizer')\n",
    "# optimizer = torch.optim.Adam( model.parameters(), lr=args.lr, )\n",
    "\n",
    "# def train_info_batch(epoch):\n",
    "#     # safe_print('\\nEpoch: %d, iterations %d' % (epoch, len(trainloader)))\n",
    "#     model.train()\n",
    "\n",
    "#     for batch_idx, blobs in  tqdm( enumerate(trainloader) ) :\n",
    "#         # print(len(blobs) )\n",
    "#         inputs, targets, idxs = blobs \n",
    "#         # print(idxs)\n",
    "#         # print(targets)\n",
    "\n",
    "#         # print(len(batch))\n",
    "#         inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "\n",
    "#         # train_dataset.set_active_indices(idxs)\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         loss_values, clean_values, robust_values, logits_nat, logits_adv = trades_loss(model=model, x_natural=inputs, y=targets, optimizer=optimizer,)\n",
    "#         # 3. use <InfoBatch>.update(loss), all scoring/rescaling/getting mean is now conducted at the backend, see previous (research version) code for details.\n",
    "\n",
    "#         train_dataset.update_scores(idxs, clean_values, robust_values, loss_values, logits_nat, logits_adv)\n",
    "#         # print(trainset.weights.shape)\n",
    "#         # print(loss_values.shape)\n",
    "#         # print(trainset.cur_batch_index)\n",
    "#         loss = train_dataset.compute_weighted_loss(idxs, loss_values)\n",
    "\n",
    "#         # print(loss)\n",
    "#         loss.backward()\n",
    "#         # print(loss)\n",
    "        \n",
    "#     # safe_print('epoch:', epoch, '  Training Accuracy:', round(100. * correct /\n",
    "#     #     total, 3), '  Train loss:', round(train_loss / len(trainloader), 4))\n",
    "#     # train_acc.append(correct / total)\n",
    "\n",
    "# for epoch in range(args.epochs):\n",
    "#     print('epoch')\n",
    "#     # trainloader.sampler.set_epoch(epoch)\n",
    "#     train_info_batch(epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TRADES_0.1_sched_CIFAR10_resnet50_none_0_random_10_0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weighted dataset\n",
      "init dataloder\n",
      "\n",
      "Start of iteration 0\n",
      "\n",
      "global [0, 1, 2, 3, 4, 5, 6]\n",
      "pruning\n",
      "post_pruning [1, 6, 2]\n",
      "remove tail\n",
      "process [6]\n",
      "[6]\n",
      "batch no. 0\n",
      "[tensor([[6]]), tensor([6]), tensor([6])]\n",
      "\n",
      "Start of iteration 1\n",
      "\n",
      "global [0, 1, 2, 3, 4, 5, 6]\n",
      "pruning\n",
      "post_pruning [6, 1, 2]\n",
      "remove tail\n",
      "process [1]\n",
      "[1]\n",
      "batch no. 0\n",
      "[tensor([[1]]), tensor([1]), tensor([1])]\n",
      "\n",
      "Start of iteration 2\n",
      "\n",
      "global [0, 1, 2, 3, 4, 5, 6]\n",
      "pruning\n",
      "post_pruning [3, 1, 4]\n",
      "remove tail\n",
      "process [1]\n",
      "[1]\n",
      "batch no. 0\n",
      "[tensor([[1]]), tensor([1]), tensor([1])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset\n",
    "from dataloader import IndexedDataLoader\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import DistributedCustomSampler\n",
    "from losses import trades_loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "args = get_args()\n",
    "args.arch = 'LeNet5'\n",
    "args.dataset = 'random'\n",
    "args.selection_method = 'none'\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "model, target_layers = load_architecture(args)\n",
    "\n",
    "args.epochs = 2\n",
    "args.ratio = 0.5\n",
    "args.delta = 1\n",
    "args.batch_size = 2\n",
    "args.pruner = 'random'\n",
    "args.sample_size= 5\n",
    "\n",
    "# train_dataset = IndexedDataset()\n",
    "print('init weighted dataset')\n",
    "train_dataset = WeightedDataset(args, train=True, prune_ratio = args.ratio,  )\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i])\n",
    "\n",
    "dist_sampler = DistributedCustomSampler(args, train_dataset, num_replicas=2, rank=1, drop_last=True)\n",
    "\n",
    "print('init dataloder')\n",
    "trainloader = DataLoader(train_dataset, batch_size=None, sampler = dist_sampler,)  \n",
    "\n",
    "\n",
    "for iteration in range(3):\n",
    "    \n",
    "    print()\n",
    "    print('Start of iteration', iteration)\n",
    "    print()\n",
    "    \n",
    "    dist_sampler.set_epoch(iteration)\n",
    "    \n",
    "    for batch_id, batch in enumerate( trainloader ):\n",
    "        print('batch no.', batch_id,)\n",
    "        print( batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
