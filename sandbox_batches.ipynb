{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset, load_data\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import DistributedCustomSampler\n",
    "from losses import trades_loss, apgd_loss\n",
    "from tqdm.notebook import tqdm\n",
    "from architectures import CustomModel, load_architecture, add_lora, set_lora_gradients #load_statedict\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "args.dataset = 'Flowers'\n",
    "args.selection_method = 'random'\n",
    "args.aug = 'aug'\n",
    "args.loss_function = 'APGD'\n",
    "\n",
    "args.iterations = 10\n",
    "args.pruning_ratio = 0\n",
    "args.delta = 1\n",
    "args.batch_size = 24\n",
    "args.init_lr = 0.001\n",
    "args.freeze_epochs = 5\n",
    "args.backbone = 'convnext_tiny' #deit_small_patch16_224.fb_in1k\n",
    "args.ft_type = 'full_fine_tuning'\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, N, train_transform, transform = load_data(args) \n",
    "# print(N)\n",
    "\n",
    "train_dataset = WeightedDataset(args, train_dataset, train_transform, N, prune_ratio = args.pruning_ratio,  )\n",
    "\n",
    "# # train_sampler = DistributedCustomSampler(args, train_dataset, num_replicas=2, rank=0, drop_last=True)\n",
    "train_sampler = DistributedSampler(train_dataset, num_replicas=2, rank=0, shuffle=True, drop_last=True)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=3, sampler = train_sampler, )\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "# rank = 0\n",
    "model = load_architecture(args, N= 100, rank=0 )\n",
    "# model = CustomModel(args, model)\n",
    "\n",
    "# model.set_fine_tuning_strategy()\n",
    "# model.to(rank)\n",
    "# model = DDP(model, device_ids=[rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up the distributed setup, 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.2.2+cu121\n",
      "cuda 12.1\n",
      "cudnn 8902\n",
      " world size 1, rank 0\n",
      "set up the master adress and port\n",
      "init process group ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/maxheuillet/robust-training20/49bcd804a17f466bb92bfddc844a9bea\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize dataset 0\n",
      "initialize sampler 0\n",
      "initialize dataoader 0\n",
      "start the loop\n",
      "effective batch size 1024 per_gpu_batch_size 8 accumulation steps 128\n",
      "start batches\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mheuillet/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [768, 1, 7, 7], strides() = [49, 1, 7, 1]\n",
      "bucket_view.sizes() = [768, 1, 7, 7], strides() = [49, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale loss and backward done\n",
      "gradient norm: 43786.422493364895 rank: 0 loss tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 62045.44580973795 rank: 0 loss tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 79828.80632852313 rank: 0 loss tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 97197.0885694511 rank: 0 loss tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 110336.72220889384 rank: 0 loss tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 117374.12041508089 rank: 0 loss tensor(0.0364, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 142614.29969854187 rank: 0 loss tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 152837.63969261647 rank: 0 loss tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 159837.55584620574 rank: 0 loss tensor(0.0348, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 153645.67352732632 rank: 0 loss tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 164415.5040525595 rank: 0 loss tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 184603.7564386284 rank: 0 loss tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 184488.37922038918 rank: 0 loss tensor(0.0348, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 206029.12949056033 rank: 0 loss tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 203837.70199743402 rank: 0 loss tensor(0.0362, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 202199.90978628406 rank: 0 loss tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 209429.97791822645 rank: 0 loss tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 199027.24355557814 rank: 0 loss tensor(0.0384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 205935.74624291313 rank: 0 loss tensor(0.0352, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 212885.63556267027 rank: 0 loss tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 222864.38706275288 rank: 0 loss tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 228951.31961430443 rank: 0 loss tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 239586.9418179214 rank: 0 loss tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 242596.9645414862 rank: 0 loss tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 260751.68555165647 rank: 0 loss tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 260954.26101457785 rank: 0 loss tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 262882.5004517027 rank: 0 loss tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 265799.04038485046 rank: 0 loss tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 263168.87976813194 rank: 0 loss tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 284190.55345323833 rank: 0 loss tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 286395.79536487075 rank: 0 loss tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 288586.3766350879 rank: 0 loss tensor(0.0366, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 276191.1703488265 rank: 0 loss tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 286660.90911369876 rank: 0 loss tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 296574.5154120186 rank: 0 loss tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 290180.86226334574 rank: 0 loss tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 297074.07191575906 rank: 0 loss tensor(0.0365, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 315673.77417082584 rank: 0 loss tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 324644.75516555225 rank: 0 loss tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 329586.104453945 rank: 0 loss tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 341290.3984341864 rank: 0 loss tensor(0.0367, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 346196.0285849217 rank: 0 loss tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 361498.96178966993 rank: 0 loss tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 378783.6006934664 rank: 0 loss tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 383787.2945761976 rank: 0 loss tensor(0.0359, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 376990.7169998958 rank: 0 loss tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 377020.7379644983 rank: 0 loss tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 382128.67197942536 rank: 0 loss tensor(0.0368, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 383798.35247192014 rank: 0 loss tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n",
      "scale loss and backward done\n",
      "gradient norm: 387999.4878546738 rank: 0 loss tensor(0.0341, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "comput loss\n",
      " loss computed\n",
      "average loss\n",
      "divide loss\n",
      "loss modification performed\n",
      "scale loss and backward\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# set_seeds(args.seed)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m experiment \u001b[38;5;241m=\u001b[39m BaseExperiment(args, world_size)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/distributed_experiment1.py:197\u001b[0m, in \u001b[0;36mBaseExperiment.training\u001b[0;34m(self, rank)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# torch.autograd.set_detect_anomaly(True)\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m#self.validate(valloader, model, experiment, 0, rank)\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart the loop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m dist\u001b[38;5;241m.\u001b[39mbarrier() \n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# Access the underlying model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/distributed_experiment1.py:284\u001b[0m, in \u001b[0;36mBaseExperiment.fit\u001b[0;34m(self, model, optimizer, trainloader, valloader, train_sampler, val_sampler, N, logger, rank)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Backward pass with gradient scaling\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale loss and backward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale loss and backward done\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from distributed_experiment1 import BaseExperiment\n",
    "\n",
    "world_size = torch.cuda.device_count()\n",
    "# set_seeds(args.seed)\n",
    "\n",
    "experiment = BaseExperiment(args, world_size)\n",
    "\n",
    "experiment.training(rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664424c9577f4bafa794a5eb04263e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 224, 224]) torch.Size([3]) tensor([44,  9, 38])\n",
      "torch.Size([3]) torch.Size([3, 3, 224, 224])\n",
      "tensor([44,  9, 38], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from losses import get_loss, get_eval_loss\n",
    "import numpy as np\n",
    "from losses import apgd_loss\n",
    "\n",
    "rank = 'cuda'\n",
    "\n",
    "optimizer = torch.optim.SGD( model.parameters(),lr=args.init_lr, weight_decay=args.weight_decay, momentum=args.momentum, nesterov=True, )\n",
    "\n",
    "from autoattack import autopgd_base\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "cutmix = v2.CutMix(num_classes=N)\n",
    "mixup = v2.MixUp(num_classes=N)\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n",
    "\n",
    "for iteration in range(args.iterations):\n",
    "\n",
    "    model.eval()\n",
    "    train_sampler.set_epoch(iteration)\n",
    "\n",
    "    # apgd = autopgd_base.APGDAttack(model, n_restarts=5, n_iter=args.perturb_steps, verbose=False,\n",
    "    #             eps=args.epsilon, norm=args.distance, eot_iter=1, rho=.75, seed=args.seed,\n",
    "    #             device='cuda', logger=None).perturb(data, target)\n",
    "    \n",
    "    for batch_id, batch in tqdm(enumerate( trainloader ) ):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, target, idxs = batch\n",
    "\n",
    "        print(data.shape, target.shape, target)\n",
    "\n",
    "        data, target_one_hot = cutmix_or_mixup(data, target)\n",
    "\n",
    "        print(target.shape, data.shape)\n",
    "\n",
    "        # print(data,target, idxs)\n",
    "\n",
    "        data, target = data.to(rank), target.to(rank) \n",
    "\n",
    "        print(target,)\n",
    "\n",
    "        break\n",
    "\n",
    "    break\n",
    "\n",
    "        # data, target_one_hot = cutmix_or_mixup(data, target)\n",
    "\n",
    "        # adv_data = apgd\n",
    "        # print('data attacked')\n",
    "\n",
    "        # loss_values, logits = get_loss(args, model, data, target, optimizer)\n",
    "\n",
    "        # loss = loss_values.mean()\n",
    "        # #loss = #train_dataset.compute_loss(idxs, loss_values)\n",
    "        # print(loss)\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        # break\n",
    "    \n",
    "    # model.update_fine_tuning_strategy(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract labels from the dataset\n",
    "labels = [ label for _, label in dataset ]\n",
    "\n",
    "# Split the dataset into train+val and test, keeping stratification\n",
    "train_val_indices, test_indices = train_test_split( range(len(labels)), test_size=0.2, stratify=labels, random_state=42 )\n",
    "\n",
    "# Extract labels for the train+val set for further stratification\n",
    "train_val_labels = [labels[i] for i in train_val_indices]\n",
    "\n",
    "# Split the train+val set into train and validation, keeping stratification\n",
    "train_indices, val_indices = train_test_split( train_val_indices, test_size=0.15, stratify=train_val_labels, random_state=42 )  # 0.25 * 0.8 = 0.2 of the dataset\n",
    "\n",
    "# Create subsets for train, validation, and test\n",
    "\n",
    "N = 10\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://madm.dfki.de/files/sentinel/EuroSAT.zip to ./data/eurosat/EuroSAT.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94280567/94280567 [00:15<00:00, 6066822.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/eurosat/EuroSAT.zip to ./data/eurosat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset EuroSAT\n",
       "    Number of datapoints: 27000\n",
       "    Root location: ./data"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat_resnext26ts.ch_in1k\n",
      "beit_base_patch16_224.in22k_ft_in22k\n",
      "beit_base_patch16_224.in22k_ft_in22k_in1k\n",
      "beit_base_patch16_384.in22k_ft_in22k_in1k\n",
      "beit_large_patch16_224.in22k_ft_in22k\n",
      "beit_large_patch16_224.in22k_ft_in22k_in1k\n",
      "beit_large_patch16_384.in22k_ft_in22k_in1k\n",
      "beit_large_patch16_512.in22k_ft_in22k_in1k\n",
      "beitv2_base_patch16_224.in1k_ft_in1k\n",
      "beitv2_base_patch16_224.in1k_ft_in22k\n",
      "beitv2_base_patch16_224.in1k_ft_in22k_in1k\n",
      "beitv2_large_patch16_224.in1k_ft_in1k\n",
      "beitv2_large_patch16_224.in1k_ft_in22k\n",
      "beitv2_large_patch16_224.in1k_ft_in22k_in1k\n",
      "botnet26t_256.c1_in1k\n",
      "caformer_b36.sail_in1k\n",
      "caformer_b36.sail_in1k_384\n",
      "caformer_b36.sail_in22k\n",
      "caformer_b36.sail_in22k_ft_in1k\n",
      "caformer_b36.sail_in22k_ft_in1k_384\n",
      "caformer_m36.sail_in1k\n",
      "caformer_m36.sail_in1k_384\n",
      "caformer_m36.sail_in22k\n",
      "caformer_m36.sail_in22k_ft_in1k\n",
      "caformer_m36.sail_in22k_ft_in1k_384\n",
      "caformer_s18.sail_in1k\n",
      "caformer_s18.sail_in1k_384\n",
      "caformer_s18.sail_in22k\n",
      "caformer_s18.sail_in22k_ft_in1k\n",
      "caformer_s18.sail_in22k_ft_in1k_384\n",
      "caformer_s36.sail_in1k\n",
      "caformer_s36.sail_in1k_384\n",
      "caformer_s36.sail_in22k\n",
      "caformer_s36.sail_in22k_ft_in1k\n",
      "caformer_s36.sail_in22k_ft_in1k_384\n",
      "cait_m36_384.fb_dist_in1k\n",
      "cait_m48_448.fb_dist_in1k\n",
      "cait_s24_224.fb_dist_in1k\n",
      "cait_s24_384.fb_dist_in1k\n",
      "cait_s36_384.fb_dist_in1k\n",
      "cait_xs24_384.fb_dist_in1k\n",
      "cait_xxs24_224.fb_dist_in1k\n",
      "cait_xxs24_384.fb_dist_in1k\n",
      "cait_xxs36_224.fb_dist_in1k\n",
      "cait_xxs36_384.fb_dist_in1k\n",
      "coat_lite_medium.in1k\n",
      "coat_lite_medium_384.in1k\n",
      "coat_lite_mini.in1k\n",
      "coat_lite_small.in1k\n",
      "coat_lite_tiny.in1k\n",
      "coat_mini.in1k\n",
      "coat_small.in1k\n",
      "coat_tiny.in1k\n",
      "coatnet_0_rw_224.sw_in1k\n",
      "coatnet_1_rw_224.sw_in1k\n",
      "coatnet_2_rw_224.sw_in12k\n",
      "coatnet_2_rw_224.sw_in12k_ft_in1k\n",
      "coatnet_3_rw_224.sw_in12k\n",
      "coatnet_bn_0_rw_224.sw_in1k\n",
      "coatnet_nano_rw_224.sw_in1k\n",
      "coatnet_rmlp_1_rw2_224.sw_in12k\n",
      "coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k\n",
      "coatnet_rmlp_1_rw_224.sw_in1k\n",
      "coatnet_rmlp_2_rw_224.sw_in1k\n",
      "coatnet_rmlp_2_rw_224.sw_in12k\n",
      "coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k\n",
      "coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k\n",
      "coatnet_rmlp_nano_rw_224.sw_in1k\n",
      "coatnext_nano_rw_224.sw_in1k\n",
      "convformer_b36.sail_in1k\n",
      "convformer_b36.sail_in1k_384\n",
      "convformer_b36.sail_in22k\n",
      "convformer_b36.sail_in22k_ft_in1k\n",
      "convformer_b36.sail_in22k_ft_in1k_384\n",
      "convformer_m36.sail_in1k\n",
      "convformer_m36.sail_in1k_384\n",
      "convformer_m36.sail_in22k\n",
      "convformer_m36.sail_in22k_ft_in1k\n",
      "convformer_m36.sail_in22k_ft_in1k_384\n",
      "convformer_s18.sail_in1k\n",
      "convformer_s18.sail_in1k_384\n",
      "convformer_s18.sail_in22k\n",
      "convformer_s18.sail_in22k_ft_in1k\n",
      "convformer_s18.sail_in22k_ft_in1k_384\n",
      "convformer_s36.sail_in1k\n",
      "convformer_s36.sail_in1k_384\n",
      "convformer_s36.sail_in22k\n",
      "convformer_s36.sail_in22k_ft_in1k\n",
      "convformer_s36.sail_in22k_ft_in1k_384\n",
      "convit_base.fb_in1k\n",
      "convit_small.fb_in1k\n",
      "convit_tiny.fb_in1k\n",
      "convmixer_768_32.in1k\n",
      "convmixer_1024_20_ks9_p14.in1k\n",
      "convmixer_1536_20.in1k\n",
      "convnext_atto.d2_in1k\n",
      "convnext_atto_ols.a2_in1k\n",
      "convnext_base.clip_laion2b\n",
      "convnext_base.clip_laion2b_augreg\n",
      "convnext_base.clip_laion2b_augreg_ft_in1k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k_in1k\n",
      "convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384\n",
      "convnext_base.clip_laiona\n",
      "convnext_base.clip_laiona_320\n",
      "convnext_base.clip_laiona_augreg_320\n",
      "convnext_base.clip_laiona_augreg_ft_in1k_384\n",
      "convnext_base.fb_in1k\n",
      "convnext_base.fb_in22k\n",
      "convnext_base.fb_in22k_ft_in1k\n",
      "convnext_base.fb_in22k_ft_in1k_384\n",
      "convnext_femto.d1_in1k\n",
      "convnext_femto_ols.d1_in1k\n",
      "convnext_large.fb_in1k\n",
      "convnext_large.fb_in22k\n",
      "convnext_large.fb_in22k_ft_in1k\n",
      "convnext_large.fb_in22k_ft_in1k_384\n",
      "convnext_large_mlp.clip_laion2b_augreg\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in1k\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in1k_384\n",
      "convnext_large_mlp.clip_laion2b_augreg_ft_in12k_384\n",
      "convnext_large_mlp.clip_laion2b_ft_320\n",
      "convnext_large_mlp.clip_laion2b_ft_soup_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_384\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320\n",
      "convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384\n",
      "convnext_nano.d1h_in1k\n",
      "convnext_nano.in12k\n",
      "convnext_nano.in12k_ft_in1k\n",
      "convnext_nano_ols.d1h_in1k\n",
      "convnext_pico.d1_in1k\n",
      "convnext_pico_ols.d1_in1k\n",
      "convnext_small.fb_in1k\n",
      "convnext_small.fb_in22k\n",
      "convnext_small.fb_in22k_ft_in1k\n",
      "convnext_small.fb_in22k_ft_in1k_384\n",
      "convnext_small.in12k\n",
      "convnext_small.in12k_ft_in1k\n",
      "convnext_small.in12k_ft_in1k_384\n",
      "convnext_tiny.fb_in1k\n",
      "convnext_tiny.fb_in22k\n",
      "convnext_tiny.fb_in22k_ft_in1k\n",
      "convnext_tiny.fb_in22k_ft_in1k_384\n",
      "convnext_tiny.in12k\n",
      "convnext_tiny.in12k_ft_in1k\n",
      "convnext_tiny.in12k_ft_in1k_384\n",
      "convnext_tiny_hnf.a2h_in1k\n",
      "convnext_xlarge.fb_in22k\n",
      "convnext_xlarge.fb_in22k_ft_in1k\n",
      "convnext_xlarge.fb_in22k_ft_in1k_384\n",
      "convnext_xxlarge.clip_laion2b_rewind\n",
      "convnext_xxlarge.clip_laion2b_soup\n",
      "convnext_xxlarge.clip_laion2b_soup_ft_in1k\n",
      "convnext_xxlarge.clip_laion2b_soup_ft_in12k\n",
      "convnextv2_atto.fcmae\n",
      "convnextv2_atto.fcmae_ft_in1k\n",
      "convnextv2_base.fcmae\n",
      "convnextv2_base.fcmae_ft_in1k\n",
      "convnextv2_base.fcmae_ft_in22k_in1k\n",
      "convnextv2_base.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_femto.fcmae\n",
      "convnextv2_femto.fcmae_ft_in1k\n",
      "convnextv2_huge.fcmae\n",
      "convnextv2_huge.fcmae_ft_in1k\n",
      "convnextv2_huge.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_huge.fcmae_ft_in22k_in1k_512\n",
      "convnextv2_large.fcmae\n",
      "convnextv2_large.fcmae_ft_in1k\n",
      "convnextv2_large.fcmae_ft_in22k_in1k\n",
      "convnextv2_large.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_nano.fcmae\n",
      "convnextv2_nano.fcmae_ft_in1k\n",
      "convnextv2_nano.fcmae_ft_in22k_in1k\n",
      "convnextv2_nano.fcmae_ft_in22k_in1k_384\n",
      "convnextv2_pico.fcmae\n",
      "convnextv2_pico.fcmae_ft_in1k\n",
      "convnextv2_tiny.fcmae\n",
      "convnextv2_tiny.fcmae_ft_in1k\n",
      "convnextv2_tiny.fcmae_ft_in22k_in1k\n",
      "convnextv2_tiny.fcmae_ft_in22k_in1k_384\n",
      "crossvit_9_240.in1k\n",
      "crossvit_9_dagger_240.in1k\n",
      "crossvit_15_240.in1k\n",
      "crossvit_15_dagger_240.in1k\n",
      "crossvit_15_dagger_408.in1k\n",
      "crossvit_18_240.in1k\n",
      "crossvit_18_dagger_240.in1k\n",
      "crossvit_18_dagger_408.in1k\n",
      "crossvit_base_240.in1k\n",
      "crossvit_small_240.in1k\n",
      "crossvit_tiny_240.in1k\n",
      "cs3darknet_focus_l.c2ns_in1k\n",
      "cs3darknet_focus_m.c2ns_in1k\n",
      "cs3darknet_l.c2ns_in1k\n",
      "cs3darknet_m.c2ns_in1k\n",
      "cs3darknet_x.c2ns_in1k\n",
      "cs3edgenet_x.c2_in1k\n",
      "cs3se_edgenet_x.c2ns_in1k\n",
      "cs3sedarknet_l.c2ns_in1k\n",
      "cs3sedarknet_x.c2ns_in1k\n",
      "cspdarknet53.ra_in1k\n",
      "cspresnet50.ra_in1k\n",
      "cspresnext50.ra_in1k\n",
      "darknet53.c2ns_in1k\n",
      "darknetaa53.c2ns_in1k\n",
      "davit_base.msft_in1k\n",
      "davit_small.msft_in1k\n",
      "davit_tiny.msft_in1k\n",
      "deit3_base_patch16_224.fb_in1k\n",
      "deit3_base_patch16_224.fb_in22k_ft_in1k\n",
      "deit3_base_patch16_384.fb_in1k\n",
      "deit3_base_patch16_384.fb_in22k_ft_in1k\n",
      "deit3_huge_patch14_224.fb_in1k\n",
      "deit3_huge_patch14_224.fb_in22k_ft_in1k\n",
      "deit3_large_patch16_224.fb_in1k\n",
      "deit3_large_patch16_224.fb_in22k_ft_in1k\n",
      "deit3_large_patch16_384.fb_in1k\n",
      "deit3_large_patch16_384.fb_in22k_ft_in1k\n",
      "deit3_medium_patch16_224.fb_in1k\n",
      "deit3_medium_patch16_224.fb_in22k_ft_in1k\n",
      "deit3_small_patch16_224.fb_in1k\n",
      "deit3_small_patch16_224.fb_in22k_ft_in1k\n",
      "deit3_small_patch16_384.fb_in1k\n",
      "deit3_small_patch16_384.fb_in22k_ft_in1k\n",
      "deit_base_distilled_patch16_224.fb_in1k\n",
      "deit_base_distilled_patch16_384.fb_in1k\n",
      "deit_base_patch16_224.fb_in1k\n",
      "deit_base_patch16_384.fb_in1k\n",
      "deit_small_distilled_patch16_224.fb_in1k\n",
      "deit_small_patch16_224.fb_in1k\n",
      "deit_tiny_distilled_patch16_224.fb_in1k\n",
      "deit_tiny_patch16_224.fb_in1k\n",
      "densenet121.ra_in1k\n",
      "densenet121.tv_in1k\n",
      "densenet161.tv_in1k\n",
      "densenet169.tv_in1k\n",
      "densenet201.tv_in1k\n",
      "densenetblur121d.ra_in1k\n",
      "dla34.in1k\n",
      "dla46_c.in1k\n",
      "dla46x_c.in1k\n",
      "dla60.in1k\n",
      "dla60_res2net.in1k\n",
      "dla60_res2next.in1k\n",
      "dla60x.in1k\n",
      "dla60x_c.in1k\n",
      "dla102.in1k\n",
      "dla102x2.in1k\n",
      "dla102x.in1k\n",
      "dla169.in1k\n",
      "dm_nfnet_f0.dm_in1k\n",
      "dm_nfnet_f1.dm_in1k\n",
      "dm_nfnet_f2.dm_in1k\n",
      "dm_nfnet_f3.dm_in1k\n",
      "dm_nfnet_f4.dm_in1k\n",
      "dm_nfnet_f5.dm_in1k\n",
      "dm_nfnet_f6.dm_in1k\n",
      "dpn68.mx_in1k\n",
      "dpn68b.mx_in1k\n",
      "dpn68b.ra_in1k\n",
      "dpn92.mx_in1k\n",
      "dpn98.mx_in1k\n",
      "dpn107.mx_in1k\n",
      "dpn131.mx_in1k\n",
      "eca_botnext26ts_256.c1_in1k\n",
      "eca_halonext26ts.c1_in1k\n",
      "eca_nfnet_l0.ra2_in1k\n",
      "eca_nfnet_l1.ra2_in1k\n",
      "eca_nfnet_l2.ra3_in1k\n",
      "eca_resnet33ts.ra2_in1k\n",
      "eca_resnext26ts.ch_in1k\n",
      "ecaresnet26t.ra2_in1k\n",
      "ecaresnet50d.miil_in1k\n",
      "ecaresnet50d_pruned.miil_in1k\n",
      "ecaresnet50t.a1_in1k\n",
      "ecaresnet50t.a2_in1k\n",
      "ecaresnet50t.a3_in1k\n",
      "ecaresnet50t.ra2_in1k\n",
      "ecaresnet101d.miil_in1k\n",
      "ecaresnet101d_pruned.miil_in1k\n",
      "ecaresnet269d.ra2_in1k\n",
      "ecaresnetlight.miil_in1k\n",
      "edgenext_base.in21k_ft_in1k\n",
      "edgenext_base.usi_in1k\n",
      "edgenext_small.usi_in1k\n",
      "edgenext_small_rw.sw_in1k\n",
      "edgenext_x_small.in1k\n",
      "edgenext_xx_small.in1k\n",
      "efficientformer_l1.snap_dist_in1k\n",
      "efficientformer_l3.snap_dist_in1k\n",
      "efficientformer_l7.snap_dist_in1k\n",
      "efficientformerv2_l.snap_dist_in1k\n",
      "efficientformerv2_s0.snap_dist_in1k\n",
      "efficientformerv2_s1.snap_dist_in1k\n",
      "efficientformerv2_s2.snap_dist_in1k\n",
      "efficientnet_b0.ra_in1k\n",
      "efficientnet_b1.ft_in1k\n",
      "efficientnet_b1_pruned.in1k\n",
      "efficientnet_b2.ra_in1k\n",
      "efficientnet_b2_pruned.in1k\n",
      "efficientnet_b3.ra2_in1k\n",
      "efficientnet_b3_pruned.in1k\n",
      "efficientnet_b4.ra2_in1k\n",
      "efficientnet_b5.sw_in12k\n",
      "efficientnet_b5.sw_in12k_ft_in1k\n",
      "efficientnet_el.ra_in1k\n",
      "efficientnet_el_pruned.in1k\n",
      "efficientnet_em.ra2_in1k\n",
      "efficientnet_es.ra_in1k\n",
      "efficientnet_es_pruned.in1k\n",
      "efficientnet_lite0.ra_in1k\n",
      "efficientnetv2_rw_m.agc_in1k\n",
      "efficientnetv2_rw_s.ra2_in1k\n",
      "efficientnetv2_rw_t.ra2_in1k\n",
      "efficientvit_b0.r224_in1k\n",
      "efficientvit_b1.r224_in1k\n",
      "efficientvit_b1.r256_in1k\n",
      "efficientvit_b1.r288_in1k\n",
      "efficientvit_b2.r224_in1k\n",
      "efficientvit_b2.r256_in1k\n",
      "efficientvit_b2.r288_in1k\n",
      "efficientvit_b3.r224_in1k\n",
      "efficientvit_b3.r256_in1k\n",
      "efficientvit_b3.r288_in1k\n",
      "efficientvit_l1.r224_in1k\n",
      "efficientvit_l2.r224_in1k\n",
      "efficientvit_l2.r256_in1k\n",
      "efficientvit_l2.r288_in1k\n",
      "efficientvit_l2.r384_in1k\n",
      "efficientvit_l3.r224_in1k\n",
      "efficientvit_l3.r256_in1k\n",
      "efficientvit_l3.r320_in1k\n",
      "efficientvit_l3.r384_in1k\n",
      "efficientvit_m0.r224_in1k\n",
      "efficientvit_m1.r224_in1k\n",
      "efficientvit_m2.r224_in1k\n",
      "efficientvit_m3.r224_in1k\n",
      "efficientvit_m4.r224_in1k\n",
      "efficientvit_m5.r224_in1k\n",
      "ese_vovnet19b_dw.ra_in1k\n",
      "ese_vovnet39b.ra_in1k\n",
      "eva02_base_patch14_224.mim_in22k\n",
      "eva02_base_patch14_448.mim_in22k_ft_in1k\n",
      "eva02_base_patch14_448.mim_in22k_ft_in22k\n",
      "eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\n",
      "eva02_base_patch16_clip_224.merged2b\n",
      "eva02_enormous_patch14_clip_224.laion2b\n",
      "eva02_enormous_patch14_clip_224.laion2b_plus\n",
      "eva02_large_patch14_224.mim_in22k\n",
      "eva02_large_patch14_224.mim_m38m\n",
      "eva02_large_patch14_448.mim_in22k_ft_in1k\n",
      "eva02_large_patch14_448.mim_in22k_ft_in22k\n",
      "eva02_large_patch14_448.mim_in22k_ft_in22k_in1k\n",
      "eva02_large_patch14_448.mim_m38m_ft_in1k\n",
      "eva02_large_patch14_448.mim_m38m_ft_in22k\n",
      "eva02_large_patch14_448.mim_m38m_ft_in22k_in1k\n",
      "eva02_large_patch14_clip_224.merged2b\n",
      "eva02_large_patch14_clip_336.merged2b\n",
      "eva02_small_patch14_224.mim_in22k\n",
      "eva02_small_patch14_336.mim_in22k_ft_in1k\n",
      "eva02_tiny_patch14_224.mim_in22k\n",
      "eva02_tiny_patch14_336.mim_in22k_ft_in1k\n",
      "eva_giant_patch14_224.clip_ft_in1k\n",
      "eva_giant_patch14_336.clip_ft_in1k\n",
      "eva_giant_patch14_336.m30m_ft_in22k_in1k\n",
      "eva_giant_patch14_560.m30m_ft_in22k_in1k\n",
      "eva_giant_patch14_clip_224.laion400m\n",
      "eva_giant_patch14_clip_224.merged2b\n",
      "eva_large_patch14_196.in22k_ft_in1k\n",
      "eva_large_patch14_196.in22k_ft_in22k_in1k\n",
      "eva_large_patch14_336.in22k_ft_in1k\n",
      "eva_large_patch14_336.in22k_ft_in22k_in1k\n",
      "fastvit_ma36.apple_dist_in1k\n",
      "fastvit_ma36.apple_in1k\n",
      "fastvit_s12.apple_dist_in1k\n",
      "fastvit_s12.apple_in1k\n",
      "fastvit_sa12.apple_dist_in1k\n",
      "fastvit_sa12.apple_in1k\n",
      "fastvit_sa24.apple_dist_in1k\n",
      "fastvit_sa24.apple_in1k\n",
      "fastvit_sa36.apple_dist_in1k\n",
      "fastvit_sa36.apple_in1k\n",
      "fastvit_t8.apple_dist_in1k\n",
      "fastvit_t8.apple_in1k\n",
      "fastvit_t12.apple_dist_in1k\n",
      "fastvit_t12.apple_in1k\n",
      "fbnetc_100.rmsp_in1k\n",
      "fbnetv3_b.ra2_in1k\n",
      "fbnetv3_d.ra2_in1k\n",
      "fbnetv3_g.ra2_in1k\n",
      "flexivit_base.300ep_in1k\n",
      "flexivit_base.300ep_in21k\n",
      "flexivit_base.600ep_in1k\n",
      "flexivit_base.1000ep_in21k\n",
      "flexivit_base.1200ep_in1k\n",
      "flexivit_base.patch16_in21k\n",
      "flexivit_base.patch30_in21k\n",
      "flexivit_large.300ep_in1k\n",
      "flexivit_large.600ep_in1k\n",
      "flexivit_large.1200ep_in1k\n",
      "flexivit_small.300ep_in1k\n",
      "flexivit_small.600ep_in1k\n",
      "flexivit_small.1200ep_in1k\n",
      "focalnet_base_lrf.ms_in1k\n",
      "focalnet_base_srf.ms_in1k\n",
      "focalnet_huge_fl3.ms_in22k\n",
      "focalnet_huge_fl4.ms_in22k\n",
      "focalnet_large_fl3.ms_in22k\n",
      "focalnet_large_fl4.ms_in22k\n",
      "focalnet_small_lrf.ms_in1k\n",
      "focalnet_small_srf.ms_in1k\n",
      "focalnet_tiny_lrf.ms_in1k\n",
      "focalnet_tiny_srf.ms_in1k\n",
      "focalnet_xlarge_fl3.ms_in22k\n",
      "focalnet_xlarge_fl4.ms_in22k\n",
      "gc_efficientnetv2_rw_t.agc_in1k\n",
      "gcresnet33ts.ra2_in1k\n",
      "gcresnet50t.ra2_in1k\n",
      "gcresnext26ts.ch_in1k\n",
      "gcresnext50ts.ch_in1k\n",
      "gcvit_base.in1k\n",
      "gcvit_small.in1k\n",
      "gcvit_tiny.in1k\n",
      "gcvit_xtiny.in1k\n",
      "gcvit_xxtiny.in1k\n",
      "gernet_l.idstcv_in1k\n",
      "gernet_m.idstcv_in1k\n",
      "gernet_s.idstcv_in1k\n",
      "ghostnet_100.in1k\n",
      "ghostnetv2_100.in1k\n",
      "ghostnetv2_130.in1k\n",
      "ghostnetv2_160.in1k\n",
      "gmixer_24_224.ra3_in1k\n",
      "gmlp_s16_224.ra3_in1k\n",
      "halo2botnet50ts_256.a1h_in1k\n",
      "halonet26t.a1h_in1k\n",
      "halonet50ts.a1h_in1k\n",
      "haloregnetz_b.ra3_in1k\n",
      "hardcorenas_a.miil_green_in1k\n",
      "hardcorenas_b.miil_green_in1k\n",
      "hardcorenas_c.miil_green_in1k\n",
      "hardcorenas_d.miil_green_in1k\n",
      "hardcorenas_e.miil_green_in1k\n",
      "hardcorenas_f.miil_green_in1k\n",
      "hgnet_base.ssld_in1k\n",
      "hgnet_small.paddle_in1k\n",
      "hgnet_small.ssld_in1k\n",
      "hgnet_tiny.paddle_in1k\n",
      "hgnet_tiny.ssld_in1k\n",
      "hgnetv2_b0.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b0.ssld_stage2_ft_in1k\n",
      "hgnetv2_b1.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b1.ssld_stage2_ft_in1k\n",
      "hgnetv2_b2.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b2.ssld_stage2_ft_in1k\n",
      "hgnetv2_b3.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b3.ssld_stage2_ft_in1k\n",
      "hgnetv2_b4.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b4.ssld_stage2_ft_in1k\n",
      "hgnetv2_b5.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b5.ssld_stage2_ft_in1k\n",
      "hgnetv2_b6.ssld_stage1_in22k_in1k\n",
      "hgnetv2_b6.ssld_stage2_ft_in1k\n",
      "hiera_base_224.mae\n",
      "hiera_base_224.mae_in1k_ft_in1k\n",
      "hiera_base_plus_224.mae\n",
      "hiera_base_plus_224.mae_in1k_ft_in1k\n",
      "hiera_huge_224.mae\n",
      "hiera_huge_224.mae_in1k_ft_in1k\n",
      "hiera_large_224.mae\n",
      "hiera_large_224.mae_in1k_ft_in1k\n",
      "hiera_small_224.mae\n",
      "hiera_small_224.mae_in1k_ft_in1k\n",
      "hiera_tiny_224.mae\n",
      "hiera_tiny_224.mae_in1k_ft_in1k\n",
      "hrnet_w18.ms_aug_in1k\n",
      "hrnet_w18.ms_in1k\n",
      "hrnet_w18_small.gluon_in1k\n",
      "hrnet_w18_small.ms_in1k\n",
      "hrnet_w18_small_v2.gluon_in1k\n",
      "hrnet_w18_small_v2.ms_in1k\n",
      "hrnet_w18_ssld.paddle_in1k\n",
      "hrnet_w30.ms_in1k\n",
      "hrnet_w32.ms_in1k\n",
      "hrnet_w40.ms_in1k\n",
      "hrnet_w44.ms_in1k\n",
      "hrnet_w48.ms_in1k\n",
      "hrnet_w48_ssld.paddle_in1k\n",
      "hrnet_w64.ms_in1k\n",
      "inception_next_base.sail_in1k\n",
      "inception_next_base.sail_in1k_384\n",
      "inception_next_small.sail_in1k\n",
      "inception_next_tiny.sail_in1k\n",
      "inception_resnet_v2.tf_ens_adv_in1k\n",
      "inception_resnet_v2.tf_in1k\n",
      "inception_v3.gluon_in1k\n",
      "inception_v3.tf_adv_in1k\n",
      "inception_v3.tf_in1k\n",
      "inception_v3.tv_in1k\n",
      "inception_v4.tf_in1k\n",
      "lambda_resnet26rpt_256.c1_in1k\n",
      "lambda_resnet26t.c1_in1k\n",
      "lambda_resnet50ts.a1h_in1k\n",
      "lamhalobotnet50ts_256.a1h_in1k\n",
      "lcnet_050.ra2_in1k\n",
      "lcnet_075.ra2_in1k\n",
      "lcnet_100.ra2_in1k\n",
      "legacy_senet154.in1k\n",
      "legacy_seresnet18.in1k\n",
      "legacy_seresnet34.in1k\n",
      "legacy_seresnet50.in1k\n",
      "legacy_seresnet101.in1k\n",
      "legacy_seresnet152.in1k\n",
      "legacy_seresnext26_32x4d.in1k\n",
      "legacy_seresnext50_32x4d.in1k\n",
      "legacy_seresnext101_32x4d.in1k\n",
      "legacy_xception.tf_in1k\n",
      "levit_128.fb_dist_in1k\n",
      "levit_128s.fb_dist_in1k\n",
      "levit_192.fb_dist_in1k\n",
      "levit_256.fb_dist_in1k\n",
      "levit_384.fb_dist_in1k\n",
      "levit_conv_128.fb_dist_in1k\n",
      "levit_conv_128s.fb_dist_in1k\n",
      "levit_conv_192.fb_dist_in1k\n",
      "levit_conv_256.fb_dist_in1k\n",
      "levit_conv_384.fb_dist_in1k\n",
      "maxvit_base_tf_224.in1k\n",
      "maxvit_base_tf_224.in21k\n",
      "maxvit_base_tf_384.in1k\n",
      "maxvit_base_tf_384.in21k_ft_in1k\n",
      "maxvit_base_tf_512.in1k\n",
      "maxvit_base_tf_512.in21k_ft_in1k\n",
      "maxvit_large_tf_224.in1k\n",
      "maxvit_large_tf_224.in21k\n",
      "maxvit_large_tf_384.in1k\n",
      "maxvit_large_tf_384.in21k_ft_in1k\n",
      "maxvit_large_tf_512.in1k\n",
      "maxvit_large_tf_512.in21k_ft_in1k\n",
      "maxvit_nano_rw_256.sw_in1k\n",
      "maxvit_rmlp_base_rw_224.sw_in12k\n",
      "maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k\n",
      "maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k\n",
      "maxvit_rmlp_nano_rw_256.sw_in1k\n",
      "maxvit_rmlp_pico_rw_256.sw_in1k\n",
      "maxvit_rmlp_small_rw_224.sw_in1k\n",
      "maxvit_rmlp_tiny_rw_256.sw_in1k\n",
      "maxvit_small_tf_224.in1k\n",
      "maxvit_small_tf_384.in1k\n",
      "maxvit_small_tf_512.in1k\n",
      "maxvit_tiny_rw_224.sw_in1k\n",
      "maxvit_tiny_tf_224.in1k\n",
      "maxvit_tiny_tf_384.in1k\n",
      "maxvit_tiny_tf_512.in1k\n",
      "maxvit_xlarge_tf_224.in21k\n",
      "maxvit_xlarge_tf_384.in21k_ft_in1k\n",
      "maxvit_xlarge_tf_512.in21k_ft_in1k\n",
      "maxxvit_rmlp_nano_rw_256.sw_in1k\n",
      "maxxvit_rmlp_small_rw_256.sw_in1k\n",
      "maxxvitv2_nano_rw_256.sw_in1k\n",
      "maxxvitv2_rmlp_base_rw_224.sw_in12k\n",
      "maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k\n",
      "maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k\n",
      "mixer_b16_224.goog_in21k\n",
      "mixer_b16_224.goog_in21k_ft_in1k\n",
      "mixer_b16_224.miil_in21k\n",
      "mixer_b16_224.miil_in21k_ft_in1k\n",
      "mixer_l16_224.goog_in21k\n",
      "mixer_l16_224.goog_in21k_ft_in1k\n",
      "mixnet_l.ft_in1k\n",
      "mixnet_m.ft_in1k\n",
      "mixnet_s.ft_in1k\n",
      "mixnet_xl.ra_in1k\n",
      "mnasnet_100.rmsp_in1k\n",
      "mnasnet_small.lamb_in1k\n",
      "mobilenetv2_050.lamb_in1k\n",
      "mobilenetv2_100.ra_in1k\n",
      "mobilenetv2_110d.ra_in1k\n",
      "mobilenetv2_120d.ra_in1k\n",
      "mobilenetv2_140.ra_in1k\n",
      "mobilenetv3_large_100.miil_in21k\n",
      "mobilenetv3_large_100.miil_in21k_ft_in1k\n",
      "mobilenetv3_large_100.ra_in1k\n",
      "mobilenetv3_rw.rmsp_in1k\n",
      "mobilenetv3_small_050.lamb_in1k\n",
      "mobilenetv3_small_075.lamb_in1k\n",
      "mobilenetv3_small_100.lamb_in1k\n",
      "mobileone_s0.apple_in1k\n",
      "mobileone_s1.apple_in1k\n",
      "mobileone_s2.apple_in1k\n",
      "mobileone_s3.apple_in1k\n",
      "mobileone_s4.apple_in1k\n",
      "mobilevit_s.cvnets_in1k\n",
      "mobilevit_xs.cvnets_in1k\n",
      "mobilevit_xxs.cvnets_in1k\n",
      "mobilevitv2_050.cvnets_in1k\n",
      "mobilevitv2_075.cvnets_in1k\n",
      "mobilevitv2_100.cvnets_in1k\n",
      "mobilevitv2_125.cvnets_in1k\n",
      "mobilevitv2_150.cvnets_in1k\n",
      "mobilevitv2_150.cvnets_in22k_ft_in1k\n",
      "mobilevitv2_150.cvnets_in22k_ft_in1k_384\n",
      "mobilevitv2_175.cvnets_in1k\n",
      "mobilevitv2_175.cvnets_in22k_ft_in1k\n",
      "mobilevitv2_175.cvnets_in22k_ft_in1k_384\n",
      "mobilevitv2_200.cvnets_in1k\n",
      "mobilevitv2_200.cvnets_in22k_ft_in1k\n",
      "mobilevitv2_200.cvnets_in22k_ft_in1k_384\n",
      "mvitv2_base.fb_in1k\n",
      "mvitv2_base_cls.fb_inw21k\n",
      "mvitv2_huge_cls.fb_inw21k\n",
      "mvitv2_large.fb_in1k\n",
      "mvitv2_large_cls.fb_inw21k\n",
      "mvitv2_small.fb_in1k\n",
      "mvitv2_tiny.fb_in1k\n",
      "nasnetalarge.tf_in1k\n",
      "nest_base_jx.goog_in1k\n",
      "nest_small_jx.goog_in1k\n",
      "nest_tiny_jx.goog_in1k\n",
      "nextvit_base.bd_in1k\n",
      "nextvit_base.bd_in1k_384\n",
      "nextvit_base.bd_ssld_6m_in1k\n",
      "nextvit_base.bd_ssld_6m_in1k_384\n",
      "nextvit_large.bd_in1k\n",
      "nextvit_large.bd_in1k_384\n",
      "nextvit_large.bd_ssld_6m_in1k\n",
      "nextvit_large.bd_ssld_6m_in1k_384\n",
      "nextvit_small.bd_in1k\n",
      "nextvit_small.bd_in1k_384\n",
      "nextvit_small.bd_ssld_6m_in1k\n",
      "nextvit_small.bd_ssld_6m_in1k_384\n",
      "nf_regnet_b1.ra2_in1k\n",
      "nf_resnet50.ra2_in1k\n",
      "nfnet_l0.ra2_in1k\n",
      "pit_b_224.in1k\n",
      "pit_b_distilled_224.in1k\n",
      "pit_s_224.in1k\n",
      "pit_s_distilled_224.in1k\n",
      "pit_ti_224.in1k\n",
      "pit_ti_distilled_224.in1k\n",
      "pit_xs_224.in1k\n",
      "pit_xs_distilled_224.in1k\n",
      "pnasnet5large.tf_in1k\n",
      "poolformer_m36.sail_in1k\n",
      "poolformer_m48.sail_in1k\n",
      "poolformer_s12.sail_in1k\n",
      "poolformer_s24.sail_in1k\n",
      "poolformer_s36.sail_in1k\n",
      "poolformerv2_m36.sail_in1k\n",
      "poolformerv2_m48.sail_in1k\n",
      "poolformerv2_s12.sail_in1k\n",
      "poolformerv2_s24.sail_in1k\n",
      "poolformerv2_s36.sail_in1k\n",
      "pvt_v2_b0.in1k\n",
      "pvt_v2_b1.in1k\n",
      "pvt_v2_b2.in1k\n",
      "pvt_v2_b2_li.in1k\n",
      "pvt_v2_b3.in1k\n",
      "pvt_v2_b4.in1k\n",
      "pvt_v2_b5.in1k\n",
      "regnetv_040.ra3_in1k\n",
      "regnetv_064.ra3_in1k\n",
      "regnetx_002.pycls_in1k\n",
      "regnetx_004.pycls_in1k\n",
      "regnetx_004_tv.tv2_in1k\n",
      "regnetx_006.pycls_in1k\n",
      "regnetx_008.pycls_in1k\n",
      "regnetx_008.tv2_in1k\n",
      "regnetx_016.pycls_in1k\n",
      "regnetx_016.tv2_in1k\n",
      "regnetx_032.pycls_in1k\n",
      "regnetx_032.tv2_in1k\n",
      "regnetx_040.pycls_in1k\n",
      "regnetx_064.pycls_in1k\n",
      "regnetx_080.pycls_in1k\n",
      "regnetx_080.tv2_in1k\n",
      "regnetx_120.pycls_in1k\n",
      "regnetx_160.pycls_in1k\n",
      "regnetx_160.tv2_in1k\n",
      "regnetx_320.pycls_in1k\n",
      "regnetx_320.tv2_in1k\n",
      "regnety_002.pycls_in1k\n",
      "regnety_004.pycls_in1k\n",
      "regnety_004.tv2_in1k\n",
      "regnety_006.pycls_in1k\n",
      "regnety_008.pycls_in1k\n",
      "regnety_008_tv.tv2_in1k\n",
      "regnety_016.pycls_in1k\n",
      "regnety_016.tv2_in1k\n",
      "regnety_032.pycls_in1k\n",
      "regnety_032.ra_in1k\n",
      "regnety_032.tv2_in1k\n",
      "regnety_040.pycls_in1k\n",
      "regnety_040.ra3_in1k\n",
      "regnety_064.pycls_in1k\n",
      "regnety_064.ra3_in1k\n",
      "regnety_080.pycls_in1k\n",
      "regnety_080.ra3_in1k\n",
      "regnety_080_tv.tv2_in1k\n",
      "regnety_120.pycls_in1k\n",
      "regnety_120.sw_in12k\n",
      "regnety_120.sw_in12k_ft_in1k\n",
      "regnety_160.deit_in1k\n",
      "regnety_160.lion_in12k_ft_in1k\n",
      "regnety_160.pycls_in1k\n",
      "regnety_160.sw_in12k\n",
      "regnety_160.sw_in12k_ft_in1k\n",
      "regnety_160.swag_ft_in1k\n",
      "regnety_160.swag_lc_in1k\n",
      "regnety_160.tv2_in1k\n",
      "regnety_320.pycls_in1k\n",
      "regnety_320.seer\n",
      "regnety_320.seer_ft_in1k\n",
      "regnety_320.swag_ft_in1k\n",
      "regnety_320.swag_lc_in1k\n",
      "regnety_320.tv2_in1k\n",
      "regnety_640.seer\n",
      "regnety_640.seer_ft_in1k\n",
      "regnety_1280.seer\n",
      "regnety_1280.seer_ft_in1k\n",
      "regnety_1280.swag_ft_in1k\n",
      "regnety_1280.swag_lc_in1k\n",
      "regnety_2560.seer_ft_in1k\n",
      "regnetz_040.ra3_in1k\n",
      "regnetz_040_h.ra3_in1k\n",
      "regnetz_b16.ra3_in1k\n",
      "regnetz_c16.ra3_in1k\n",
      "regnetz_c16_evos.ch_in1k\n",
      "regnetz_d8.ra3_in1k\n",
      "regnetz_d8_evos.ch_in1k\n",
      "regnetz_d32.ra3_in1k\n",
      "regnetz_e8.ra3_in1k\n",
      "repghostnet_050.in1k\n",
      "repghostnet_058.in1k\n",
      "repghostnet_080.in1k\n",
      "repghostnet_100.in1k\n",
      "repghostnet_111.in1k\n",
      "repghostnet_130.in1k\n",
      "repghostnet_150.in1k\n",
      "repghostnet_200.in1k\n",
      "repvgg_a0.rvgg_in1k\n",
      "repvgg_a1.rvgg_in1k\n",
      "repvgg_a2.rvgg_in1k\n",
      "repvgg_b0.rvgg_in1k\n",
      "repvgg_b1.rvgg_in1k\n",
      "repvgg_b1g4.rvgg_in1k\n",
      "repvgg_b2.rvgg_in1k\n",
      "repvgg_b2g4.rvgg_in1k\n",
      "repvgg_b3.rvgg_in1k\n",
      "repvgg_b3g4.rvgg_in1k\n",
      "repvgg_d2se.rvgg_in1k\n",
      "repvit_m0_9.dist_300e_in1k\n",
      "repvit_m0_9.dist_450e_in1k\n",
      "repvit_m1.dist_in1k\n",
      "repvit_m1_0.dist_300e_in1k\n",
      "repvit_m1_0.dist_450e_in1k\n",
      "repvit_m1_1.dist_300e_in1k\n",
      "repvit_m1_1.dist_450e_in1k\n",
      "repvit_m1_5.dist_300e_in1k\n",
      "repvit_m1_5.dist_450e_in1k\n",
      "repvit_m2.dist_in1k\n",
      "repvit_m2_3.dist_300e_in1k\n",
      "repvit_m2_3.dist_450e_in1k\n",
      "repvit_m3.dist_in1k\n",
      "res2net50_14w_8s.in1k\n",
      "res2net50_26w_4s.in1k\n",
      "res2net50_26w_6s.in1k\n",
      "res2net50_26w_8s.in1k\n",
      "res2net50_48w_2s.in1k\n",
      "res2net50d.in1k\n",
      "res2net101_26w_4s.in1k\n",
      "res2net101d.in1k\n",
      "res2next50.in1k\n",
      "resmlp_12_224.fb_dino\n",
      "resmlp_12_224.fb_distilled_in1k\n",
      "resmlp_12_224.fb_in1k\n",
      "resmlp_24_224.fb_dino\n",
      "resmlp_24_224.fb_distilled_in1k\n",
      "resmlp_24_224.fb_in1k\n",
      "resmlp_36_224.fb_distilled_in1k\n",
      "resmlp_36_224.fb_in1k\n",
      "resmlp_big_24_224.fb_distilled_in1k\n",
      "resmlp_big_24_224.fb_in1k\n",
      "resmlp_big_24_224.fb_in22k_ft_in1k\n",
      "resnest14d.gluon_in1k\n",
      "resnest26d.gluon_in1k\n",
      "resnest50d.in1k\n",
      "resnest50d_1s4x24d.in1k\n",
      "resnest50d_4s2x40d.in1k\n",
      "resnest101e.in1k\n",
      "resnest200e.in1k\n",
      "resnest269e.in1k\n",
      "resnet10t.c3_in1k\n",
      "resnet14t.c3_in1k\n",
      "resnet18.a1_in1k\n",
      "resnet18.a2_in1k\n",
      "resnet18.a3_in1k\n",
      "resnet18.fb_ssl_yfcc100m_ft_in1k\n",
      "resnet18.fb_swsl_ig1b_ft_in1k\n",
      "resnet18.gluon_in1k\n",
      "resnet18.tv_in1k\n",
      "resnet18d.ra2_in1k\n",
      "resnet26.bt_in1k\n",
      "resnet26d.bt_in1k\n",
      "resnet26t.ra2_in1k\n",
      "resnet32ts.ra2_in1k\n",
      "resnet33ts.ra2_in1k\n",
      "resnet34.a1_in1k\n",
      "resnet34.a2_in1k\n",
      "resnet34.a3_in1k\n",
      "resnet34.bt_in1k\n",
      "resnet34.gluon_in1k\n",
      "resnet34.tv_in1k\n",
      "resnet34d.ra2_in1k\n",
      "resnet50.a1_in1k\n",
      "resnet50.a1h_in1k\n",
      "resnet50.a2_in1k\n",
      "resnet50.a3_in1k\n",
      "resnet50.am_in1k\n",
      "resnet50.b1k_in1k\n",
      "resnet50.b2k_in1k\n",
      "resnet50.bt_in1k\n",
      "resnet50.c1_in1k\n",
      "resnet50.c2_in1k\n",
      "resnet50.d_in1k\n",
      "resnet50.fb_ssl_yfcc100m_ft_in1k\n",
      "resnet50.fb_swsl_ig1b_ft_in1k\n",
      "resnet50.gluon_in1k\n",
      "resnet50.ra_in1k\n",
      "resnet50.ram_in1k\n",
      "resnet50.tv2_in1k\n",
      "resnet50.tv_in1k\n",
      "resnet50_gn.a1h_in1k\n",
      "resnet50c.gluon_in1k\n",
      "resnet50d.a1_in1k\n",
      "resnet50d.a2_in1k\n",
      "resnet50d.a3_in1k\n",
      "resnet50d.gluon_in1k\n",
      "resnet50d.ra2_in1k\n",
      "resnet50s.gluon_in1k\n",
      "resnet51q.ra2_in1k\n",
      "resnet61q.ra2_in1k\n",
      "resnet101.a1_in1k\n",
      "resnet101.a1h_in1k\n",
      "resnet101.a2_in1k\n",
      "resnet101.a3_in1k\n",
      "resnet101.gluon_in1k\n",
      "resnet101.tv2_in1k\n",
      "resnet101.tv_in1k\n",
      "resnet101c.gluon_in1k\n",
      "resnet101d.gluon_in1k\n",
      "resnet101d.ra2_in1k\n",
      "resnet101s.gluon_in1k\n",
      "resnet152.a1_in1k\n",
      "resnet152.a1h_in1k\n",
      "resnet152.a2_in1k\n",
      "resnet152.a3_in1k\n",
      "resnet152.gluon_in1k\n",
      "resnet152.tv2_in1k\n",
      "resnet152.tv_in1k\n",
      "resnet152c.gluon_in1k\n",
      "resnet152d.gluon_in1k\n",
      "resnet152d.ra2_in1k\n",
      "resnet152s.gluon_in1k\n",
      "resnet200d.ra2_in1k\n",
      "resnetaa50.a1h_in1k\n",
      "resnetaa50d.d_in12k\n",
      "resnetaa50d.sw_in12k\n",
      "resnetaa50d.sw_in12k_ft_in1k\n",
      "resnetaa101d.sw_in12k\n",
      "resnetaa101d.sw_in12k_ft_in1k\n",
      "resnetblur50.bt_in1k\n",
      "resnetrs50.tf_in1k\n",
      "resnetrs101.tf_in1k\n",
      "resnetrs152.tf_in1k\n",
      "resnetrs200.tf_in1k\n",
      "resnetrs270.tf_in1k\n",
      "resnetrs350.tf_in1k\n",
      "resnetrs420.tf_in1k\n",
      "resnetv2_50.a1h_in1k\n",
      "resnetv2_50d_evos.ah_in1k\n",
      "resnetv2_50d_gn.ah_in1k\n",
      "resnetv2_50x1_bit.goog_distilled_in1k\n",
      "resnetv2_50x1_bit.goog_in21k\n",
      "resnetv2_50x1_bit.goog_in21k_ft_in1k\n",
      "resnetv2_50x3_bit.goog_in21k\n",
      "resnetv2_50x3_bit.goog_in21k_ft_in1k\n",
      "resnetv2_101.a1h_in1k\n",
      "resnetv2_101x1_bit.goog_in21k\n",
      "resnetv2_101x1_bit.goog_in21k_ft_in1k\n",
      "resnetv2_101x3_bit.goog_in21k\n",
      "resnetv2_101x3_bit.goog_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_in21k\n",
      "resnetv2_152x2_bit.goog_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k\n",
      "resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384\n",
      "resnetv2_152x4_bit.goog_in21k\n",
      "resnetv2_152x4_bit.goog_in21k_ft_in1k\n",
      "resnext26ts.ra2_in1k\n",
      "resnext50_32x4d.a1_in1k\n",
      "resnext50_32x4d.a1h_in1k\n",
      "resnext50_32x4d.a2_in1k\n",
      "resnext50_32x4d.a3_in1k\n",
      "resnext50_32x4d.fb_ssl_yfcc100m_ft_in1k\n",
      "resnext50_32x4d.fb_swsl_ig1b_ft_in1k\n",
      "resnext50_32x4d.gluon_in1k\n",
      "resnext50_32x4d.ra_in1k\n",
      "resnext50_32x4d.tv2_in1k\n",
      "resnext50_32x4d.tv_in1k\n",
      "resnext50d_32x4d.bt_in1k\n",
      "resnext101_32x4d.fb_ssl_yfcc100m_ft_in1k\n",
      "resnext101_32x4d.fb_swsl_ig1b_ft_in1k\n",
      "resnext101_32x4d.gluon_in1k\n",
      "resnext101_32x8d.fb_ssl_yfcc100m_ft_in1k\n",
      "resnext101_32x8d.fb_swsl_ig1b_ft_in1k\n",
      "resnext101_32x8d.fb_wsl_ig1b_ft_in1k\n",
      "resnext101_32x8d.tv2_in1k\n",
      "resnext101_32x8d.tv_in1k\n",
      "resnext101_32x16d.fb_ssl_yfcc100m_ft_in1k\n",
      "resnext101_32x16d.fb_swsl_ig1b_ft_in1k\n",
      "resnext101_32x16d.fb_wsl_ig1b_ft_in1k\n",
      "resnext101_32x32d.fb_wsl_ig1b_ft_in1k\n",
      "resnext101_64x4d.c1_in1k\n",
      "resnext101_64x4d.gluon_in1k\n",
      "resnext101_64x4d.tv_in1k\n",
      "rexnet_100.nav_in1k\n",
      "rexnet_130.nav_in1k\n",
      "rexnet_150.nav_in1k\n",
      "rexnet_200.nav_in1k\n",
      "rexnet_300.nav_in1k\n",
      "rexnetr_200.sw_in12k\n",
      "rexnetr_200.sw_in12k_ft_in1k\n",
      "rexnetr_300.sw_in12k\n",
      "rexnetr_300.sw_in12k_ft_in1k\n",
      "samvit_base_patch16.sa1b\n",
      "samvit_huge_patch16.sa1b\n",
      "samvit_large_patch16.sa1b\n",
      "sebotnet33ts_256.a1h_in1k\n",
      "sehalonet33ts.ra2_in1k\n",
      "selecsls42b.in1k\n",
      "selecsls60.in1k\n",
      "selecsls60b.in1k\n",
      "semnasnet_075.rmsp_in1k\n",
      "semnasnet_100.rmsp_in1k\n",
      "senet154.gluon_in1k\n",
      "sequencer2d_l.in1k\n",
      "sequencer2d_m.in1k\n",
      "sequencer2d_s.in1k\n",
      "seresnet33ts.ra2_in1k\n",
      "seresnet50.a1_in1k\n",
      "seresnet50.a2_in1k\n",
      "seresnet50.a3_in1k\n",
      "seresnet50.ra2_in1k\n",
      "seresnet152d.ra2_in1k\n",
      "seresnext26d_32x4d.bt_in1k\n",
      "seresnext26t_32x4d.bt_in1k\n",
      "seresnext26ts.ch_in1k\n",
      "seresnext50_32x4d.gluon_in1k\n",
      "seresnext50_32x4d.racm_in1k\n",
      "seresnext101_32x4d.gluon_in1k\n",
      "seresnext101_32x8d.ah_in1k\n",
      "seresnext101_64x4d.gluon_in1k\n",
      "seresnext101d_32x8d.ah_in1k\n",
      "seresnextaa101d_32x8d.ah_in1k\n",
      "seresnextaa101d_32x8d.sw_in12k\n",
      "seresnextaa101d_32x8d.sw_in12k_ft_in1k\n",
      "seresnextaa101d_32x8d.sw_in12k_ft_in1k_288\n",
      "seresnextaa201d_32x8d.sw_in12k\n",
      "seresnextaa201d_32x8d.sw_in12k_ft_in1k_384\n",
      "skresnet18.ra_in1k\n",
      "skresnet34.ra_in1k\n",
      "skresnext50_32x4d.ra_in1k\n",
      "spnasnet_100.rmsp_in1k\n",
      "swin_base_patch4_window7_224.ms_in1k\n",
      "swin_base_patch4_window7_224.ms_in22k\n",
      "swin_base_patch4_window7_224.ms_in22k_ft_in1k\n",
      "swin_base_patch4_window12_384.ms_in1k\n",
      "swin_base_patch4_window12_384.ms_in22k\n",
      "swin_base_patch4_window12_384.ms_in22k_ft_in1k\n",
      "swin_large_patch4_window7_224.ms_in22k\n",
      "swin_large_patch4_window7_224.ms_in22k_ft_in1k\n",
      "swin_large_patch4_window12_384.ms_in22k\n",
      "swin_large_patch4_window12_384.ms_in22k_ft_in1k\n",
      "swin_s3_base_224.ms_in1k\n",
      "swin_s3_small_224.ms_in1k\n",
      "swin_s3_tiny_224.ms_in1k\n",
      "swin_small_patch4_window7_224.ms_in1k\n",
      "swin_small_patch4_window7_224.ms_in22k\n",
      "swin_small_patch4_window7_224.ms_in22k_ft_in1k\n",
      "swin_tiny_patch4_window7_224.ms_in1k\n",
      "swin_tiny_patch4_window7_224.ms_in22k\n",
      "swin_tiny_patch4_window7_224.ms_in22k_ft_in1k\n",
      "swinv2_base_window8_256.ms_in1k\n",
      "swinv2_base_window12_192.ms_in22k\n",
      "swinv2_base_window12to16_192to256.ms_in22k_ft_in1k\n",
      "swinv2_base_window12to24_192to384.ms_in22k_ft_in1k\n",
      "swinv2_base_window16_256.ms_in1k\n",
      "swinv2_cr_small_224.sw_in1k\n",
      "swinv2_cr_small_ns_224.sw_in1k\n",
      "swinv2_cr_tiny_ns_224.sw_in1k\n",
      "swinv2_large_window12_192.ms_in22k\n",
      "swinv2_large_window12to16_192to256.ms_in22k_ft_in1k\n",
      "swinv2_large_window12to24_192to384.ms_in22k_ft_in1k\n",
      "swinv2_small_window8_256.ms_in1k\n",
      "swinv2_small_window16_256.ms_in1k\n",
      "swinv2_tiny_window8_256.ms_in1k\n",
      "swinv2_tiny_window16_256.ms_in1k\n",
      "tf_efficientnet_b0.aa_in1k\n",
      "tf_efficientnet_b0.ap_in1k\n",
      "tf_efficientnet_b0.in1k\n",
      "tf_efficientnet_b0.ns_jft_in1k\n",
      "tf_efficientnet_b1.aa_in1k\n",
      "tf_efficientnet_b1.ap_in1k\n",
      "tf_efficientnet_b1.in1k\n",
      "tf_efficientnet_b1.ns_jft_in1k\n",
      "tf_efficientnet_b2.aa_in1k\n",
      "tf_efficientnet_b2.ap_in1k\n",
      "tf_efficientnet_b2.in1k\n",
      "tf_efficientnet_b2.ns_jft_in1k\n",
      "tf_efficientnet_b3.aa_in1k\n",
      "tf_efficientnet_b3.ap_in1k\n",
      "tf_efficientnet_b3.in1k\n",
      "tf_efficientnet_b3.ns_jft_in1k\n",
      "tf_efficientnet_b4.aa_in1k\n",
      "tf_efficientnet_b4.ap_in1k\n",
      "tf_efficientnet_b4.in1k\n",
      "tf_efficientnet_b4.ns_jft_in1k\n",
      "tf_efficientnet_b5.aa_in1k\n",
      "tf_efficientnet_b5.ap_in1k\n",
      "tf_efficientnet_b5.in1k\n",
      "tf_efficientnet_b5.ns_jft_in1k\n",
      "tf_efficientnet_b5.ra_in1k\n",
      "tf_efficientnet_b6.aa_in1k\n",
      "tf_efficientnet_b6.ap_in1k\n",
      "tf_efficientnet_b6.ns_jft_in1k\n",
      "tf_efficientnet_b7.aa_in1k\n",
      "tf_efficientnet_b7.ap_in1k\n",
      "tf_efficientnet_b7.ns_jft_in1k\n",
      "tf_efficientnet_b7.ra_in1k\n",
      "tf_efficientnet_b8.ap_in1k\n",
      "tf_efficientnet_b8.ra_in1k\n",
      "tf_efficientnet_cc_b0_4e.in1k\n",
      "tf_efficientnet_cc_b0_8e.in1k\n",
      "tf_efficientnet_cc_b1_8e.in1k\n",
      "tf_efficientnet_el.in1k\n",
      "tf_efficientnet_em.in1k\n",
      "tf_efficientnet_es.in1k\n",
      "tf_efficientnet_l2.ns_jft_in1k\n",
      "tf_efficientnet_l2.ns_jft_in1k_475\n",
      "tf_efficientnet_lite0.in1k\n",
      "tf_efficientnet_lite1.in1k\n",
      "tf_efficientnet_lite2.in1k\n",
      "tf_efficientnet_lite3.in1k\n",
      "tf_efficientnet_lite4.in1k\n",
      "tf_efficientnetv2_b0.in1k\n",
      "tf_efficientnetv2_b1.in1k\n",
      "tf_efficientnetv2_b2.in1k\n",
      "tf_efficientnetv2_b3.in1k\n",
      "tf_efficientnetv2_b3.in21k\n",
      "tf_efficientnetv2_b3.in21k_ft_in1k\n",
      "tf_efficientnetv2_l.in1k\n",
      "tf_efficientnetv2_l.in21k\n",
      "tf_efficientnetv2_l.in21k_ft_in1k\n",
      "tf_efficientnetv2_m.in1k\n",
      "tf_efficientnetv2_m.in21k\n",
      "tf_efficientnetv2_m.in21k_ft_in1k\n",
      "tf_efficientnetv2_s.in1k\n",
      "tf_efficientnetv2_s.in21k\n",
      "tf_efficientnetv2_s.in21k_ft_in1k\n",
      "tf_efficientnetv2_xl.in21k\n",
      "tf_efficientnetv2_xl.in21k_ft_in1k\n",
      "tf_mixnet_l.in1k\n",
      "tf_mixnet_m.in1k\n",
      "tf_mixnet_s.in1k\n",
      "tf_mobilenetv3_large_075.in1k\n",
      "tf_mobilenetv3_large_100.in1k\n",
      "tf_mobilenetv3_large_minimal_100.in1k\n",
      "tf_mobilenetv3_small_075.in1k\n",
      "tf_mobilenetv3_small_100.in1k\n",
      "tf_mobilenetv3_small_minimal_100.in1k\n",
      "tiny_vit_5m_224.dist_in22k\n",
      "tiny_vit_5m_224.dist_in22k_ft_in1k\n",
      "tiny_vit_5m_224.in1k\n",
      "tiny_vit_11m_224.dist_in22k\n",
      "tiny_vit_11m_224.dist_in22k_ft_in1k\n",
      "tiny_vit_11m_224.in1k\n",
      "tiny_vit_21m_224.dist_in22k\n",
      "tiny_vit_21m_224.dist_in22k_ft_in1k\n",
      "tiny_vit_21m_224.in1k\n",
      "tiny_vit_21m_384.dist_in22k_ft_in1k\n",
      "tiny_vit_21m_512.dist_in22k_ft_in1k\n",
      "tinynet_a.in1k\n",
      "tinynet_b.in1k\n",
      "tinynet_c.in1k\n",
      "tinynet_d.in1k\n",
      "tinynet_e.in1k\n",
      "tnt_s_patch16_224\n",
      "tresnet_l.miil_in1k\n",
      "tresnet_l.miil_in1k_448\n",
      "tresnet_m.miil_in1k\n",
      "tresnet_m.miil_in1k_448\n",
      "tresnet_m.miil_in21k\n",
      "tresnet_m.miil_in21k_ft_in1k\n",
      "tresnet_v2_l.miil_in21k\n",
      "tresnet_v2_l.miil_in21k_ft_in1k\n",
      "tresnet_xl.miil_in1k\n",
      "tresnet_xl.miil_in1k_448\n",
      "twins_pcpvt_base.in1k\n",
      "twins_pcpvt_large.in1k\n",
      "twins_pcpvt_small.in1k\n",
      "twins_svt_base.in1k\n",
      "twins_svt_large.in1k\n",
      "twins_svt_small.in1k\n",
      "vgg11.tv_in1k\n",
      "vgg11_bn.tv_in1k\n",
      "vgg13.tv_in1k\n",
      "vgg13_bn.tv_in1k\n",
      "vgg16.tv_in1k\n",
      "vgg16_bn.tv_in1k\n",
      "vgg19.tv_in1k\n",
      "vgg19_bn.tv_in1k\n",
      "visformer_small.in1k\n",
      "visformer_tiny.in1k\n",
      "vit_base_patch8_224.augreg2_in21k_ft_in1k\n",
      "vit_base_patch8_224.augreg_in21k\n",
      "vit_base_patch8_224.augreg_in21k_ft_in1k\n",
      "vit_base_patch8_224.dino\n",
      "vit_base_patch14_dinov2.lvd142m\n",
      "vit_base_patch14_reg4_dinov2.lvd142m\n",
      "vit_base_patch16_224.augreg2_in21k_ft_in1k\n",
      "vit_base_patch16_224.augreg_in1k\n",
      "vit_base_patch16_224.augreg_in21k\n",
      "vit_base_patch16_224.augreg_in21k_ft_in1k\n",
      "vit_base_patch16_224.dino\n",
      "vit_base_patch16_224.mae\n",
      "vit_base_patch16_224.orig_in21k\n",
      "vit_base_patch16_224.orig_in21k_ft_in1k\n",
      "vit_base_patch16_224.sam_in1k\n",
      "vit_base_patch16_224_miil.in21k\n",
      "vit_base_patch16_224_miil.in21k_ft_in1k\n",
      "vit_base_patch16_384.augreg_in1k\n",
      "vit_base_patch16_384.augreg_in21k_ft_in1k\n",
      "vit_base_patch16_384.orig_in21k_ft_in1k\n",
      "vit_base_patch16_clip_224.datacompxl\n",
      "vit_base_patch16_clip_224.dfn2b\n",
      "vit_base_patch16_clip_224.laion2b\n",
      "vit_base_patch16_clip_224.laion2b_ft_in1k\n",
      "vit_base_patch16_clip_224.laion2b_ft_in12k\n",
      "vit_base_patch16_clip_224.laion2b_ft_in12k_in1k\n",
      "vit_base_patch16_clip_224.metaclip_2pt5b\n",
      "vit_base_patch16_clip_224.openai\n",
      "vit_base_patch16_clip_224.openai_ft_in1k\n",
      "vit_base_patch16_clip_224.openai_ft_in12k\n",
      "vit_base_patch16_clip_224.openai_ft_in12k_in1k\n",
      "vit_base_patch16_clip_384.laion2b_ft_in1k\n",
      "vit_base_patch16_clip_384.laion2b_ft_in12k_in1k\n",
      "vit_base_patch16_clip_384.openai_ft_in1k\n",
      "vit_base_patch16_clip_384.openai_ft_in12k_in1k\n",
      "vit_base_patch16_clip_quickgelu_224.metaclip_2pt5b\n",
      "vit_base_patch16_clip_quickgelu_224.openai\n",
      "vit_base_patch16_rope_reg1_gap_256.sbb_in1k\n",
      "vit_base_patch16_rpn_224.sw_in1k\n",
      "vit_base_patch16_siglip_224.webli\n",
      "vit_base_patch16_siglip_256.webli\n",
      "vit_base_patch16_siglip_384.webli\n",
      "vit_base_patch16_siglip_512.webli\n",
      "vit_base_patch16_siglip_gap_224.webli\n",
      "vit_base_patch16_siglip_gap_256.webli\n",
      "vit_base_patch16_siglip_gap_384.webli\n",
      "vit_base_patch16_siglip_gap_512.webli\n",
      "vit_base_patch32_224.augreg_in1k\n",
      "vit_base_patch32_224.augreg_in21k\n",
      "vit_base_patch32_224.augreg_in21k_ft_in1k\n",
      "vit_base_patch32_224.orig_in21k\n",
      "vit_base_patch32_224.sam_in1k\n",
      "vit_base_patch32_384.augreg_in1k\n",
      "vit_base_patch32_384.augreg_in21k_ft_in1k\n",
      "vit_base_patch32_clip_224.datacompxl\n",
      "vit_base_patch32_clip_224.laion2b\n",
      "vit_base_patch32_clip_224.laion2b_ft_in1k\n",
      "vit_base_patch32_clip_224.laion2b_ft_in12k_in1k\n",
      "vit_base_patch32_clip_224.metaclip_2pt5b\n",
      "vit_base_patch32_clip_224.openai\n",
      "vit_base_patch32_clip_224.openai_ft_in1k\n",
      "vit_base_patch32_clip_256.datacompxl\n",
      "vit_base_patch32_clip_384.laion2b_ft_in12k_in1k\n",
      "vit_base_patch32_clip_384.openai_ft_in12k_in1k\n",
      "vit_base_patch32_clip_448.laion2b_ft_in12k_in1k\n",
      "vit_base_patch32_clip_quickgelu_224.metaclip_2pt5b\n",
      "vit_base_patch32_clip_quickgelu_224.openai\n",
      "vit_base_r50_s16_224.orig_in21k\n",
      "vit_base_r50_s16_384.orig_in21k_ft_in1k\n",
      "vit_betwixt_patch16_reg1_gap_256.sbb_in1k\n",
      "vit_betwixt_patch16_reg4_gap_256.sbb_in1k\n",
      "vit_betwixt_patch16_reg4_gap_256.sbb_in12k\n",
      "vit_betwixt_patch16_reg4_gap_256.sbb_in12k_ft_in1k\n",
      "vit_betwixt_patch16_rope_reg4_gap_256.sbb_in1k\n",
      "vit_betwixt_patch32_clip_224.tinyclip_laion400m\n",
      "vit_giant_patch14_clip_224.laion2b\n",
      "vit_giant_patch14_dinov2.lvd142m\n",
      "vit_giant_patch14_reg4_dinov2.lvd142m\n",
      "vit_giant_patch16_gap_224.in22k_ijepa\n",
      "vit_gigantic_patch14_clip_224.laion2b\n",
      "vit_huge_patch14_224.mae\n",
      "vit_huge_patch14_224.orig_in21k\n",
      "vit_huge_patch14_clip_224.dfn5b\n",
      "vit_huge_patch14_clip_224.laion2b\n",
      "vit_huge_patch14_clip_224.laion2b_ft_in1k\n",
      "vit_huge_patch14_clip_224.laion2b_ft_in12k\n",
      "vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k\n",
      "vit_huge_patch14_clip_224.metaclip_2pt5b\n",
      "vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k\n",
      "vit_huge_patch14_clip_378.dfn5b\n",
      "vit_huge_patch14_clip_quickgelu_224.dfn5b\n",
      "vit_huge_patch14_clip_quickgelu_224.metaclip_2pt5b\n",
      "vit_huge_patch14_clip_quickgelu_378.dfn5b\n",
      "vit_huge_patch14_gap_224.in1k_ijepa\n",
      "vit_huge_patch14_gap_224.in22k_ijepa\n",
      "vit_huge_patch16_gap_448.in1k_ijepa\n",
      "vit_large_patch14_clip_224.datacompxl\n",
      "vit_large_patch14_clip_224.dfn2b\n",
      "vit_large_patch14_clip_224.laion2b\n",
      "vit_large_patch14_clip_224.laion2b_ft_in1k\n",
      "vit_large_patch14_clip_224.laion2b_ft_in12k\n",
      "vit_large_patch14_clip_224.laion2b_ft_in12k_in1k\n",
      "vit_large_patch14_clip_224.metaclip_2pt5b\n",
      "vit_large_patch14_clip_224.openai\n",
      "vit_large_patch14_clip_224.openai_ft_in1k\n",
      "vit_large_patch14_clip_224.openai_ft_in12k\n",
      "vit_large_patch14_clip_224.openai_ft_in12k_in1k\n",
      "vit_large_patch14_clip_336.laion2b_ft_in1k\n",
      "vit_large_patch14_clip_336.laion2b_ft_in12k_in1k\n",
      "vit_large_patch14_clip_336.openai\n",
      "vit_large_patch14_clip_336.openai_ft_in12k_in1k\n",
      "vit_large_patch14_clip_quickgelu_224.dfn2b\n",
      "vit_large_patch14_clip_quickgelu_224.metaclip_2pt5b\n",
      "vit_large_patch14_clip_quickgelu_224.openai\n",
      "vit_large_patch14_clip_quickgelu_336.openai\n",
      "vit_large_patch14_dinov2.lvd142m\n",
      "vit_large_patch14_reg4_dinov2.lvd142m\n",
      "vit_large_patch16_224.augreg_in21k\n",
      "vit_large_patch16_224.augreg_in21k_ft_in1k\n",
      "vit_large_patch16_224.mae\n",
      "vit_large_patch16_224.orig_in21k\n",
      "vit_large_patch16_384.augreg_in21k_ft_in1k\n",
      "vit_large_patch16_siglip_256.webli\n",
      "vit_large_patch16_siglip_384.webli\n",
      "vit_large_patch16_siglip_gap_256.webli\n",
      "vit_large_patch16_siglip_gap_384.webli\n",
      "vit_large_patch32_224.orig_in21k\n",
      "vit_large_patch32_384.orig_in21k_ft_in1k\n",
      "vit_large_r50_s32_224.augreg_in21k\n",
      "vit_large_r50_s32_224.augreg_in21k_ft_in1k\n",
      "vit_large_r50_s32_384.augreg_in21k_ft_in1k\n",
      "vit_little_patch16_reg4_gap_256.sbb_in1k\n",
      "vit_medium_patch16_clip_224.tinyclip_yfcc15m\n",
      "vit_medium_patch16_gap_240.sw_in12k\n",
      "vit_medium_patch16_gap_256.sw_in12k_ft_in1k\n",
      "vit_medium_patch16_gap_384.sw_in12k_ft_in1k\n",
      "vit_medium_patch16_reg1_gap_256.sbb_in1k\n",
      "vit_medium_patch16_reg4_gap_256.sbb_in1k\n",
      "vit_medium_patch16_rope_reg1_gap_256.sbb_in1k\n",
      "vit_medium_patch32_clip_224.tinyclip_laion400m\n",
      "vit_mediumd_patch16_reg4_gap_256.sbb_in12k\n",
      "vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k\n",
      "vit_mediumd_patch16_rope_reg1_gap_256.sbb_in1k\n",
      "vit_pwee_patch16_reg1_gap_256.sbb_in1k\n",
      "vit_relpos_base_patch16_224.sw_in1k\n",
      "vit_relpos_base_patch16_clsgap_224.sw_in1k\n",
      "vit_relpos_base_patch32_plus_rpn_256.sw_in1k\n",
      "vit_relpos_medium_patch16_224.sw_in1k\n",
      "vit_relpos_medium_patch16_cls_224.sw_in1k\n",
      "vit_relpos_medium_patch16_rpn_224.sw_in1k\n",
      "vit_relpos_small_patch16_224.sw_in1k\n",
      "vit_small_patch8_224.dino\n",
      "vit_small_patch14_dinov2.lvd142m\n",
      "vit_small_patch14_reg4_dinov2.lvd142m\n",
      "vit_small_patch16_224.augreg_in1k\n",
      "vit_small_patch16_224.augreg_in21k\n",
      "vit_small_patch16_224.augreg_in21k_ft_in1k\n",
      "vit_small_patch16_224.dino\n",
      "vit_small_patch16_384.augreg_in1k\n",
      "vit_small_patch16_384.augreg_in21k_ft_in1k\n",
      "vit_small_patch32_224.augreg_in21k\n",
      "vit_small_patch32_224.augreg_in21k_ft_in1k\n",
      "vit_small_patch32_384.augreg_in21k_ft_in1k\n",
      "vit_small_r26_s32_224.augreg_in21k\n",
      "vit_small_r26_s32_224.augreg_in21k_ft_in1k\n",
      "vit_small_r26_s32_384.augreg_in21k_ft_in1k\n",
      "vit_so400m_patch14_siglip_224.webli\n",
      "vit_so400m_patch14_siglip_384.webli\n",
      "vit_so400m_patch14_siglip_gap_224.pali_mix\n",
      "vit_so400m_patch14_siglip_gap_224.pali_pt\n",
      "vit_so400m_patch14_siglip_gap_224.webli\n",
      "vit_so400m_patch14_siglip_gap_384.webli\n",
      "vit_so400m_patch14_siglip_gap_448.pali_mix\n",
      "vit_so400m_patch14_siglip_gap_448.pali_pt\n",
      "vit_so400m_patch14_siglip_gap_896.pali_pt\n",
      "vit_srelpos_medium_patch16_224.sw_in1k\n",
      "vit_srelpos_small_patch16_224.sw_in1k\n",
      "vit_tiny_patch16_224.augreg_in21k\n",
      "vit_tiny_patch16_224.augreg_in21k_ft_in1k\n",
      "vit_tiny_patch16_384.augreg_in21k_ft_in1k\n",
      "vit_tiny_r_s16_p8_224.augreg_in21k\n",
      "vit_tiny_r_s16_p8_224.augreg_in21k_ft_in1k\n",
      "vit_tiny_r_s16_p8_384.augreg_in21k_ft_in1k\n",
      "vit_wee_patch16_reg1_gap_256.sbb_in1k\n",
      "vit_xsmall_patch16_clip_224.tinyclip_yfcc15m\n",
      "volo_d1_224.sail_in1k\n",
      "volo_d1_384.sail_in1k\n",
      "volo_d2_224.sail_in1k\n",
      "volo_d2_384.sail_in1k\n",
      "volo_d3_224.sail_in1k\n",
      "volo_d3_448.sail_in1k\n",
      "volo_d4_224.sail_in1k\n",
      "volo_d4_448.sail_in1k\n",
      "volo_d5_224.sail_in1k\n",
      "volo_d5_448.sail_in1k\n",
      "volo_d5_512.sail_in1k\n",
      "wide_resnet50_2.racm_in1k\n",
      "wide_resnet50_2.tv2_in1k\n",
      "wide_resnet50_2.tv_in1k\n",
      "wide_resnet101_2.tv2_in1k\n",
      "wide_resnet101_2.tv_in1k\n",
      "xception41.tf_in1k\n",
      "xception41p.ra3_in1k\n",
      "xception65.ra3_in1k\n",
      "xception65.tf_in1k\n",
      "xception65p.ra3_in1k\n",
      "xception71.tf_in1k\n",
      "xcit_large_24_p8_224.fb_dist_in1k\n",
      "xcit_large_24_p8_224.fb_in1k\n",
      "xcit_large_24_p8_384.fb_dist_in1k\n",
      "xcit_large_24_p16_224.fb_dist_in1k\n",
      "xcit_large_24_p16_224.fb_in1k\n",
      "xcit_large_24_p16_384.fb_dist_in1k\n",
      "xcit_medium_24_p8_224.fb_dist_in1k\n",
      "xcit_medium_24_p8_224.fb_in1k\n",
      "xcit_medium_24_p8_384.fb_dist_in1k\n",
      "xcit_medium_24_p16_224.fb_dist_in1k\n",
      "xcit_medium_24_p16_224.fb_in1k\n",
      "xcit_medium_24_p16_384.fb_dist_in1k\n",
      "xcit_nano_12_p8_224.fb_dist_in1k\n",
      "xcit_nano_12_p8_224.fb_in1k\n",
      "xcit_nano_12_p8_384.fb_dist_in1k\n",
      "xcit_nano_12_p16_224.fb_dist_in1k\n",
      "xcit_nano_12_p16_224.fb_in1k\n",
      "xcit_nano_12_p16_384.fb_dist_in1k\n",
      "xcit_small_12_p8_224.fb_dist_in1k\n",
      "xcit_small_12_p8_224.fb_in1k\n",
      "xcit_small_12_p8_384.fb_dist_in1k\n",
      "xcit_small_12_p16_224.fb_dist_in1k\n",
      "xcit_small_12_p16_224.fb_in1k\n",
      "xcit_small_12_p16_384.fb_dist_in1k\n",
      "xcit_small_24_p8_224.fb_dist_in1k\n",
      "xcit_small_24_p8_224.fb_in1k\n",
      "xcit_small_24_p8_384.fb_dist_in1k\n",
      "xcit_small_24_p16_224.fb_dist_in1k\n",
      "xcit_small_24_p16_224.fb_in1k\n",
      "xcit_small_24_p16_384.fb_dist_in1k\n",
      "xcit_tiny_12_p8_224.fb_dist_in1k\n",
      "xcit_tiny_12_p8_224.fb_in1k\n",
      "xcit_tiny_12_p8_384.fb_dist_in1k\n",
      "xcit_tiny_12_p16_224.fb_dist_in1k\n",
      "xcit_tiny_12_p16_224.fb_in1k\n",
      "xcit_tiny_12_p16_384.fb_dist_in1k\n",
      "xcit_tiny_24_p8_224.fb_dist_in1k\n",
      "xcit_tiny_24_p8_224.fb_in1k\n",
      "xcit_tiny_24_p8_384.fb_dist_in1k\n",
      "xcit_tiny_24_p16_224.fb_dist_in1k\n",
      "xcit_tiny_24_p16_224.fb_in1k\n",
      "xcit_tiny_24_p16_384.fb_dist_in1k\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "pretrained_models = timm.list_models(pretrained=True)\n",
    "\n",
    "for m in pretrained_models:\n",
    "    print(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoattack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autopgd_base\n\u001b[0;32m----> 4\u001b[0m autopgd_base\u001b[38;5;241m.\u001b[39mAPGDAttack(\u001b[43mmodel\u001b[49m, n_restarts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_iter\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mperturb_steps, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                 eps\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepsilon, norm\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdistance, eot_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, rho\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.75\u001b[39m, seed\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mseed,\n\u001b[1;32m      6\u001b[0m                 device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1280 1024\n"
     ]
    }
   ],
   "source": [
    "effective_batch_size = 1024\n",
    "world_size = 4\n",
    "per_gpu_batch_size = 320\n",
    "\n",
    "print( effective_batch_size // (world_size * per_gpu_batch_size), (world_size * per_gpu_batch_size), effective_batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "from architectures.resnet_imagenet import ResNet_imagenet, Bottleneck_imagenet\n",
    "model = ResNet_imagenet(Bottleneck_imagenet, [3, 4, 6, 3], )\n",
    "num_features = model.fc.in_features\n",
    "print(num_features)\n",
    "model.fc = nn.Linear(num_features, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
