{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n",
      "init weighted dataset\n",
      "(1000000, 32, 32, 3)\n",
      "50000\n",
      "concatenation\n",
      "1050000 1050000\n",
      "init dataloder\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import DistributedCustomSampler\n",
    "from losses import trades_loss\n",
    "from tqdm.notebook import tqdm\n",
    "from architectures import load_architecture, load_statedict, add_lora\n",
    "\n",
    "args = get_args()\n",
    "args.arch = 'resnet50'\n",
    "args.dataset = 'CIFAR10s'\n",
    "args.selection_method = 'random'\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "model, target_layers = load_architecture(args)\n",
    "model.to('cuda')\n",
    "\n",
    "# statedict = load_statedict(args)\n",
    "# model.load_state_dict(statedict)\n",
    "# add_lora(target_layers, model)\n",
    "\n",
    "args.pruning_ratio = 0\n",
    "args.delta = 1\n",
    "args.batch_size = 64\n",
    "args.pruning_strategy = 'random'\n",
    "args.batch_strategy = 'random'\n",
    "args.sample_size= 128\n",
    "\n",
    "# train_dataset = IndexedDataset()\n",
    "print('init weighted dataset')\n",
    "train_dataset = WeightedDataset(args, train=True, prune_ratio = args.pruning_ratio,  )\n",
    "\n",
    "train_sampler = DistributedCustomSampler(args, train_dataset, num_replicas=2, rank=0, drop_last=True)\n",
    "\n",
    "print('init dataloder')\n",
    "trainloader = DataLoader(train_dataset, batch_size=None, sampler = train_sampler,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning\n",
      "remove tail\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7e8bc3b2d44b19807c20a374de952f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 90,  58,  86, 122,  68,  66,  62,  80,  52, 124,  44,   4,  22,  56,\n",
      "        108,   8,  20,  70, 104,  92,  60,  14,  28,  54, 126, 110,  82,  84,\n",
      "        116,  36, 120,  64,  30,  10,  32,  40, 112,  16,  26,  50,  74,  34,\n",
      "         96, 102, 114,  76,   2,  24,  98,  48,  12,  46,  72, 100,  42,  38,\n",
      "         18,  78, 118,   6,   0, 106,  94,  88])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "indices shape: torch.Size([64, 1])\n",
      "clean_loss_val shape: torch.Size([64, 1])\n",
      "robust_loss_val shape: torch.Size([64, 1])\n",
      "global_loss_val shape: torch.Size([64, 1])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "torch.Size([64, 1])\n",
      "indices shape: torch.Size([64])\n",
      "clean_loss_val shape: torch.Size([64])\n",
      "robust_loss_val shape: torch.Size([64])\n",
      "global_loss_val shape: torch.Size([64])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "hey torch.Size([64]) torch.Size([64])\n",
      "tensor([238, 176, 166, 218, 144, 200, 236, 248, 184, 142, 246, 148, 224, 192,\n",
      "        188, 194, 132, 232, 226, 180, 172, 178, 222, 202, 146, 138, 162, 160,\n",
      "        186, 170, 154, 196, 174, 214, 158, 228, 216, 134, 208, 140, 254, 230,\n",
      "        168, 250, 152, 234, 156, 128, 182, 164, 220, 150, 204, 198, 252, 206,\n",
      "        242, 130, 240, 190, 244, 212, 210, 136])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "indices shape: torch.Size([64, 1])\n",
      "clean_loss_val shape: torch.Size([64, 1])\n",
      "robust_loss_val shape: torch.Size([64, 1])\n",
      "global_loss_val shape: torch.Size([64, 1])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "torch.Size([64, 1])\n",
      "indices shape: torch.Size([64])\n",
      "clean_loss_val shape: torch.Size([64])\n",
      "robust_loss_val shape: torch.Size([64])\n",
      "global_loss_val shape: torch.Size([64])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "hey torch.Size([64]) torch.Size([64])\n",
      "tensor([298, 286, 358, 270, 364, 366, 266, 344, 308, 342, 274, 268, 312, 354,\n",
      "        382, 264, 330, 352, 280, 318, 360, 314, 322, 346, 306, 272, 334, 362,\n",
      "        350, 332, 380, 368, 340, 304, 376, 282, 294, 256, 320, 288, 378, 278,\n",
      "        336, 300, 276, 328, 258, 338, 284, 310, 356, 290, 296, 372, 348, 292,\n",
      "        262, 316, 326, 324, 370, 260, 374, 302])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "indices shape: torch.Size([64, 1])\n",
      "clean_loss_val shape: torch.Size([64, 1])\n",
      "robust_loss_val shape: torch.Size([64, 1])\n",
      "global_loss_val shape: torch.Size([64, 1])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "torch.Size([64, 1])\n",
      "indices shape: torch.Size([64])\n",
      "clean_loss_val shape: torch.Size([64])\n",
      "robust_loss_val shape: torch.Size([64])\n",
      "global_loss_val shape: torch.Size([64])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "hey torch.Size([64]) torch.Size([64])\n",
      "tensor([468, 470, 392, 494, 436, 452, 480, 476, 434, 504, 416, 510, 488, 496,\n",
      "        402, 486, 418, 454, 412, 388, 438, 474, 498, 440, 502, 404, 424, 400,\n",
      "        444, 384, 472, 430, 394, 428, 408, 458, 478, 390, 462, 484, 508, 386,\n",
      "        442, 460, 406, 482, 500, 420, 466, 414, 432, 398, 410, 446, 426, 396,\n",
      "        456, 448, 464, 450, 422, 506, 490, 492])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "indices shape: torch.Size([64, 1])\n",
      "clean_loss_val shape: torch.Size([64, 1])\n",
      "robust_loss_val shape: torch.Size([64, 1])\n",
      "global_loss_val shape: torch.Size([64, 1])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "torch.Size([64, 1])\n",
      "indices shape: torch.Size([64])\n",
      "clean_loss_val shape: torch.Size([64])\n",
      "robust_loss_val shape: torch.Size([64])\n",
      "global_loss_val shape: torch.Size([64])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "hey torch.Size([64]) torch.Size([64])\n",
      "tensor([612, 610, 532, 636, 520, 538, 540, 620, 558, 568, 530, 608, 594, 550,\n",
      "        564, 630, 576, 524, 616, 618, 556, 526, 596, 542, 588, 554, 602, 536,\n",
      "        574, 614, 516, 598, 552, 586, 606, 548, 628, 570, 572, 604, 590, 518,\n",
      "        562, 622, 624, 566, 582, 546, 514, 528, 544, 626, 632, 534, 512, 584,\n",
      "        592, 522, 638, 560, 634, 580, 600, 578])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "indices shape: torch.Size([64, 1])\n",
      "clean_loss_val shape: torch.Size([64, 1])\n",
      "robust_loss_val shape: torch.Size([64, 1])\n",
      "global_loss_val shape: torch.Size([64, 1])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "torch.Size([64, 1])\n",
      "indices shape: torch.Size([64])\n",
      "clean_loss_val shape: torch.Size([64])\n",
      "robust_loss_val shape: torch.Size([64])\n",
      "global_loss_val shape: torch.Size([64])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "hey torch.Size([64]) torch.Size([64])\n",
      "tensor([702, 652, 760, 708, 682, 644, 658, 714, 746, 642, 722, 710, 660, 664,\n",
      "        752, 740, 718, 754, 670, 726, 720, 650, 758, 730, 728, 738, 692, 748,\n",
      "        706, 648, 676, 680, 716, 762, 668, 734, 690, 766, 696, 662, 764, 674,\n",
      "        666, 736, 654, 750, 678, 704, 686, 732, 698, 694, 656, 742, 646, 744,\n",
      "        688, 700, 712, 724, 672, 684, 640, 756])\n",
      "torch.Size([64, 10])\n",
      "torch.Size([64, 10])\n",
      "indices shape: torch.Size([64, 1])\n",
      "clean_loss_val shape: torch.Size([64, 1])\n",
      "robust_loss_val shape: torch.Size([64, 1])\n",
      "global_loss_val shape: torch.Size([64, 1])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "torch.Size([64, 1])\n",
      "indices shape: torch.Size([64])\n",
      "clean_loss_val shape: torch.Size([64])\n",
      "robust_loss_val shape: torch.Size([64])\n",
      "global_loss_val shape: torch.Size([64])\n",
      "clean_pred shape: torch.Size([64, 10])\n",
      "robust_pred shape: torch.Size([64, 10])\n",
      "hey torch.Size([64]) torch.Size([64])\n",
      "tensor([822, 860, 810, 796, 826, 768, 788, 834, 866, 882, 790, 832, 872, 870,\n",
      "        806, 774, 824, 778, 880, 830, 792, 840, 776, 850, 886, 816, 842, 772,\n",
      "        836, 808, 862, 802, 892, 848, 818, 798, 812, 868, 782, 894, 784, 846,\n",
      "        844, 890, 794, 856, 786, 876, 838, 820, 800, 828, 864, 874, 852, 780,\n",
      "        770, 814, 858, 804, 888, 878, 854, 884])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(idxs)\n\u001b[1;32m     21\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(rank), target\u001b[38;5;241m.\u001b[39mto(rank) \n\u001b[0;32m---> 23\u001b[0m loss_values, clean_values, robust_values, logits_nat, logits_adv \u001b[38;5;241m=\u001b[39m \u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits_adv\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits_nat\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Desktop/robust_training/losses/loss_loader.py:12\u001b[0m, in \u001b[0;36mget_loss\u001b[0;34m(args, model, x_natural, y, optimizer)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trades_loss(args, model, x_natural, y, optimizer)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mloss_function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRADES_v2\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrades_loss_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_natural\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mloss_function \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRADES_v3\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trades_loss_v3(args, model, x_natural, y, optimizer)\n",
      "File \u001b[0;32m~/Desktop/robust_training/losses/trades_v2.py:34\u001b[0m, in \u001b[0;36mtrades_loss_v2\u001b[0;34m(args, model, x_natural, y, optimizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m x_adv \u001b[38;5;241m=\u001b[39m x_adv\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#print('infer')\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     logits_adv \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_adv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#print('kl loss')\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# loss_kl = nn.KLDivLoss(reduction='sum')( F.log_softmax(logits_adv, dim=1), F.softmax(logits_nat, dim=1) )\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy( logits_adv, y)\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/architectures/resnet_cifar10.py:259\u001b[0m, in \u001b[0;36mResNet_cifar10.forward\u001b[0;34m(self, x_natural, x_adv)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits_nat, logits_adv\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_natural\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/architectures/resnet_cifar10.py:244\u001b[0m, in \u001b[0;36mResNet_cifar10._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    242\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    243\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 244\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    247\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/architectures/resnet_cifar10.py:111\u001b[0m, in \u001b[0;36mBottleneck_cifar10.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    108\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    109\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 111\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from losses import get_loss, get_eval_loss\n",
    "\n",
    "iterations = 1\n",
    "rank = 'cuda'\n",
    "\n",
    "optimizer = torch.optim.SGD( model.parameters(),lr=args.init_lr, weight_decay=args.weight_decay, momentum=args.momentum, nesterov=True, )\n",
    "\n",
    "for iteration in range(iterations):\n",
    "\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(iteration)\n",
    "\n",
    "    for batch_id, batch in tqdm(enumerate( trainloader ) ):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, target, idxs = batch\n",
    "\n",
    "        print(idxs)\n",
    "\n",
    "        data, target = data.to(rank), target.to(rank) \n",
    "         \n",
    "        loss_values, clean_values, robust_values, logits_nat, logits_adv = get_loss(args, model, data, target, optimizer)\n",
    "        print(logits_adv.shape)\n",
    "        print(logits_nat.shape)\n",
    "        # assert torch.isfinite(loss_values).all(), \"Loss contains NaNs!\"\n",
    "        # assert torch.isfinite(logits_nat).all(), \"Logits_nat contains NaNs!\"\n",
    "        # assert torch.isfinite(logits_adv).all(), \"Logits_adv contains NaNs!\"\n",
    "\n",
    "        # train_dataset.update_scores(iteration, idxs,loss_values)\n",
    "        train_dataset.update_scores(rank, idxs, clean_values, robust_values, loss_values, logits_nat, logits_adv)\n",
    "        loss = train_dataset.compute_loss(idxs, loss_values)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),   ])\n",
    "                # transforms.Normalize( mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616) \n",
    "                                    #  )  ])\n",
    "\n",
    "\n",
    "dataset = datasets.CIFAR10(root=args.data_dir, train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the .npz file using NumPy\n",
    "npz_file = np.load('/home/mheuillet/Downloads/1m.npz')\n",
    "\n",
    "# Print keys to see what arrays are available in the .npz file\n",
    "print(\"Available arrays in the .npz file:\", npz_file.files)\n",
    "\n",
    "# Example: Load a specific array by its key\n",
    "# Replace 'array_key' with the actual key in your .npz file\n",
    "array_key = 'image'  # Change to your actual key\n",
    "numpy_array = npz_file[array_key]\n",
    "\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), \n",
    "                                      transforms.RandomHorizontalFlip(0.5), \n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# transform = transforms.Compose([\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize( mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616) )  ])\n",
    "# # \n",
    "#         if train:\n",
    "#             dataset = datasets.CIFAR10(root=args.data_dir, train=True, download=True, transform=transform)\n",
    "#         else:\n",
    "#             dataset = datasets.CIFAR10(root=args.data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "#         # pool_dataset = IndexedDataset('CIFAR10', train_folder, transform= transform ) \n",
    "#         # test_dataset = IndexedDataset('CIFAR10', test_folder, transform= transform) \n",
    "#         N = 10\n",
    "\n",
    "#         print('load dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 10\n",
    "# Initializing tensors with specified shapes\n",
    "indices = torch.randint(0, 10, (N, ))\n",
    "print(\"indices shape:\", indices.shape)\n",
    "\n",
    "indices.shape[0]\n",
    "# clean_loss_val = torch.randn((N, ))\n",
    "# print(\"clean_loss_val shape:\", clean_loss_val.shape)\n",
    "\n",
    "# robust_loss_val = torch.randn((N, ))\n",
    "# print(\"robust_loss_val shape:\", robust_loss_val.shape)\n",
    "\n",
    "# clean_pred = torch.randn((N, 5))\n",
    "# print(\"clean_pred shape:\", clean_pred.shape)\n",
    "\n",
    "# robust_pred = torch.randn((N, 5))\n",
    "# print(\"robust_pred shape:\", robust_pred.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # Concatenating all tensors along the column dimension (dim=1)\n",
    "# iv = torch.cat([indices_reshaped,\n",
    "#                 clean_loss_val_reshaped,\n",
    "#                 robust_loss_val_reshaped,\n",
    "#                 clean_pred,\n",
    "#                 robust_pred], dim=1)\n",
    "\n",
    "# print(\"iv shape:\", iv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def check_for_nans(tensors, tensor_names):\n",
    "    for tensor, name in zip(tensors, tensor_names):\n",
    "        if torch.isnan(tensor).any():\n",
    "            print(f\"{name} contains NaNs!\")\n",
    "\n",
    "# Example tensors with potential NaN values\n",
    "loss_values = torch.randn((10, 1))\n",
    "clean_values = torch.randn((10,))\n",
    "robust_values = torch.randn((10,))\n",
    "logits_nat = torch.randn((10, 5))\n",
    "logits_adv = torch.randn((10, 5))\n",
    "\n",
    "# Introducing NaNs for testing purposes\n",
    "loss_values[0, 0] = float('nan')  # Introducing a NaN for demonstration\n",
    "\n",
    "# List of tensors and their names for easy reference in the check\n",
    "tensors = [loss_values, clean_values, robust_values, logits_nat, logits_adv]\n",
    "tensor_names = ['loss_values', 'clean_values', 'robust_values', 'logits_nat', 'logits_adv']\n",
    "\n",
    "check_for_nans(tensors, tensor_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.global_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample tensor with dimensions (10, 60000)\n",
    "# Each of the 10 rows represents an epoch, and each column represents a loss value for one of the 60,000 observations.\n",
    "np.random.seed(0)  # For reproducibility\n",
    "tensor = train_dataset.global_scores2 #np.random.rand(10, 60000)  # Simulating loss values\n",
    "\n",
    "# Sampling 1000 observations from the 60,000\n",
    "\n",
    "sample_indices = np.random.choice(tensor.shape[1], size=60000, replace=False)\n",
    "\n",
    "sampled_tensor = tensor[:, sample_indices]\n",
    "\n",
    "# Plotting the evolution of the loss for the 60,000 observations over 10 epochs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting each observation's loss over the 10 epochs\n",
    "for i in tqdm(range(sampled_tensor.shape[1])):\n",
    "    plt.plot(range(10), sampled_tensor[:, i], alpha=0.25, linewidth=0.5)  # Plotting with low opacity and thin lines for clarity\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.yscale('log')  # Setting y-axis to log scale\n",
    "\n",
    "plt.title('Evolution of Loss for 60,000 Observations Over 10 Epochs')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i])\n",
    "import numpy as np\n",
    "\n",
    "def obtain_latent_dataset(model, dataset, batch_size=32):\n",
    "\n",
    "    # Assuming the dataset is a list or similar iterable with a known length\n",
    "    num_samples = len(dataset)\n",
    "\n",
    "    # Assume the dimensionality of the latent representation can be determined from one sample\n",
    "    image,label, idx = dataset[0]\n",
    "    image = torch.Tensor(image).to('cuda').unsqueeze(0)\n",
    "    print(image.shape)\n",
    "    first_latent_rep = model.get_latent_representation(image)\n",
    "    latent_dim = first_latent_rep.shape[1]\n",
    "    print(first_latent_rep.shape)\n",
    "        \n",
    "    # Preallocate the array for the latent representations\n",
    "    latent_dataset = torch.zeros((num_samples, latent_dim))\n",
    "\n",
    "\n",
    "    for i in tqdm( range(0, num_samples, batch_size) ):\n",
    "        # Get the current batch of data\n",
    "        batch_indices = list(range(i, min(i + batch_size, num_samples)))\n",
    "        images,labels,idxs = dataset[batch_indices]\n",
    "        images = images.to('cuda')\n",
    "            \n",
    "        # Process the batch to get latent representations\n",
    "        batch_latent_reps = model.get_latent_representation(images) \n",
    "            \n",
    "        # Store the results in the preallocated array\n",
    "        latent_dataset[i:i + batch_size] = batch_latent_reps.detach().cpu()\n",
    "\n",
    "    return latent_dataset\n",
    "\n",
    "features =  obtain_latent_dataset(model,train_dataset,64)\n",
    "train_dataset.define_latent_features(features)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
