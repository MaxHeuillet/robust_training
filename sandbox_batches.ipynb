{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import DistributedCustomSampler\n",
    "from losses import trades_loss\n",
    "from tqdm.notebook import tqdm\n",
    "from architectures import load_architecture, load_statedict, add_lora\n",
    "\n",
    "args = get_args()\n",
    "args.arch = 'LeNet5'\n",
    "args.dataset = 'MNIST'\n",
    "args.selection_method = 'uncertainty'\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "model, target_layers = load_architecture(args)\n",
    "model.to('cuda')\n",
    "\n",
    "# statedict = load_statedict(args)\n",
    "# model.load_state_dict(statedict)\n",
    "# add_lora(target_layers, model)\n",
    "\n",
    "args.pruning_ratio = 0\n",
    "args.delta = 1\n",
    "args.batch_size = 128\n",
    "args.pruning_strategy = 'random'\n",
    "args.batch_strategy = 'random'\n",
    "args.sample_size= 128\n",
    "\n",
    "# train_dataset = IndexedDataset()\n",
    "print('init weighted dataset')\n",
    "train_dataset = WeightedDataset(args, train=True, prune_ratio = args.pruning_ratio,  )\n",
    "\n",
    "train_sampler = DistributedCustomSampler(args, train_dataset, num_replicas=2, rank=0, drop_last=True)\n",
    "\n",
    "print('init dataloder')\n",
    "trainloader = DataLoader(train_dataset, batch_size=None, sampler = train_sampler,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import get_loss, get_eval_loss\n",
    "\n",
    "iterations = 1\n",
    "rank = 'cuda'\n",
    "\n",
    "optimizer = torch.optim.SGD( model.parameters(),lr=args.init_lr, weight_decay=args.weight_decay, momentum=args.momentum, nesterov=True, )\n",
    "\n",
    "for iteration in range(iterations):\n",
    "\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(iteration)\n",
    "\n",
    "    for batch_id, batch in tqdm(enumerate( trainloader ) ):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, target, idxs = batch\n",
    "\n",
    "        print(idxs)\n",
    "\n",
    "        data, target = data.to(rank), target.to(rank) \n",
    "         \n",
    "        loss_values, clean_values, robust_values, logits_nat, logits_adv = get_loss(args, model, data, target, optimizer)\n",
    "\n",
    "        # assert torch.isfinite(loss_values).all(), \"Loss contains NaNs!\"\n",
    "        # assert torch.isfinite(logits_nat).all(), \"Logits_nat contains NaNs!\"\n",
    "        # assert torch.isfinite(logits_adv).all(), \"Logits_adv contains NaNs!\"\n",
    "\n",
    "        # train_dataset.update_scores(iteration, idxs,loss_values)\n",
    "        train_dataset.update_scores(rank, idxs, clean_values, robust_values, loss_values, logits_nat, logits_adv)\n",
    "        # loss = train_dataset.compute_loss(idxs, loss_values)\n",
    "\n",
    "        # loss.backward()\n",
    "        # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),   ])\n",
    "                # transforms.Normalize( mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616) \n",
    "                                    #  )  ])\n",
    "\n",
    "\n",
    "dataset = datasets.CIFAR10(root=args.data_dir, train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the .npz file using NumPy\n",
    "npz_file = np.load('/home/mheuillet/Downloads/1m.npz')\n",
    "\n",
    "# Print keys to see what arrays are available in the .npz file\n",
    "print(\"Available arrays in the .npz file:\", npz_file.files)\n",
    "\n",
    "# Example: Load a specific array by its key\n",
    "# Replace 'array_key' with the actual key in your .npz file\n",
    "array_key = 'image'  # Change to your actual key\n",
    "numpy_array = npz_file[array_key]\n",
    "\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), \n",
    "                                      transforms.RandomHorizontalFlip(0.5), \n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# transform = transforms.Compose([\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize( mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616) )  ])\n",
    "# # \n",
    "#         if train:\n",
    "#             dataset = datasets.CIFAR10(root=args.data_dir, train=True, download=True, transform=transform)\n",
    "#         else:\n",
    "#             dataset = datasets.CIFAR10(root=args.data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "#         # pool_dataset = IndexedDataset('CIFAR10', train_folder, transform= transform ) \n",
    "#         # test_dataset = IndexedDataset('CIFAR10', test_folder, transform= transform) \n",
    "#         N = 10\n",
    "\n",
    "#         print('load dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 10\n",
    "# Initializing tensors with specified shapes\n",
    "indices = torch.randint(0, 10, (N, ))\n",
    "print(\"indices shape:\", indices.shape)\n",
    "\n",
    "indices.shape[0]\n",
    "# clean_loss_val = torch.randn((N, ))\n",
    "# print(\"clean_loss_val shape:\", clean_loss_val.shape)\n",
    "\n",
    "# robust_loss_val = torch.randn((N, ))\n",
    "# print(\"robust_loss_val shape:\", robust_loss_val.shape)\n",
    "\n",
    "# clean_pred = torch.randn((N, 5))\n",
    "# print(\"clean_pred shape:\", clean_pred.shape)\n",
    "\n",
    "# robust_pred = torch.randn((N, 5))\n",
    "# print(\"robust_pred shape:\", robust_pred.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # Concatenating all tensors along the column dimension (dim=1)\n",
    "# iv = torch.cat([indices_reshaped,\n",
    "#                 clean_loss_val_reshaped,\n",
    "#                 robust_loss_val_reshaped,\n",
    "#                 clean_pred,\n",
    "#                 robust_pred], dim=1)\n",
    "\n",
    "# print(\"iv shape:\", iv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def check_for_nans(tensors, tensor_names):\n",
    "    for tensor, name in zip(tensors, tensor_names):\n",
    "        if torch.isnan(tensor).any():\n",
    "            print(f\"{name} contains NaNs!\")\n",
    "\n",
    "# Example tensors with potential NaN values\n",
    "loss_values = torch.randn((10, 1))\n",
    "clean_values = torch.randn((10,))\n",
    "robust_values = torch.randn((10,))\n",
    "logits_nat = torch.randn((10, 5))\n",
    "logits_adv = torch.randn((10, 5))\n",
    "\n",
    "# Introducing NaNs for testing purposes\n",
    "loss_values[0, 0] = float('nan')  # Introducing a NaN for demonstration\n",
    "\n",
    "# List of tensors and their names for easy reference in the check\n",
    "tensors = [loss_values, clean_values, robust_values, logits_nat, logits_adv]\n",
    "tensor_names = ['loss_values', 'clean_values', 'robust_values', 'logits_nat', 'logits_adv']\n",
    "\n",
    "check_for_nans(tensors, tensor_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.global_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample tensor with dimensions (10, 60000)\n",
    "# Each of the 10 rows represents an epoch, and each column represents a loss value for one of the 60,000 observations.\n",
    "np.random.seed(0)  # For reproducibility\n",
    "tensor = train_dataset.global_scores2 #np.random.rand(10, 60000)  # Simulating loss values\n",
    "\n",
    "# Sampling 1000 observations from the 60,000\n",
    "\n",
    "sample_indices = np.random.choice(tensor.shape[1], size=60000, replace=False)\n",
    "\n",
    "sampled_tensor = tensor[:, sample_indices]\n",
    "\n",
    "# Plotting the evolution of the loss for the 60,000 observations over 10 epochs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting each observation's loss over the 10 epochs\n",
    "for i in tqdm(range(sampled_tensor.shape[1])):\n",
    "    plt.plot(range(10), sampled_tensor[:, i], alpha=0.25, linewidth=0.5)  # Plotting with low opacity and thin lines for clarity\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.yscale('log')  # Setting y-axis to log scale\n",
    "\n",
    "plt.title('Evolution of Loss for 60,000 Observations Over 10 Epochs')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i])\n",
    "import numpy as np\n",
    "\n",
    "def obtain_latent_dataset(model, dataset, batch_size=32):\n",
    "\n",
    "    # Assuming the dataset is a list or similar iterable with a known length\n",
    "    num_samples = len(dataset)\n",
    "\n",
    "    # Assume the dimensionality of the latent representation can be determined from one sample\n",
    "    image,label, idx = dataset[0]\n",
    "    image = torch.Tensor(image).to('cuda').unsqueeze(0)\n",
    "    print(image.shape)\n",
    "    first_latent_rep = model.get_latent_representation(image)\n",
    "    latent_dim = first_latent_rep.shape[1]\n",
    "    print(first_latent_rep.shape)\n",
    "        \n",
    "    # Preallocate the array for the latent representations\n",
    "    latent_dataset = torch.zeros((num_samples, latent_dim))\n",
    "\n",
    "\n",
    "    for i in tqdm( range(0, num_samples, batch_size) ):\n",
    "        # Get the current batch of data\n",
    "        batch_indices = list(range(i, min(i + batch_size, num_samples)))\n",
    "        images,labels,idxs = dataset[batch_indices]\n",
    "        images = images.to('cuda')\n",
    "            \n",
    "        # Process the batch to get latent representations\n",
    "        batch_latent_reps = model.get_latent_representation(images) \n",
    "            \n",
    "        # Store the results in the preallocated array\n",
    "        latent_dataset[i:i + batch_size] = batch_latent_reps.detach().cpu()\n",
    "\n",
    "    return latent_dataset\n",
    "\n",
    "features =  obtain_latent_dataset(model,train_dataset,64)\n",
    "train_dataset.define_latent_features(features)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
