{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n",
      "init weighted dataset\n",
      "Files already downloaded and verified\n",
      "load dataloader\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 2048])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed6d4c5c564cf0b56e4a6a76384e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 50000\n",
      "init dataloder\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import DistributedCustomSampler\n",
    "from losses import trades_loss\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "args = get_args()\n",
    "args.arch = 'resnet50'\n",
    "args.dataset = 'CIFAR10'\n",
    "args.selection_method = 'none'\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "model, target_layers = load_architecture(args)\n",
    "model.to('cuda')\n",
    "\n",
    "args.epochs = 2\n",
    "args.pruning_ratio = 0.5\n",
    "args.delta = 1\n",
    "args.batch_size = 2\n",
    "args.pruning_strategy = 'TS_pruning'\n",
    "args.batch_strategy = 'random'\n",
    "args.sample_size= 5\n",
    "\n",
    "# train_dataset = IndexedDataset()\n",
    "print('init weighted dataset')\n",
    "train_dataset = WeightedDataset(args, train=True, prune_ratio = args.pruning_ratio,  )\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i])\n",
    "import numpy as np\n",
    "\n",
    "def obtain_latent_dataset(model, dataset, batch_size=32):\n",
    "\n",
    "    # Assuming the dataset is a list or similar iterable with a known length\n",
    "    num_samples = len(dataset)\n",
    "\n",
    "    # Assume the dimensionality of the latent representation can be determined from one sample\n",
    "    image,label, idx = dataset[0]\n",
    "    image = torch.Tensor(image).to('cuda').unsqueeze(0)\n",
    "    print(image.shape)\n",
    "    first_latent_rep = model.get_latent_representation(image)\n",
    "    latent_dim = first_latent_rep.shape[1]\n",
    "    print(first_latent_rep.shape)\n",
    "        \n",
    "    # Preallocate the array for the latent representations\n",
    "    latent_dataset = torch.zeros((num_samples, latent_dim))\n",
    "\n",
    "\n",
    "    for i in tqdm( range(0, num_samples, batch_size) ):\n",
    "        # Get the current batch of data\n",
    "        batch_indices = list(range(i, min(i + batch_size, num_samples)))\n",
    "        images,labels,idxs = dataset[batch_indices]\n",
    "        images = images.to('cuda')\n",
    "            \n",
    "        # Process the batch to get latent representations\n",
    "        batch_latent_reps = model.get_latent_representation(images) \n",
    "            \n",
    "        # Store the results in the preallocated array\n",
    "        latent_dataset[i:i + batch_size] = batch_latent_reps.detach().cpu()\n",
    "\n",
    "    return latent_dataset\n",
    "\n",
    "features =  obtain_latent_dataset(model,train_dataset,64)\n",
    "train_dataset.define_latent_features(features)\n",
    "\n",
    "train_sampler = DistributedCustomSampler(args, train_dataset, num_replicas=2, rank=1, drop_last=True)\n",
    "\n",
    "print('init dataloder')\n",
    "trainloader = DataLoader(train_dataset, batch_size=None, sampler = train_sampler,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruning\n",
      "remove tail\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dot(): argument 'tensor' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(rank), target\u001b[38;5;241m.\u001b[39mto(rank) \n\u001b[1;32m     19\u001b[0m loss_values, clean_values, robust_values, logits_nat, logits_adv \u001b[38;5;241m=\u001b[39m get_loss(args, model, data, target, optimizer)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_nat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_adv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mcompute_loss(idxs, loss_values)\n",
      "File \u001b[0;32m~/Desktop/robust_training/datasets/weighted_dataset.py:117\u001b[0m, in \u001b[0;36mWeightedDataset.update_scores\u001b[0;34m(self, indices, clean_values, robust_values, global_values, clean_pred, robust_pred)\u001b[0m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent[indices]\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Compute the scalar factor for the Sherman-Morrison update\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m scaling_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSigma_inv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdot(x)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Update the inverse covariance matrix using the Sherman-Morrison formula\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSigma_inv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSigma_inv \u001b[38;5;241m-\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSigma_inv\u001b[38;5;241m.\u001b[39mdot(x)\u001b[38;5;241m.\u001b[39mdot(x\u001b[38;5;241m.\u001b[39mT)\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSigma_inv)) \u001b[38;5;241m/\u001b[39m scaling_factor\n",
      "\u001b[0;31mTypeError\u001b[0m: dot(): argument 'tensor' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "from losses import get_loss, get_eval_loss\n",
    "\n",
    "iterations = 2\n",
    "rank = 'cuda'\n",
    "\n",
    "optimizer = torch.optim.SGD( model.parameters(),lr=args.init_lr, weight_decay=args.weight_decay, momentum=args.momentum, nesterov=True, )\n",
    "\n",
    "for iteration in range(iterations):\n",
    "\n",
    "    model.train()\n",
    "    train_sampler.set_epoch(iteration)\n",
    "\n",
    "    for batch_id, batch in enumerate( trainloader ):\n",
    "\n",
    "        data, target, idxs = batch\n",
    "\n",
    "        data, target = data.to(rank), target.to(rank) \n",
    "         \n",
    "        loss_values, clean_values, robust_values, logits_nat, logits_adv = get_loss(args, model, data, target, optimizer)\n",
    "\n",
    "        train_dataset.update_scores(idxs, clean_values, robust_values, loss_values, logits_nat, logits_adv)\n",
    "        loss = train_dataset.compute_loss(idxs, loss_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
