{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from torch.nn.modules.conv import _ConvNd\n",
    "from typing import Union, Optional, Tuple, Union, List\n",
    "from torch.nn.common_types import _size_1_t, _size_2_t, _size_3_t\n",
    "from torch.nn.modules.utils import _single, _pair, _triple, _reverse_repeat_tuple\n",
    "import torch.nn.init as init\n",
    "\n",
    "from architectures import SubspaceConv2d, SubspaceLinear\n",
    "\n",
    "\"\"\"\n",
    "Intrinsic dims of MNIST - Fully-connected and CNN\n",
    "\"\"\"\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from tqdm import trange\n",
    "\n",
    "# from data import load_mnist\n",
    "# from net import SubspaceConv2d, SubspaceLinear\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SubspaceConstrainedLeNet(nn.Module):\n",
    "    def __init__(self, intrinsic_dim: int, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Subspace constrained version of PyImageSearch's LeNet implementation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.theta = Parameter(torch.empty((intrinsic_dim, 1), device=device))\n",
    "        self.theta.data.fill_(0)\n",
    "\n",
    "        self.conv1 = SubspaceConv2d(\n",
    "            self.theta,\n",
    "            in_channels=1,\n",
    "            out_channels=20,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=1,\n",
    "            device=device,\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv2 = SubspaceConv2d(\n",
    "            self.theta,\n",
    "            in_channels=20,\n",
    "            out_channels=50,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=1,\n",
    "            device=device,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.flatten1 = nn.Flatten()\n",
    "\n",
    "        self.fc1 = SubspaceLinear(\n",
    "            self.theta, in_features=800, out_features=500, device=device\n",
    "        )\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = SubspaceLinear(\n",
    "            self.theta, in_features=500, out_features=10, device=device\n",
    "        )\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.flatten1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n",
      "theta torch.Size([10, 1])\n",
      "stem\n",
      "0\n",
      "1\n",
      "stages\n",
      "0\n",
      "downsample\n",
      "blocks\n",
      "0\n",
      "conv_dw\n",
      "norm\n",
      "mlp\n",
      "fc1\n",
      "act\n",
      "drop1\n",
      "norm\n",
      "fc2\n",
      "drop2\n",
      "shortcut\n",
      "drop_path\n",
      "1\n",
      "conv_dw\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 2.94 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 2.78 GiB is allocated by PyTorch, and 30.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 91\u001b[0m\n\u001b[1;32m     86\u001b[0m args\u001b[38;5;241m.\u001b[39mft_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_fine_tuning\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     88\u001b[0m model \u001b[38;5;241m=\u001b[39m load_architecture(args, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m )\n\u001b[0;32m---> 91\u001b[0m \u001b[43mreplace_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m, in \u001b[0;36mreplace_layers\u001b[0;34m(model, intrinsic_dim, device, theta)\u001b[0m\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m new_module\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Recursively apply to child modules\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mreplace_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintrinsic_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m, in \u001b[0;36mreplace_layers\u001b[0;34m(model, intrinsic_dim, device, theta)\u001b[0m\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m new_module\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Recursively apply to child modules\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mreplace_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintrinsic_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: replace_layers at line 66 (1 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 66\u001b[0m, in \u001b[0;36mreplace_layers\u001b[0;34m(model, intrinsic_dim, device, theta)\u001b[0m\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m new_module\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Recursively apply to child modules\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mreplace_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintrinsic_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mreplace_layers\u001b[0;34m(model, intrinsic_dim, device, theta)\u001b[0m\n\u001b[1;32m     28\u001b[0m padding_mode \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Create a new SubspaceConv2d with the same parameters\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m new_module \u001b[38;5;241m=\u001b[39m \u001b[43mSubspaceConv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Replace the module in the parent module's _modules\u001b[39;00m\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m new_module\n",
      "Cell \u001b[0;32mIn[1], line 293\u001b[0m, in \u001b[0;36mSubspaceConv2d.__init__\u001b[0;34m(self, theta, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_zero \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Generate projection matrix for weights\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_mat_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mkaiming_uniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_mat_weights, a\u001b[38;5;241m=\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# Create and init bias, save bias zero\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 2.94 GiB of which 10.12 MiB is free. Including non-PyTorch memory, this process has 2.92 GiB memory in use. Of the allocated memory 2.78 GiB is allocated by PyTorch, and 30.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def replace_layers(model, intrinsic_dim, device, theta=None):\n",
    "    # Initialize theta if not provided\n",
    "    if theta is None:\n",
    "        theta = nn.Parameter(torch.empty((intrinsic_dim, 1), device=device))\n",
    "        theta.data.fill_(0)\n",
    "        print('theta', theta.shape)\n",
    "        # Register theta as a parameter of the model\n",
    "        # model.register_parameter('theta', theta)\n",
    "\n",
    "    for name, module in model._modules.items():\n",
    "        print(name)\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Get parameters from the existing Conv2d layer\n",
    "            in_channels = module.in_channels\n",
    "            out_channels = module.out_channels\n",
    "            kernel_size = module.kernel_size\n",
    "            stride = module.stride\n",
    "            padding = module.padding\n",
    "            dilation = module.dilation\n",
    "            groups = module.groups\n",
    "            bias = module.bias is not None\n",
    "            padding_mode = module.padding_mode\n",
    "\n",
    "            # Create a new SubspaceConv2d with the same parameters\n",
    "            new_module = SubspaceConv2d(\n",
    "                theta=theta,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "                groups=groups,\n",
    "                bias=bias,\n",
    "                padding_mode=padding_mode,\n",
    "                device=device\n",
    "            )\n",
    "            # Replace the module in the parent module's _modules\n",
    "            model._modules[name] = new_module\n",
    "\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            # Get parameters from the existing Linear layer\n",
    "            in_features = module.in_features\n",
    "            out_features = module.out_features\n",
    "            bias = module.bias is not None\n",
    "\n",
    "            # Create a new SubspaceLinear with the same parameters\n",
    "            new_module = SubspaceLinear(\n",
    "                theta=theta,\n",
    "                in_features=in_features,\n",
    "                out_features=out_features,\n",
    "                bias=bias,\n",
    "                device=device\n",
    "            )\n",
    "            # Replace the module\n",
    "            model._modules[name] = new_module\n",
    "\n",
    "        elif isinstance(module, nn.Module):\n",
    "            # Recursively apply to child modules\n",
    "            replace_layers(module, intrinsic_dim, device, theta=theta)\n",
    "\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "args.dataset = 'CIFAR10'\n",
    "args.selection_method = 'random'\n",
    "args.aug = 'aug'\n",
    "args.loss_function = 'CLASSIC_AT'\n",
    "\n",
    "args.iterations = 10\n",
    "args.pruning_ratio = 0\n",
    "args.delta = 1\n",
    "args.batch_size = 24\n",
    "args.init_lr = 0.001\n",
    "args.freeze_epochs = 5\n",
    "args.backbone = 'convnext_tiny' #deit_small_patch16_224.fb_in1k\n",
    "args.ft_type = 'full_fine_tuning'\n",
    "\n",
    "model = load_architecture(args, N=10, rank=0 )\n",
    "\n",
    "\n",
    "replace_layers(model, 10, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNeXt(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvNeXtStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (3): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (4): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (5): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (6): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (7): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (8): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): ConvNeXtStage(\n",
       "      (downsample): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (2): ConvNeXtBlock(\n",
       "          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU()\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (shortcut): Identity()\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_pre): Identity()\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_mnist(flatten=True):\n",
    "    if flatten is True:\n",
    "        dataset_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Lambda(lambda x: torch.flatten(x)),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        dataset_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    train = torchvision.datasets.MNIST(\n",
    "        root=\"~/.torchdata/\",\n",
    "        download=True,\n",
    "        # natively stored as PIL images\n",
    "        transform=dataset_transform,\n",
    "    )\n",
    "\n",
    "    test = torchvision.datasets.MNIST(\n",
    "        root=\"~/.torchdata/\", download=True, train=False, transform=dataset_transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=100, shuffle=True)\n",
    "    # If flatten\n",
    "    # Returns (torch.Size([100, 784]), torch.Size([100]))\n",
    "    # Else\n",
    "    # Returns (torch.Size([100, 1, 28, 28]), torch.Size([100]))\n",
    "\n",
    "    test_loader = DataLoader(test, batch_size=500, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "\n",
    "## Util functions\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(net, num_epochs, train_loader, device=\"cuda\"):\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    net.train()\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "\n",
    "    # Single progress bar over all epochs\n",
    "    pbar = trange(\n",
    "        len(train_loader) * num_epochs,\n",
    "        bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\",\n",
    "        ascii=True,\n",
    "    )\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for batch_id, (features, target) in tqdm(enumerate(train_loader)):\n",
    "            # forward pass, calculate loss and backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = net(features.to(device))\n",
    "            loss = F.nll_loss(preds, target.to(device))\n",
    "            loss.backward()\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    # Verified don't need to return the net\n",
    "    return loss_history, acc_history\n",
    "\n",
    "\n",
    "def eval(net, test_loader, device=\"cuda\"):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for features, target in test_loader:\n",
    "        output = net(features.to(device))\n",
    "        test_loss += F.nll_loss(output, target.to(device)).item()\n",
    "        pred = torch.argmax(output, dim=-1)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.to(device)).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader)  # loss function already averages over batch size\n",
    "    accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return test_loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n",
      "Training 1 repetitions for intrinsic dimension: 10\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12000 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d9f4cb707a48a8ab994fac3a0d228e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|4         | 595/12000 [00:09<02:55, 64.83it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c38cf5a4104024811ee57f28e44837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|9         | 1197/12000 [00:18<02:41, 66.90it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64021573ee89430ba9813c3a17994249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|#4        | 1798/12000 [00:27<02:38, 64.33it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6cdea35e994026bd863d3c903f18f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|##        | 2400/12000 [00:36<02:27, 65.15it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99fc883558c4cbcafcda443c186ad2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|##4       | 2994/12000 [00:46<02:15, 66.33it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7140a6db1c845ed9b8f11dd84f5d1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|##9       | 3596/12000 [00:55<02:06, 66.34it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f63cdb11414bb8acecef91a24af7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|###4      | 4198/12000 [01:04<02:01, 63.96it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f3472bd1bb442daf3d76805577f261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|###9      | 4799/12000 [01:14<01:49, 65.78it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0278094f86a944d8a825a3f43dae3820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|####5     | 5400/12000 [01:23<01:40, 65.67it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c85b6293544552b9f4e8ae425c9d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|####9     | 5995/12000 [01:32<01:33, 64.45it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8771b4af800d4c7db9731679f690e323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|#####4    | 6595/12000 [01:42<01:28, 61.40it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a233f153bc84dcaacc00f891ae248c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|#####9    | 7196/12000 [01:52<01:25, 56.11it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d41919b36543d2a445ed3ea26e2087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|######4   | 7796/12000 [02:01<01:04, 65.55it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ec67446dda4e6b88f9b5931cb0bf99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|######9   | 8398/12000 [02:10<00:54, 65.51it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c36c41782f4739b0f6606f9bb4e940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|#######5  | 9000/12000 [02:19<00:43, 68.63it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e2eca33016416cb69ddb2509f2dd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|#######9  | 9595/12000 [02:28<00:37, 63.51it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebba5b4f4864ad8bc965637dfbb725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|########4 | 10199/12000 [02:38<00:28, 64.04it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0d0b28106048129f6949579a81b636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|########9 | 10794/12000 [02:47<00:18, 66.07it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ed9efddead46988746c7963c4446d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|#########4| 11396/12000 [02:56<00:09, 61.13it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649774959705492381eafa258486f31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 12000/12000 [03:06<00:00, 64.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 2.2977, Accuracy: 1137/10000 (11%)\n",
      "\n",
      "Time taken for dim 10: 187.64s. Remaining dims: []\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset, load_data\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from architectures import CustomModel, load_architecture, add_lora, set_lora_gradients #load_statedict\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "args.dataset = 'CIFAR10'\n",
    "args.selection_method = 'random'\n",
    "args.aug = 'aug'\n",
    "args.loss_function = 'CLASSIC_AT'\n",
    "\n",
    "args.iterations = 10\n",
    "args.pruning_ratio = 0\n",
    "args.delta = 1\n",
    "args.batch_size = 24\n",
    "args.init_lr = 0.001\n",
    "args.freeze_epochs = 5\n",
    "args.backbone = 'convnext_tiny' #deit_small_patch16_224.fb_in1k\n",
    "args.ft_type = 'full_fine_tuning'\n",
    "\n",
    "\n",
    "\n",
    "im_train_loader, im_test_loader = load_mnist(flatten=False)\n",
    "flat_train_loader, flat_test_loader = load_mnist(flatten=True)\n",
    "\n",
    "\n",
    "# ## Data\n",
    "\n",
    "# im_train_loader, im_test_loader = load_mnist(flatten=False)\n",
    "# flat_train_loader, flat_test_loader = load_mnist(flatten=True)\n",
    "\n",
    "## Config\n",
    "\n",
    "\n",
    "dims = [10, ] #30, 50, 100, 300, 500, 1000, 3000, 5000\n",
    "\n",
    "num_reps = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "training_epochs = 5\n",
    "\n",
    "\n",
    "## Intrinsic dim for fully-connected network on MNIST\n",
    "\n",
    "fc_corrects = {}\n",
    "\n",
    "for count, d in enumerate(dims):\n",
    "    start_ts = timer()\n",
    "    print(f\"Training {num_reps} repetitions for intrinsic dimension: {d}\")\n",
    "\n",
    "    corrects_per_dim = {}\n",
    "\n",
    "    for i in range(num_reps):\n",
    "        print(i)\n",
    "        ssnet = SubspaceConstrainedLeNet(intrinsic_dim=d, device=device)\n",
    "\n",
    "        loss_history, acc_history = train(ssnet, 20, im_train_loader, device =device)\n",
    "        test_loss, correct = eval(ssnet, im_test_loader, device=device)\n",
    "\n",
    "        corrects_per_dim[i] = correct / 10000 * 100\n",
    "\n",
    "    fc_corrects[d] = corrects_per_dim\n",
    "\n",
    "    end_ts = timer()\n",
    "    print(\n",
    "        f\"Time taken for dim {d}: {end_ts - start_ts:.2f}s. Remaining dims: {dims[count+1:]}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(fc_corrects)\n",
    "df.to_csv(\"fc-mnist-accuracy.csv\")\n",
    "\n",
    "tidydf = df.melt()\n",
    "tidydf = tidydf.rename(columns={\"variable\": \"num_id\", \"value\": \"acc\"})\n",
    "\n",
    "# Generate and save plot\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=tidydf, x=\"num_id\", y=\"acc\")\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\n",
    "    \"Accuracy of fully-connected network on MNIST, by constrained intrinsic dimension\"\n",
    ")\n",
    "plt.savefig(\"fc-results.PNG\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "## Intrinsic dim for convolutional network on MNIST\n",
    "\n",
    "conv_corrects = {}\n",
    "\n",
    "for count, d in enumerate(dims):\n",
    "    start_ts = timer()\n",
    "    print(f\"Training {num_reps} repetitions for intrinsic dimension: {d}\")\n",
    "\n",
    "    corrects_per_dim = {}\n",
    "\n",
    "    for i in range(num_reps):\n",
    "        ssnet = SubspaceConstrainedLeNet(intrinsic_dim=d, device=device)\n",
    "\n",
    "        loss_history, acc_history = train(ssnet, 20, im_train_loader, device=device)\n",
    "        test_loss, correct = eval(ssnet, im_test_loader, device=device)\n",
    "\n",
    "        corrects_per_dim[i] = correct / 10000 * 100\n",
    "\n",
    "    conv_corrects[d] = corrects_per_dim\n",
    "\n",
    "    end_ts = timer()\n",
    "    print(\n",
    "        f\"Time taken for dim {d}: {end_ts - start_ts:.2f}s. Remaining dims: {dims[count+1:]}!\"\n",
    "    )\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(conv_corrects)\n",
    "df.to_csv(\"conv-mnist-accuracy.csv\")\n",
    "\n",
    "tidydf = df.melt()\n",
    "tidydf = tidydf.rename(columns={\"variable\": \"num_id\", \"value\": \"acc\"})\n",
    "\n",
    "# Generate and save plot\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "sns.boxplot(data=tidydf, x=\"num_id\", y=\"acc\")\n",
    "plt.xlabel(\"Intrinsic dimension\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy of conv network on MNIST, by constrained intrinsic dimension\")\n",
    "plt.savefig(\"conv-results.PNG\", bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
