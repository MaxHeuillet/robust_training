{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uc-merced-land-use-dataset\n",
      "Warning: test dataset has only 420 samples. No subsampling performed.\n",
      "Warning: test dataset has only 420 samples. No subsampling performed.\n",
      "✅ Saved to: /home/mheuillet/Desktop/processed_dataset/uc-merced-land-use-dataset_processed.pkl.gz\n",
      "flowers-102\n",
      "✅ Saved to: /home/mheuillet/Desktop/processed_dataset/flowers-102_processed.pkl.gz\n",
      "caltech101\n",
      "✅ Saved to: /home/mheuillet/Desktop/processed_dataset/caltech101_processed.pkl.gz\n",
      "stanford_cars\n",
      "✅ Saved to: /home/mheuillet/Desktop/processed_dataset/stanford_cars_processed.pkl.gz\n",
      "fgvc-aircraft-2013b\n",
      "✅ Saved to: /home/mheuillet/Desktop/processed_dataset/fgvc-aircraft-2013b_processed.pkl.gz\n",
      "oxford-iiit-pet\n",
      "✅ Saved to: /home/mheuillet/Desktop/processed_dataset/oxford-iiit-pet_processed.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from databases import load_data\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "config = OmegaConf.load(\"./configs/default_config_linearprobe50.yaml\")\n",
    "config.datasets_path = './data' # where you store datasets locally \n",
    "\n",
    "for dataset in [ \n",
    "                'uc-merced-land-use-dataset',   \n",
    "                 'flowers-102', \n",
    "                 'caltech101',\n",
    "                 'stanford_cars', \n",
    "                 'fgvc-aircraft-2013b',\n",
    "                 'oxford-iiit-pet'\n",
    "                   ]:\n",
    "                #]: #'dtd', 'eurosat' ,\n",
    "\n",
    "    print(dataset)\n",
    "    \n",
    "    config.dataset = dataset\n",
    "    train_dataset, val_dataset, test_dataset, N = load_data(config, common_corruption=False)\n",
    "    _, _, common_test_dataset, N = load_data(config, common_corruption=True)\n",
    "\n",
    "    output_path = \"/home/mheuillet/Desktop/processed_dataset/{}_processed.pkl.gz\".format(config.dataset)\n",
    "    with gzip.open(output_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"train\": train_dataset,\n",
    "            \"val\": val_dataset,\n",
    "            \"test\": test_dataset,\n",
    "            \"test_common\":common_test_dataset,\n",
    "            \"N\": N\n",
    "        }, f)\n",
    "\n",
    "    print(f\"✅ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uc-merced-land-use-dataset\n",
      "✅ Saved to: /home/mheuillet/Desktop/processed_dataset/oxford-iiit-pet_processed.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from databases import load_data\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "config = OmegaConf.load(\"./configs/default_config_linearprobe50.yaml\")\n",
    "config.datasets_path = './data' # where you store datasets locally \n",
    "\n",
    "for dataset in [ \n",
    "                'uc-merced-land-use-dataset',   \n",
    "                #  'flowers-102', \n",
    "                #  'caltech101',\n",
    "                #  'stanford_cars', \n",
    "                #  'fgvc-aircraft-2013b',\n",
    "                #  'oxford-iiit-pet'\n",
    "                   ]:\n",
    "                #]: #'dtd', 'eurosat' ,\n",
    "\n",
    "    print(dataset)\n",
    "    \n",
    "    config.dataset = dataset\n",
    "    \n",
    "    path = \"/home/mheuillet/Desktop/processed_dataset/{}_processed.pkl.gz\".format(config.dataset)\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(f\"✅ Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f11588cae10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['val'].subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import tarfile\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import zstandard as zstd\n",
    "\n",
    "for dataset in [ \n",
    "                # 'uc-merced-land-use-dataset',   \n",
    "                 'flowers-102', \n",
    "                 'caltech101',\n",
    "                 'stanford_cars', \n",
    "                 'fgvc-aircraft-2013b',\n",
    "                 'oxford-iiit-pet'\n",
    "                   ]:\n",
    "\n",
    "    # === CONFIG ===\n",
    "    pkl_path = \"/home/mheuillet/Desktop/processed_dataset/{}_processed.pkl.gz\".format(dataset)\n",
    "    output_tar_path = \"/home/mheuillet/Desktop/processed_dataset/{}_processed.tar.zst\".format(dataset)\n",
    "    tmp_dir = Path(\"/home/mheuillet/Desktop/processed_dataset/tmp_extracted\")  # temp folder before archiving\n",
    "\n",
    "    # === STEP 1: Load pkl.gz ===\n",
    "    with gzip.open(pkl_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    splits = [\"train\", \"val\", \"test\", \"test_common\"]\n",
    "    N = data.get(\"N\", None)\n",
    "\n",
    "    # === STEP 2: Write files to temporary directory ===\n",
    "    if tmp_dir.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(tmp_dir)\n",
    "    tmp_dir.mkdir(parents=True)\n",
    "\n",
    "    metadata = {\"N\": N, \"splits\": {}}\n",
    "\n",
    "    for split in splits:\n",
    "        dataset = data[split]\n",
    "        split_dir = tmp_dir / split\n",
    "        split_dir.mkdir()\n",
    "\n",
    "        labels_path = split_dir / \"labels.csv\"\n",
    "        with open(labels_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"filename\", \"label\"])\n",
    "\n",
    "            for i, (img, label) in enumerate(dataset):\n",
    "                filename = f\"{i:05d}.png\"\n",
    "                img_path = split_dir / filename\n",
    "\n",
    "                # Save image\n",
    "                if isinstance(img, torch.Tensor):\n",
    "                    img = transforms.ToPILImage()(img)\n",
    "                img.save(img_path)\n",
    "\n",
    "                # Save label\n",
    "                writer.writerow([filename, label])\n",
    "\n",
    "            metadata[\"splits\"][split] = {\"count\": i + 1}\n",
    "\n",
    "    # === STEP 3: Create tar.zst archive ===\n",
    "    cctx = zstd.ZstdCompressor(level=3)\n",
    "    with open(output_tar_path, \"wb\") as f_out:\n",
    "        with cctx.stream_writer(f_out) as zst_stream:\n",
    "            with tarfile.open(fileobj=zst_stream, mode=\"w|\") as tar:\n",
    "                for root, dirs, files in os.walk(tmp_dir):\n",
    "                    for file in files:\n",
    "                        fullpath = os.path.join(root, file)\n",
    "                        arcname = os.path.relpath(fullpath, start=tmp_dir)\n",
    "                        tar.add(fullpath, arcname=arcname)\n",
    "\n",
    "                # Include metadata.json\n",
    "                meta_bytes = json.dumps(metadata, indent=2).encode(\"utf-8\")\n",
    "                tarinfo = tarfile.TarInfo(name=\"metadata.json\")\n",
    "                tarinfo.size = len(meta_bytes)\n",
    "                tar.addfile(tarinfo, io.BytesIO(meta_bytes))\n",
    "\n",
    "    print(f\"✅ Saved: {output_tar_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### don't pay attention to this cell\n",
    "\n",
    "# EuroSAT: seulement 8 runs on complete et tous les autres OOM\n",
    "# Caltech: 26 runs ok, \n",
    "# Stanford Cars: 34 ok\n",
    "# Aircraft: 26 ok\n",
    "# DTD : 26 ok\n",
    "# Oxford pet: 26 ok\n",
    "# Flowers: 26 ok\n",
    "\n",
    "stanford_cars = 6922 + 1222 + 8041 \n",
    "caltech101 = 5899 + 1042 + 1736 \n",
    "dtd = 1880 + 1880 + 1880 \n",
    "eurosat = 18360 + 3240 + 5400 \n",
    "fgvcaircraft2013b =  3334 + 3333 + 3333 \n",
    "flowers102 = 1020 + 1020 + 6149 \n",
    "oxfordiiitpet = 3128 + 552 + 3669 \n",
    "\n",
    "print(eurosat, caltech101, dtd, fgvcaircraft2013b, flowers102, oxfordiiitpet, stanford_cars)\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Initialize the Caltech101 dataset without transformations\n",
    "caltech101_sample = datasets.Caltech101(root='./data', download=False)\n",
    "print(len(caltech101_sample))\n",
    "for i in range( len(caltech101_sample) ) :\n",
    "    image, label = caltech101_sample[i]\n",
    "\n",
    "    # Check the image mode\n",
    "    # print(f\"Image Mode: {image.mode}\")  # Expected: 'RGB'\n",
    "    if image.mode != 'RGB':\n",
    "        print('issue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
