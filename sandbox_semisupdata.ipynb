{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loading data from /cluster/scratch/rarade/cifar10s/ti_500K_pseudo_labeled.pickle\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/cluster/scratch/rarade/cifar10s/ti_500K_pseudo_labeled.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 287\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, test_dataset, train_dataloader, test_dataloader\n\u001b[1;32m    286\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 287\u001b[0m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m          \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m          \u001b[49m\u001b[43muse_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m          \u001b[49m\u001b[43muse_consistency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m          \u001b[49m\u001b[43mshuffle_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m          \u001b[49m\u001b[43maux_data_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m          \u001b[49m\u001b[43munsup_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 266\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(data_dir, batch_size, batch_size_test, num_workers, use_augmentation, use_consistency, shuffle_train, aux_data_filename, unsup_fraction, validation)\u001b[0m\n\u001b[1;32m    263\u001b[0m     train_dataset, test_dataset, val_dataset \u001b[38;5;241m=\u001b[39m load_cifar10s(data_dir\u001b[38;5;241m=\u001b[39mdata_dir, use_augmentation\u001b[38;5;241m=\u001b[39muse_augmentation, use_consistency\u001b[38;5;241m=\u001b[39muse_consistency,\n\u001b[1;32m    264\u001b[0m                                                                aux_data_filename\u001b[38;5;241m=\u001b[39maux_data_filename, validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     train_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_cifar10s\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_augmentation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_augmentation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# if dataset in SEMISUP_DATASETS:\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation:\n",
      "Cell \u001b[0;32mIn[1], line 220\u001b[0m, in \u001b[0;36mload_cifar10s\u001b[0;34m(data_dir, use_augmentation, use_consistency, aux_take_amount, aux_data_filename, validation)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# train_transform = MultiDataTransform(train_transform)\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSemiSupervisedCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcifar10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_data_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_data_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43madd_aux_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_take_amount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_take_amount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SemiSupervisedCIFAR10(base_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m'\u001b[39m, root\u001b[38;5;241m=\u001b[39mdata_dir, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    224\u001b[0m                                      transform\u001b[38;5;241m=\u001b[39mtest_transform)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation:\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36mSemiSupervisedDataset.__init__\u001b[0;34m(self, base_dataset, take_amount, take_amount_seed, aux_data_filename, add_aux_labels, aux_take_amount, train, validation, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading data from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m aux_path)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(aux_path)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# for data from Carmon et al, 2019.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maux_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     44\u001b[0m         aux \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     45\u001b[0m     aux_data \u001b[38;5;241m=\u001b[39m aux[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/cluster/scratch/rarade/cifar10s/ti_500K_pseudo_labeled.pickle'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import re\n",
    "\n",
    "class SemiSupervisedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset with auxiliary pseudo-labeled data.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset='cifar10', take_amount=None, take_amount_seed=13, aux_data_filename=None, \n",
    "                 add_aux_labels=False, aux_take_amount=None, train=False, validation=False, **kwargs):\n",
    "\n",
    "        self.base_dataset = base_dataset\n",
    "        self.load_base_dataset(train, **kwargs)\n",
    "\n",
    "\n",
    "        if validation:\n",
    "            self.dataset.data = self.dataset.data[1024:]\n",
    "            self.dataset.targets = self.dataset.targets[1024:]\n",
    "        \n",
    "        self.train = train\n",
    "\n",
    "        if self.train:\n",
    "            if take_amount is not None:\n",
    "                rng_state = np.random.get_state()\n",
    "                np.random.seed(take_amount_seed)\n",
    "                take_inds = np.random.choice(len(self.sup_indices), take_amount, replace=False)\n",
    "                np.random.set_state(rng_state)\n",
    "\n",
    "                self.targets = self.targets[take_inds]\n",
    "                self.data = self.data[take_inds]\n",
    "\n",
    "            self.sup_indices = list(range(len(self.targets)))\n",
    "            self.unsup_indices = []\n",
    "\n",
    "            if aux_data_filename is not None:\n",
    "                aux_path = aux_data_filename\n",
    "                print('Loading data from %s' % aux_path)\n",
    "                if os.path.splitext(aux_path)[1] == '.pickle':\n",
    "                    # for data from Carmon et al, 2019.\n",
    "                    with open(aux_path, 'rb') as f:\n",
    "                        aux = pickle.load(f)\n",
    "                    aux_data = aux['data']\n",
    "                    aux_targets = aux['extrapolated_targets']\n",
    "                else:\n",
    "                    # for data from Rebuffi et al, 2021.\n",
    "                    aux = np.load(aux_path)\n",
    "                    aux_data = aux['image']\n",
    "                    print(aux_data.shape)\n",
    "                    aux_targets = aux['label']\n",
    "                \n",
    "                orig_len = len(self.data)\n",
    "\n",
    "                if aux_take_amount is not None:\n",
    "                    rng_state = np.random.get_state()\n",
    "                    np.random.seed(take_amount_seed)\n",
    "                    take_inds = np.random.choice(len(aux_data), aux_take_amount, replace=False)\n",
    "                    np.random.set_state(rng_state)\n",
    "\n",
    "                    aux_data = aux_data[take_inds]\n",
    "                    aux_targets = aux_targets[take_inds]\n",
    "\n",
    "                self.data = np.concatenate((self.data, aux_data), axis=0)\n",
    "\n",
    "                if not add_aux_labels:\n",
    "                    self.targets.extend([-1] * len(aux_data))\n",
    "                else:\n",
    "                    self.targets.extend(aux_targets)\n",
    "                self.unsup_indices.extend(range(orig_len, orig_len+len(aux_data)))\n",
    "\n",
    "        else:\n",
    "            self.sup_indices = list(range(len(self.targets)))\n",
    "            self.unsup_indices = []\n",
    "    \n",
    "    def load_base_dataset(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.dataset.data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, value):\n",
    "        self.dataset.data = value\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self.dataset.targets\n",
    "\n",
    "    @targets.setter\n",
    "    def targets(self, value):\n",
    "        self.dataset.targets = value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        self.dataset.labels = self.targets\n",
    "        return self.dataset[item]\n",
    "    \n",
    "\n",
    "class SemiSupervisedSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"\n",
    "    Balanced sampling from the labeled and unlabeled data.\n",
    "    \"\"\"\n",
    "    def __init__(self, sup_inds, unsup_inds, batch_size, unsup_fraction=0.5, num_batches=None):\n",
    "        if unsup_fraction is None or unsup_fraction < 0:\n",
    "            self.sup_inds = sup_inds + unsup_inds\n",
    "            unsup_fraction = 0.0\n",
    "        else:\n",
    "            self.sup_inds = sup_inds\n",
    "            self.unsup_inds = unsup_inds\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        unsup_batch_size = int(batch_size * unsup_fraction)\n",
    "        self.sup_batch_size = batch_size - unsup_batch_size\n",
    "\n",
    "        if num_batches is not None:\n",
    "            self.num_batches = num_batches\n",
    "        else:\n",
    "            self.num_batches = int(np.ceil(len(self.sup_inds) / self.sup_batch_size))\n",
    "        super().__init__(None)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_counter = 0\n",
    "        while batch_counter < self.num_batches:\n",
    "            sup_inds_shuffled = [self.sup_inds[i]\n",
    "                                 for i in torch.randperm(len(self.sup_inds))]\n",
    "            for sup_k in range(0, len(self.sup_inds), self.sup_batch_size):\n",
    "                if batch_counter == self.num_batches:\n",
    "                    break\n",
    "                batch = sup_inds_shuffled[sup_k:(sup_k + self.sup_batch_size)]\n",
    "                if self.sup_batch_size < self.batch_size:\n",
    "                    batch.extend([self.unsup_inds[i] for i in torch.randint(high=len(self.unsup_inds), \n",
    "                                                                            size=(self.batch_size - len(batch),), \n",
    "                                                                            dtype=torch.int64)])\n",
    "                np.random.shuffle(batch)\n",
    "                yield batch\n",
    "                batch_counter += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "def get_semisup_dataloaders(train_dataset, test_dataset, val_dataset=None, batch_size=256, batch_size_test=256, num_workers=4, \n",
    "                            unsup_fraction=0.5):\n",
    "    \"\"\"\n",
    "    Return dataloaders with custom sampling of pseudo-labeled data.\n",
    "    \"\"\"\n",
    "    dataset_size = train_dataset.dataset_size\n",
    "    train_batch_sampler = SemiSupervisedSampler(train_dataset.sup_indices, train_dataset.unsup_indices, batch_size, \n",
    "                                                unsup_fraction, num_batches=int(np.ceil(dataset_size/batch_size)))\n",
    "    \n",
    "    epoch_size = len(train_batch_sampler) * batch_size\n",
    "\n",
    "    # kwargs = {'num_workers': num_workers, 'pin_memory': torch.cuda.is_available() }\n",
    "    kwargs = {'num_workers': num_workers, 'pin_memory': False}    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, **kwargs)\n",
    "    \n",
    "    if val_dataset:\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size_test, shuffle=False, **kwargs)\n",
    "        return train_dataloader, test_dataloader, val_dataloader\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_cifar10s(data_dir, use_augmentation='base', use_consistency=False, aux_take_amount=None, \n",
    "                  aux_data_filename='/cluster/scratch/rarade/cifar10s/ti_500K_pseudo_labeled.pickle', \n",
    "                  validation=False):\n",
    "    \"\"\"\n",
    "    Returns semisupervised CIFAR10 train, test datasets and dataloaders (with Tiny Images).\n",
    "    Arguments:\n",
    "        data_dir (str): path to data directory.\n",
    "        use_augmentation: use different augmentations for training set.\n",
    "        aux_take_amount (int): number of semi-supervised examples to use (if None, use all).\n",
    "        aux_data_filename (str): path to additional data pickle file.\n",
    "    Returns:\n",
    "        train dataset, test dataset. \n",
    "    \"\"\"\n",
    "    data_dir = re.sub('cifar10s', 'cifar10', data_dir)\n",
    "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    if use_augmentation == 'none':\n",
    "        train_transform = test_transform\n",
    "    elif use_augmentation == 'base':\n",
    "        train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(0.5), \n",
    "                                              transforms.ToTensor()])\n",
    "    # elif use_augmentation == 'cutout':\n",
    "    #     train_transform = transforms.Compose([\n",
    "    #         transforms.RandomCrop(32, padding=4),\n",
    "    #         transforms.RandomHorizontalFlip(0.5),\n",
    "    #         transforms.ToTensor(),\n",
    "    #     ])\n",
    "    #     train_transform.transforms.append(CutoutDefault(18))\n",
    "    # elif use_augmentation == 'autoaugment':\n",
    "    #     train_transform = transforms.Compose([\n",
    "    #         transforms.RandomCrop(32, padding=4),\n",
    "    #         transforms.RandomHorizontalFlip(0.5),\n",
    "    #         CIFAR10Policy(),\n",
    "    #         transforms.ToTensor(),\n",
    "    #     ])\n",
    "    #     train_transform.transforms.append(CutoutDefault(18))\n",
    "    # elif use_augmentation == 'randaugment':\n",
    "    #     train_transform = transforms.Compose([\n",
    "    #         transforms.RandomCrop(32, padding=4),\n",
    "    #         transforms.RandomHorizontalFlip(0.5),\n",
    "    #         transforms.ToTensor(),\n",
    "    #     ])\n",
    "    #     # Add RandAugment with N, M(hyperparameter), N=2, M=14 for wdn-28-10\n",
    "    #     train_transform.transforms.insert(0, RandAugment(2, 14))\n",
    "    # elif use_augmentation == 'idbh':\n",
    "    #     train_transform = IDBH('cifar10-weak')\n",
    "    \n",
    "    if use_consistency:\n",
    "        pass\n",
    "        # train_transform = MultiDataTransform(train_transform)\n",
    "\n",
    "    train_dataset = SemiSupervisedCIFAR10(base_dataset='cifar10', root=data_dir, train=True, download=True, \n",
    "                                          transform=train_transform, aux_data_filename=aux_data_filename, \n",
    "                                          add_aux_labels=True, aux_take_amount=aux_take_amount, validation=validation)\n",
    "    test_dataset = SemiSupervisedCIFAR10(base_dataset='cifar10', root=data_dir, train=False, download=True, \n",
    "                                         transform=test_transform)\n",
    "    if validation:\n",
    "        val_dataset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=test_transform)\n",
    "        val_dataset = torch.utils.data.Subset(val_dataset, np.arange(0, 1024))  # split from training set\n",
    "        return train_dataset, test_dataset, val_dataset\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "class SemiSupervisedCIFAR10(SemiSupervisedDataset):\n",
    "    \"\"\"\n",
    "    A dataset with auxiliary pseudo-labeled data for CIFAR10.\n",
    "    \"\"\"\n",
    "    def load_base_dataset(self, train=False, **kwargs):\n",
    "        assert self.base_dataset == 'cifar10', 'Only semi-supervised cifar10 is supported. Please use correct dataset!'\n",
    "        self.dataset = datasets.CIFAR10(train=train, **kwargs)\n",
    "        self.dataset_size = len(self.dataset)\n",
    "\n",
    "\n",
    "def load_data(data_dir, batch_size=256, batch_size_test=256, num_workers=4, use_augmentation='base', use_consistency=False, shuffle_train=True, \n",
    "              aux_data_filename=None, unsup_fraction=None, validation=False):\n",
    "    \"\"\"\n",
    "    Returns train, test datasets and dataloaders.\n",
    "    Arguments:\n",
    "        data_dir (str): path to data directory.\n",
    "        batch_size (int): batch size for training.\n",
    "        batch_size_test (int): batch size for validation.\n",
    "        num_workers (int): number of workers for loading the data.\n",
    "        use_augmentation (base/none): whether to use augmentations for training set.\n",
    "        shuffle_train (bool): whether to shuffle training set.\n",
    "        aux_data_filename (str): path to unlabelled data.\n",
    "        unsup_fraction (float): fraction of unlabelled data per batch.\n",
    "        validation (bool): if True, also returns a validation dataloader for unspervised cifar10 (as in Gowal et al, 2020).\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = os.path.basename(os.path.normpath(data_dir))\n",
    "    # load_dataset_fn = _LOAD_DATASET_FN[dataset]\n",
    "    \n",
    "    if validation:\n",
    "        # assert dataset in SEMISUP_DATASETS, 'Only semi-supervised datasets allow a validation set.'\n",
    "        train_dataset, test_dataset, val_dataset = load_cifar10s(data_dir=data_dir, use_augmentation=use_augmentation, use_consistency=use_consistency,\n",
    "                                                                   aux_data_filename=aux_data_filename, validation=True)\n",
    "    else:\n",
    "        train_dataset, test_dataset = load_cifar10s(data_dir=data_dir, use_augmentation=use_augmentation)\n",
    "       \n",
    "    # if dataset in SEMISUP_DATASETS:\n",
    "    if validation:\n",
    "        train_dataloader, test_dataloader, val_dataloader = get_semisup_dataloaders(\n",
    "                train_dataset, test_dataset, val_dataset, batch_size, batch_size_test, num_workers, unsup_fraction )\n",
    "    else:\n",
    "        train_dataloader, test_dataloader = get_semisup_dataloaders(\n",
    "                train_dataset, test_dataset, None, batch_size, batch_size_test, num_workers, unsup_fraction )\n",
    "    # else:\n",
    "    #     #pin_memory = torch.cuda.is_available()\n",
    "    #     pin_memory = False\n",
    "    #     train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train, \n",
    "    #                                                    num_workers=num_workers, pin_memory=pin_memory)\n",
    "    #     test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, \n",
    "    #                                                   num_workers=num_workers, pin_memory=pin_memory)\n",
    "    if validation:\n",
    "        return train_dataset, test_dataset, val_dataset, train_dataloader, test_dataloader, val_dataloader\n",
    "    return train_dataset, test_dataset, train_dataloader, test_dataloader\n",
    "\n",
    "data_dir = './data'\n",
    "load_data(data_dir, \n",
    "          batch_size=256, \n",
    "          batch_size_test=256, \n",
    "          num_workers=1, \n",
    "          use_augmentation='base', \n",
    "          use_consistency=False, \n",
    "          shuffle_train=True, \n",
    "          aux_data_filename=None, \n",
    "          unsup_fraction=None, \n",
    "          validation=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
