{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Loading data from ./data/1m.npz\n",
      "(1000000, 32, 32, 3)\n",
      "48976\n",
      "concatenation\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 290\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, test_dataset, train_dataloader, test_dataloader\n\u001b[1;32m    289\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 290\u001b[0m train_dataset, test_dataset, train_dataloader, test_dataloader \u001b[38;5;241m=\u001b[39m load_data(data_dir, \n\u001b[1;32m    291\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m    292\u001b[0m           batch_size_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, \n\u001b[1;32m    293\u001b[0m           num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m    294\u001b[0m           use_augmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    295\u001b[0m           use_consistency\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    296\u001b[0m           shuffle_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    297\u001b[0m           aux_data_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/1m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    298\u001b[0m           unsup_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    299\u001b[0m           validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import re\n",
    "\n",
    "class SemiSupervisedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset with auxiliary pseudo-labeled data.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dataset='cifar10', take_amount=None, take_amount_seed=13, aux_data_filename=None, \n",
    "                 add_aux_labels=False, aux_take_amount=None, train=False, validation=False, **kwargs):\n",
    "\n",
    "        self.base_dataset = base_dataset\n",
    "        self.load_base_dataset(train, **kwargs)\n",
    "\n",
    "\n",
    "        if validation:\n",
    "            self.dataset.data = self.dataset.data[1024:]\n",
    "            self.dataset.targets = self.dataset.targets[1024:]\n",
    "        \n",
    "        self.train = train\n",
    "\n",
    "        if self.train:\n",
    "            if take_amount is not None:\n",
    "                rng_state = np.random.get_state()\n",
    "                np.random.seed(take_amount_seed)\n",
    "                take_inds = np.random.choice(len(self.sup_indices), take_amount, replace=False)\n",
    "                np.random.set_state(rng_state)\n",
    "\n",
    "                self.targets = self.targets[take_inds]\n",
    "                self.data = self.data[take_inds]\n",
    "\n",
    "            self.sup_indices = list(range(len(self.targets)))\n",
    "            self.unsup_indices = []\n",
    "\n",
    "            if aux_data_filename is not None:\n",
    "                aux_path = aux_data_filename\n",
    "                print('Loading data from %s' % aux_path)\n",
    "                if os.path.splitext(aux_path)[1] == '.pickle':\n",
    "                    # for data from Carmon et al, 2019.\n",
    "                    with open(aux_path, 'rb') as f:\n",
    "                        aux = pickle.load(f)\n",
    "                    aux_data = aux['data']\n",
    "                    aux_targets = aux['extrapolated_targets']\n",
    "                else:\n",
    "                    # for data from Rebuffi et al, 2021.\n",
    "                    aux = np.load(aux_path)\n",
    "                    aux_data = aux['image']\n",
    "                    print(aux_data.shape)\n",
    "                    aux_targets = aux['label']\n",
    "                \n",
    "                orig_len = len(self.data)\n",
    "                print(orig_len)\n",
    "\n",
    "                if aux_take_amount is not None:\n",
    "                    rng_state = np.random.get_state()\n",
    "                    np.random.seed(take_amount_seed)\n",
    "                    take_inds = np.random.choice(len(aux_data), aux_take_amount, replace=False)\n",
    "                    np.random.set_state(rng_state)\n",
    "\n",
    "                    aux_data = aux_data[take_inds]\n",
    "                    aux_targets = aux_targets[take_inds]\n",
    "\n",
    "                self.data = np.concatenate((self.data, aux_data), axis=0)\n",
    "                print('concatenation')\n",
    "                if not add_aux_labels:\n",
    "                    self.targets.extend([-1] * len(aux_data))\n",
    "                else:\n",
    "                    self.targets.extend(aux_targets)\n",
    "                self.unsup_indices.extend(range(orig_len, orig_len+len(aux_data)))\n",
    "\n",
    "        else:\n",
    "            self.sup_indices = list(range(len(self.targets)))\n",
    "            self.unsup_indices = []\n",
    "    \n",
    "    def load_base_dataset(self, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.dataset.data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, value):\n",
    "        self.dataset.data = value\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self.dataset.targets\n",
    "\n",
    "    @targets.setter\n",
    "    def targets(self, value):\n",
    "        self.dataset.targets = value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        self.dataset.labels = self.targets\n",
    "        return self.dataset[item]\n",
    "    \n",
    "\n",
    "class SemiSupervisedSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"\n",
    "    Balanced sampling from the labeled and unlabeled data.\n",
    "    \"\"\"\n",
    "    def __init__(self, sup_inds, unsup_inds, batch_size, unsup_fraction=0.5, num_batches=None):\n",
    "        if unsup_fraction is None or unsup_fraction < 0:\n",
    "            self.sup_inds = sup_inds + unsup_inds\n",
    "            unsup_fraction = 0.0\n",
    "        else:\n",
    "            self.sup_inds = sup_inds\n",
    "            self.unsup_inds = unsup_inds\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        unsup_batch_size = int(batch_size * unsup_fraction)\n",
    "        self.sup_batch_size = batch_size - unsup_batch_size\n",
    "\n",
    "        if num_batches is not None:\n",
    "            self.num_batches = num_batches\n",
    "        else:\n",
    "            self.num_batches = int(np.ceil(len(self.sup_inds) / self.sup_batch_size))\n",
    "        super().__init__(None)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_counter = 0\n",
    "        while batch_counter < self.num_batches:\n",
    "            sup_inds_shuffled = [self.sup_inds[i]\n",
    "                                 for i in torch.randperm(len(self.sup_inds))]\n",
    "            for sup_k in range(0, len(self.sup_inds), self.sup_batch_size):\n",
    "                if batch_counter == self.num_batches:\n",
    "                    break\n",
    "                batch = sup_inds_shuffled[sup_k:(sup_k + self.sup_batch_size)]\n",
    "                if self.sup_batch_size < self.batch_size:\n",
    "                    batch.extend([self.unsup_inds[i] for i in torch.randint(high=len(self.unsup_inds), \n",
    "                                                                            size=(self.batch_size - len(batch),), \n",
    "                                                                            dtype=torch.int64)])\n",
    "                np.random.shuffle(batch)\n",
    "                yield batch\n",
    "                batch_counter += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "def get_semisup_dataloaders(train_dataset, test_dataset, val_dataset=None, batch_size=256, batch_size_test=256, num_workers=4, \n",
    "                            unsup_fraction=0.5):\n",
    "    \"\"\"\n",
    "    Return dataloaders with custom sampling of pseudo-labeled data.\n",
    "    \"\"\"\n",
    "    dataset_size = train_dataset.dataset_size\n",
    "    train_batch_sampler = SemiSupervisedSampler(train_dataset.sup_indices, train_dataset.unsup_indices, batch_size, \n",
    "                                                unsup_fraction, num_batches=int(np.ceil(dataset_size/batch_size)))\n",
    "    \n",
    "    epoch_size = len(train_batch_sampler) * batch_size\n",
    "\n",
    "    # kwargs = {'num_workers': num_workers, 'pin_memory': torch.cuda.is_available() }\n",
    "    kwargs = {'num_workers': num_workers, 'pin_memory': False}    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_batch_sampler, **kwargs)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, **kwargs)\n",
    "    \n",
    "    if val_dataset:\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size_test, shuffle=False, **kwargs)\n",
    "        return train_dataloader, test_dataloader, val_dataloader\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_cifar10s(data_dir, use_augmentation='base', use_consistency=False, aux_take_amount=None, \n",
    "                  aux_data_filename=None, \n",
    "                  validation=False):\n",
    "    \"\"\"\n",
    "    Returns semisupervised CIFAR10 train, test datasets and dataloaders (with Tiny Images).\n",
    "    Arguments:\n",
    "        data_dir (str): path to data directory.\n",
    "        use_augmentation: use different augmentations for training set.\n",
    "        aux_take_amount (int): number of semi-supervised examples to use (if None, use all).\n",
    "        aux_data_filename (str): path to additional data pickle file.\n",
    "    Returns:\n",
    "        train dataset, test dataset. \n",
    "    \"\"\"\n",
    "    data_dir = re.sub('cifar10s', 'cifar10', data_dir)\n",
    "    test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    if use_augmentation == 'none':\n",
    "        train_transform = test_transform\n",
    "    elif use_augmentation == 'base':\n",
    "        train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(0.5), \n",
    "                                              transforms.ToTensor()])\n",
    "    # elif use_augmentation == 'cutout':\n",
    "    #     train_transform = transforms.Compose([\n",
    "    #         transforms.RandomCrop(32, padding=4),\n",
    "    #         transforms.RandomHorizontalFlip(0.5),\n",
    "    #         transforms.ToTensor(),\n",
    "    #     ])\n",
    "    #     train_transform.transforms.append(CutoutDefault(18))\n",
    "    # elif use_augmentation == 'autoaugment':\n",
    "    #     train_transform = transforms.Compose([\n",
    "    #         transforms.RandomCrop(32, padding=4),\n",
    "    #         transforms.RandomHorizontalFlip(0.5),\n",
    "    #         CIFAR10Policy(),\n",
    "    #         transforms.ToTensor(),\n",
    "    #     ])\n",
    "    #     train_transform.transforms.append(CutoutDefault(18))\n",
    "    # elif use_augmentation == 'randaugment':\n",
    "    #     train_transform = transforms.Compose([\n",
    "    #         transforms.RandomCrop(32, padding=4),\n",
    "    #         transforms.RandomHorizontalFlip(0.5),\n",
    "    #         transforms.ToTensor(),\n",
    "    #     ])\n",
    "    #     # Add RandAugment with N, M(hyperparameter), N=2, M=14 for wdn-28-10\n",
    "    #     train_transform.transforms.insert(0, RandAugment(2, 14))\n",
    "    # elif use_augmentation == 'idbh':\n",
    "    #     train_transform = IDBH('cifar10-weak')\n",
    "    \n",
    "    if use_consistency:\n",
    "        pass\n",
    "        # train_transform = MultiDataTransform(train_transform)\n",
    "\n",
    "    train_dataset = SemiSupervisedCIFAR10(base_dataset='cifar10', \n",
    "                                          root=data_dir, train=True, download=True, \n",
    "                                          transform=train_transform, aux_data_filename=aux_data_filename, \n",
    "                                          add_aux_labels=True, aux_take_amount=aux_take_amount, validation=validation)\n",
    "    \n",
    "    test_dataset = SemiSupervisedCIFAR10(base_dataset='cifar10', root=data_dir, train=False, download=True, \n",
    "                                         transform=test_transform)\n",
    "    if validation:\n",
    "        val_dataset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=test_transform)\n",
    "        val_dataset = torch.utils.data.Subset(val_dataset, np.arange(0, 1024))  # split from training set\n",
    "        return train_dataset, test_dataset, val_dataset\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "class SemiSupervisedCIFAR10(SemiSupervisedDataset):\n",
    "    \"\"\"\n",
    "    A dataset with auxiliary pseudo-labeled data for CIFAR10.\n",
    "    \"\"\"\n",
    "    def load_base_dataset(self, train=False, **kwargs):\n",
    "        assert self.base_dataset == 'cifar10', 'Only semi-supervised cifar10 is supported. Please use correct dataset!'\n",
    "        self.dataset = datasets.CIFAR10(train=train, **kwargs)\n",
    "        self.dataset_size = len(self.dataset)\n",
    "\n",
    "\n",
    "def load_data(data_dir, batch_size=256, batch_size_test=256, num_workers=4, use_augmentation='base', use_consistency=False, shuffle_train=True, \n",
    "              aux_data_filename=None, unsup_fraction=None, validation=False):\n",
    "    \"\"\"\n",
    "    Returns train, test datasets and dataloaders.\n",
    "    Arguments:\n",
    "        data_dir (str): path to data directory.\n",
    "        batch_size (int): batch size for training.\n",
    "        batch_size_test (int): batch size for validation.\n",
    "        num_workers (int): number of workers for loading the data.\n",
    "        use_augmentation (base/none): whether to use augmentations for training set.\n",
    "        shuffle_train (bool): whether to shuffle training set.\n",
    "        aux_data_filename (str): path to unlabelled data.\n",
    "        unsup_fraction (float): fraction of unlabelled data per batch.\n",
    "        validation (bool): if True, also returns a validation dataloader for unspervised cifar10 (as in Gowal et al, 2020).\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = os.path.basename(os.path.normpath(data_dir))\n",
    "    # load_dataset_fn = _LOAD_DATASET_FN[dataset]\n",
    "    \n",
    "    if validation:\n",
    "        # assert dataset in SEMISUP_DATASETS, 'Only semi-supervised datasets allow a validation set.'\n",
    "        train_dataset, test_dataset, val_dataset = load_cifar10s(data_dir=data_dir, use_augmentation=use_augmentation, use_consistency=use_consistency,\n",
    "                                                                   aux_data_filename=aux_data_filename, validation=True)\n",
    "    else:\n",
    "        train_dataset, test_dataset = load_cifar10s(data_dir=data_dir, use_augmentation=use_augmentation)\n",
    "       \n",
    "    # if dataset in SEMISUP_DATASETS:\n",
    "    if validation:\n",
    "        train_dataloader, test_dataloader, val_dataloader = get_semisup_dataloaders(\n",
    "                train_dataset, test_dataset, val_dataset, batch_size, batch_size_test, num_workers, unsup_fraction )\n",
    "    else:\n",
    "        train_dataloader, test_dataloader = get_semisup_dataloaders(\n",
    "                train_dataset, test_dataset, None, batch_size, batch_size_test, num_workers, unsup_fraction )\n",
    "    # else:\n",
    "    #     #pin_memory = torch.cuda.is_available()\n",
    "    #     pin_memory = False\n",
    "    #     train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train, \n",
    "    #                                                    num_workers=num_workers, pin_memory=pin_memory)\n",
    "    #     test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, \n",
    "    #                                                   num_workers=num_workers, pin_memory=pin_memory)\n",
    "    if validation:\n",
    "        return train_dataset, test_dataset, val_dataset, train_dataloader, test_dataloader, val_dataloader\n",
    "    return train_dataset, test_dataset, train_dataloader, test_dataloader\n",
    "\n",
    "data_dir = './data'\n",
    "train_dataset, test_dataset, train_dataloader, test_dataloader = load_data(data_dir, \n",
    "          batch_size=256, \n",
    "          batch_size_test=256, \n",
    "          num_workers=1, \n",
    "          use_augmentation='base', \n",
    "          use_consistency=False, \n",
    "          shuffle_train=True, \n",
    "          aux_data_filename='./data/1m.npz', \n",
    "          unsup_fraction=None, \n",
    "          validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.dataset_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
