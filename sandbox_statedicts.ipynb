{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset, load_data\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import DistributedCustomSampler\n",
    "from tqdm.notebook import tqdm\n",
    "from architectures import load_architecture, add_lora, set_lora_gradients #load_statedict\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "\n",
    "# args.loss_function = 'APGD'\n",
    "\n",
    "# args.iterations = 20\n",
    "# args.pruning_ratio = 0.99\n",
    "# args.delta = 1\n",
    "\n",
    "# args.init_lr = 0.001\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "# model = load_architecture(args)\n",
    "\n",
    "# state_dict = torch.load('./state_dicts/convnext_tiny_22k_224.pth')\n",
    "# model.load_state_dict(state_dict)\n",
    "# model, target_layers = load_architecture(args)\n",
    "# add_lora(args, model, target_layers)\n",
    "# set_lora_gradients(args, model, target_layers)\n",
    "\n",
    "# model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deit_small_patch16_224.fb_in1k\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "vit_models = timm.list_models('*deit_small_patch16_224*', pretrained=True)\n",
    "for model_name in vit_models:\n",
    "    print(model_name)\n",
    "\n",
    "\n",
    "# vit_small_patch16_224, vit_base_patch16_224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from timm.models import create_model\n",
    "\n",
    "# #'convnext_base_robust','convnext_base_robust',\n",
    "\n",
    "# backbones = [ 'convnext_base',  'convnext_base.fb_in22k', 'convnext_tiny',  'convnext_tiny.fb_in22k' ]\n",
    "backbones = [ 'deit_small_patch16_224.fb_in1k',  \n",
    "              'vit_base_patch16_224.augreg_in1k', \n",
    "              'vit_base_patch16_224.augreg_in21k' ]\n",
    "\n",
    "for backbone in backbones:\n",
    "\n",
    "    model = timm.create_model(backbone, pretrained=True)\n",
    "    torch.save(model.state_dict(), './state_dicts/{}.pt'.format(backbone) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard loading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ImageNormalizer(nn.Module):\n",
    "    def __init__(self, mean: Tuple[float, float, float],\n",
    "        std: Tuple[float, float, float],\n",
    "        persistent: bool = True) -> None:\n",
    "        super(ImageNormalizer, self).__init__()\n",
    "\n",
    "        self.register_buffer('mean', torch.as_tensor(mean).view(1, 3, 1, 1),\n",
    "            persistent=persistent)\n",
    "        self.register_buffer('std', torch.as_tensor(std).view(1, 3, 1, 1),\n",
    "            persistent=persistent)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return (input - self.mean) / self.std\n",
    "\n",
    "def normalize_model(model: nn.Module, mean: Tuple[float, float, float],\n",
    "    std: Tuple[float, float, float]) -> nn.Module:\n",
    "    layers = OrderedDict([\n",
    "        ('normalize', ImageNormalizer(mean, std)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    return nn.Sequential(layers)\n",
    "\n",
    "IMAGENET_MEAN = [c * 1. for c in (0.485, 0.456, 0.406)] #[np.array([0., 0., 0.]), np.array([0.485, 0.456, 0.406])][-1] * 255\n",
    "IMAGENET_STD = [c * 1. for c in (0.229, 0.224, 0.225)] #[np.array([1., 1., 1.]), np.array([0.229, 0.224, 0.225])][-1] * 255\n",
    "\n",
    "backbone = 'vit_base_patch16_224'\n",
    "\n",
    "model = create_model(backbone, pretrained=False)\n",
    "# model = timm.create_model(backbone, pretrained=False)\n",
    "# model = normalize_model(model, IMAGENET_MEAN, IMAGENET_STD)\n",
    "\n",
    "ckpt = torch.load('./state_dicts/weights_vit_b_50_ep.pt', map_location='cpu', weights_only=False)\n",
    "ckpt = {k.replace('module.', ''): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace('base_model.', ''): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace('se_', 'se_module.'): v for k, v in ckpt.items()}\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(ckpt)\n",
    "    print('standard loading')\n",
    "\n",
    "except:\n",
    "    try:\n",
    "        ckpt = {f'base_model.{k}': v for k, v in ckpt.items()}\n",
    "        model.load_state_dict(ckpt)\n",
    "        print('loaded from clean model')\n",
    "    except:\n",
    "        ckpt = {k.replace('base_model.', ''): v for k, v in ckpt.items()}\n",
    "        # ckpt = {f'base_model.{k}': v for k, v in ckpt.items()}\n",
    "        model.load_state_dict(ckpt)\n",
    "        print('loaded')\n",
    "\n",
    "if isinstance(model, nn.Sequential) and 'normalize' in model._modules: # remove normalization layer\n",
    "    # Rebuild the sequential model without the 'normalize' layer\n",
    "    model = model._modules['model']\n",
    "\n",
    "torch.save(model.state_dict(), './state_dicts/robust_{}.pt'.format(backbone) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ckpt = torch.load('./state_dicts/tiny_linf_wrn28-10.pt')\n",
    "# ckpt = {k.replace('module.0.', ''): v for k, v in ckpt['model_state_dict'].items()}\n",
    "# model.load_state_dict(ckpt)\n",
    "\n",
    "# torch.save(model.state_dict(), './state_dicts/robust_wideresnet_28_10.pt'.format(backbone) )\n",
    "\n",
    "from architectures.wideresnetswish import wideresnet\n",
    "ckpt = torch.load('./state_dicts/wideresnet_28_10.pt')\n",
    "ckpt = {k.replace('module.0.', ''): v for k, v in ckpt['model_state_dict'].items()}\n",
    "model = wideresnet(depth = 28, widen = 10, act_fn = 'swish', num_classes = 200)\n",
    "model.load_state_dict(ckpt)\n",
    "torch.save(model.state_dict(), './state_dicts/wideresnet_28_10.pt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnext_base\n",
      "\n",
      "convnext_base.fb_in22k\n",
      "\n",
      "robust_convnext_base\n",
      "\n",
      "random_convnext_base\n",
      "\n",
      "convnext_tiny\n",
      "\n",
      "convnext_tiny.fb_in22k\n",
      "\n",
      "robust_convnext_tiny\n",
      "\n",
      "random_convnext_tiny\n",
      "\n",
      "deit_small_patch16_224.fb_in1k\n",
      "\n",
      "robust_deit_small_patch16_224\n",
      "\n",
      "random_deit_small_patch16_224\n",
      "\n",
      "vit_base_patch16_224.augreg_in1k\n",
      "\n",
      "vit_base_patch16_224.augreg_in21k\n",
      "\n",
      "robust_vit_base_patch16_224\n",
      "\n",
      "random_vit_base_patch16_224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from architectures.wideresnetswish import wideresnet\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import timm\n",
    "from timm.models import create_model\n",
    "\n",
    "backbones =  [ 'convnext_base',  'convnext_base.fb_in22k', 'robust_convnext_base', 'random_convnext_base',\n",
    "               'convnext_tiny',  'convnext_tiny.fb_in22k', 'robust_convnext_tiny', 'random_convnext_tiny',\n",
    "\n",
    "              'deit_small_patch16_224.fb_in1k', 'robust_deit_small_patch16_224', 'random_deit_small_patch16_224',\n",
    "              \n",
    "              'vit_base_patch16_224.augreg_in1k', 'vit_base_patch16_224.augreg_in21k', 'robust_vit_base_patch16_224', 'random_vit_base_patch16_224' ] \n",
    "\n",
    "#'robust_wideresnet_28_10', 'wideresnet_28_10', \n",
    "\n",
    "\n",
    "for backbone in backbones:\n",
    "    print(backbone)\n",
    "    args.backbone = backbone\n",
    "    N = 10\n",
    "    model = load_architecture(args,N, rank= 0)\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(name)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (init_conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer): Sequential(\n",
       "    (0): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batchnorm): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (relu): SiLU(inplace=True)\n",
       "  (logits): Linear(in_features=640, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import peft\n",
    "\n",
    "args.backbone = \"robust_wideresnet_28_10\"\n",
    "args.N = 10\n",
    "model = load_architecture(args,)\n",
    "model\n",
    "# Configure LoRA for intermediate layers in DeiT\n",
    "\n",
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (init_conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer): Sequential(\n",
       "    (0): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batchnorm): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (relu): SiLU(inplace=True)\n",
       "  (logits): Linear(in_features=640, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from architectures.wideresnetswish import wideresnet\n",
    "\n",
    "depth = 28\n",
    "widen = 10\n",
    "act_fn = 'swish'  # Assuming 'swish' is the desired activation function\n",
    "num_classes = 200\n",
    "model = wideresnet(depth, widen, act_fn, num_classes)\n",
    "ckpt = torch.load('./state_dicts/tiny_linf_wrn28-10.pt')\n",
    "ckpt = {k.replace('module.0.', ''): v for k, v in ckpt['model_state_dict'].items()}\n",
    "model.load_state_dict(ckpt)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "from timm.models import create_model\n",
    "\n",
    "model = timm.create_model('convnext_tiny.fb_in22k', pretrained=True)\n",
    "model_save_path = \"./state_dicts/test.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "state_dict = torch.load('./state_dicts/test.pth')\n",
    "model = timm.models.convnext.convnext_tiny(pretrained=False)\n",
    "num_features = model.head.fc.in_features\n",
    "model.head.fc = nn.Linear(num_features, 21841)  \n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "equivalencies = { 'convnext_base':'convnext_base',\n",
    "                      'convnext_base.fb_in22k':'convnext_base.fb_in22k', \n",
    "                      'robust_convnext_base':'convnext_base',\n",
    "                      \n",
    "                      'convnext_tiny':'convnext_tiny',\n",
    "                      'convnext_tiny.fb_in22k':'convnext_tiny.fb_in22k',\n",
    "                      'robust_convnext_tiny':'convnext_tiny',\n",
    "\n",
    "                      'robust_wideresnet_28_10': 'robust_wideresnet_28_10',\n",
    "\n",
    "                      'deit_small_patch16_224.fb_in1k': 'deit_small_patch16_224.fb_in1k',\n",
    "                      'robust_deit_small_patch16_224': 'deit_small_patch16_224',\n",
    "\n",
    "                      'vit_base_patch16_224.augreg_in1k':'vit_base_patch16_224.augreg_in1k',\n",
    "                      'vit_base_patch16_224.augreg_in21k':'vit_base_patch16_224.augreg_in21k',\n",
    "                      'robust_vit_base_patch16_224': 'vit_base_patch16_224'\n",
    "                           \n",
    "                }\n",
    "\n",
    "backbone = 'robust_wideresnet_28_10'\n",
    "\n",
    "from architectures.wideresnetswish import wideresnet\n",
    "model = wideresnet(depth = 28, widen = 10, act_fn = 'swish', num_classes = 200)\n",
    "\n",
    "# model = timm.create_model( equivalencies[backbone], pretrained=False )\n",
    "state_dict = torch.load('./state_dicts/{}.pt'.format(backbone) )\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (init_conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer): Sequential(\n",
       "    (0): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batchnorm): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (relu): SiLU(inplace=True)\n",
       "  (logits): Linear(in_features=640, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model.head.in_features\n",
    "model.head = nn.Linear(num_features, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
