{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import IndexedDataset, WeightedDataset, load_data\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from utils import get_args\n",
    "from architectures import load_architecture\n",
    "\n",
    "from samplers import DistributedCustomSampler\n",
    "from tqdm.notebook import tqdm\n",
    "from architectures import load_architecture, add_lora, set_lora_gradients #load_statedict\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "\n",
    "# args.loss_function = 'APGD'\n",
    "\n",
    "# args.iterations = 20\n",
    "# args.pruning_ratio = 0.99\n",
    "# args.delta = 1\n",
    "\n",
    "# args.init_lr = 0.001\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "# model = load_architecture(args)\n",
    "\n",
    "# state_dict = torch.load('./state_dicts/convnext_tiny_22k_224.pth')\n",
    "# model.load_state_dict(state_dict)\n",
    "# model, target_layers = load_architecture(args)\n",
    "# add_lora(args, model, target_layers)\n",
    "# set_lora_gradients(args, model, target_layers)\n",
    "\n",
    "# model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deit_small_patch16_224.fb_in1k\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "vit_models = timm.list_models('*deit_small_patch16_224*', pretrained=True)\n",
    "for model_name in vit_models:\n",
    "    print(model_name)\n",
    "\n",
    "\n",
    "# vit_small_patch16_224, vit_base_patch16_224\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from timm.models import create_model\n",
    "\n",
    "# #'convnext_base_robust','convnext_base_robust',\n",
    "\n",
    "# backbones = [ 'convnext_base',  'convnext_base.fb_in22k', 'convnext_tiny',  'convnext_tiny.fb_in22k' ]\n",
    "backbones = [ 'deit_small_patch16_224.fb_in1k',  \n",
    "              'vit_base_patch16_224.augreg_in1k', \n",
    "              'vit_base_patch16_224.augreg_in21k' ]\n",
    "\n",
    "for backbone in backbones:\n",
    "\n",
    "    model = timm.create_model(backbone, pretrained=True)\n",
    "    torch.save(model.state_dict(), './state_dicts/{}.pt'.format(backbone) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard loading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ImageNormalizer(nn.Module):\n",
    "    def __init__(self, mean: Tuple[float, float, float],\n",
    "        std: Tuple[float, float, float],\n",
    "        persistent: bool = True) -> None:\n",
    "        super(ImageNormalizer, self).__init__()\n",
    "\n",
    "        self.register_buffer('mean', torch.as_tensor(mean).view(1, 3, 1, 1),\n",
    "            persistent=persistent)\n",
    "        self.register_buffer('std', torch.as_tensor(std).view(1, 3, 1, 1),\n",
    "            persistent=persistent)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return (input - self.mean) / self.std\n",
    "\n",
    "def normalize_model(model: nn.Module, mean: Tuple[float, float, float],\n",
    "    std: Tuple[float, float, float]) -> nn.Module:\n",
    "    layers = OrderedDict([\n",
    "        ('normalize', ImageNormalizer(mean, std)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    return nn.Sequential(layers)\n",
    "\n",
    "IMAGENET_MEAN = [c * 1. for c in (0.485, 0.456, 0.406)] #[np.array([0., 0., 0.]), np.array([0.485, 0.456, 0.406])][-1] * 255\n",
    "IMAGENET_STD = [c * 1. for c in (0.229, 0.224, 0.225)] #[np.array([1., 1., 1.]), np.array([0.229, 0.224, 0.225])][-1] * 255\n",
    "\n",
    "backbone = 'vit_base_patch16_224'\n",
    "\n",
    "model = create_model(backbone, pretrained=False)\n",
    "# model = timm.create_model(backbone, pretrained=False)\n",
    "# model = normalize_model(model, IMAGENET_MEAN, IMAGENET_STD)\n",
    "\n",
    "ckpt = torch.load('./state_dicts/weights_vit_b_50_ep.pt', map_location='cpu', weights_only=False)\n",
    "ckpt = {k.replace('module.', ''): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace('base_model.', ''): v for k, v in ckpt.items()}\n",
    "ckpt = {k.replace('se_', 'se_module.'): v for k, v in ckpt.items()}\n",
    "\n",
    "try:\n",
    "    model.load_state_dict(ckpt)\n",
    "    print('standard loading')\n",
    "\n",
    "except:\n",
    "    try:\n",
    "        ckpt = {f'base_model.{k}': v for k, v in ckpt.items()}\n",
    "        model.load_state_dict(ckpt)\n",
    "        print('loaded from clean model')\n",
    "    except:\n",
    "        ckpt = {k.replace('base_model.', ''): v for k, v in ckpt.items()}\n",
    "        # ckpt = {f'base_model.{k}': v for k, v in ckpt.items()}\n",
    "        model.load_state_dict(ckpt)\n",
    "        print('loaded')\n",
    "\n",
    "if isinstance(model, nn.Sequential) and 'normalize' in model._modules: # remove normalization layer\n",
    "    # Rebuild the sequential model without the 'normalize' layer\n",
    "    model = model._modules['model']\n",
    "\n",
    "torch.save(model.state_dict(), './state_dicts/robust_{}.pt'.format(backbone) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ckpt = torch.load('./state_dicts/tiny_linf_wrn28-10.pt')\n",
    "# ckpt = {k.replace('module.0.', ''): v for k, v in ckpt['model_state_dict'].items()}\n",
    "# model.load_state_dict(ckpt)\n",
    "\n",
    "# torch.save(model.state_dict(), './state_dicts/robust_wideresnet_28_10.pt'.format(backbone) )\n",
    "\n",
    "from architectures.wideresnetswish import wideresnet\n",
    "ckpt = torch.load('./state_dicts/wideresnet_28_10.pt')\n",
    "ckpt = {k.replace('module.0.', ''): v for k, v in ckpt['model_state_dict'].items()}\n",
    "model = wideresnet(depth = 28, widen = 10, act_fn = 'swish', num_classes = 200)\n",
    "model.load_state_dict(ckpt)\n",
    "torch.save(model.state_dict(), './state_dicts/wideresnet_28_10.pt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deit_small_patch16_224.in1k\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './state_dicts/deit_small_patch16_224.in1k.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m args\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m backbone\n\u001b[1;32m     30\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_architecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name)\n",
      "File \u001b[0;32m~/Desktop/robust_training/architectures/loaders.py:55\u001b[0m, in \u001b[0;36mload_architecture\u001b[0;34m(args, N, rank)\u001b[0m\n\u001b[1;32m     53\u001b[0m     model \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(equivalencies[args\u001b[38;5;241m.\u001b[39mbackbone], pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbackbone:\n\u001b[0;32m---> 55\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./state_dicts/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvit\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbackbone:\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Desktop/robust_training/.venv/lib/python3.8/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './state_dicts/deit_small_patch16_224.in1k.pt'"
     ]
    }
   ],
   "source": [
    "from architectures.wideresnetswish import wideresnet\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import timm\n",
    "from timm.models import create_model\n",
    "\n",
    "backbones =  [ 'convnext_base',  'convnext_base.fb_in22k', 'robust_convnext_base',\n",
    "               'convnext_tiny',  'convnext_tiny.fb_in22k', 'robust_convnext_tiny',\n",
    "\n",
    "              'robust_wideresnet_28_10', 'wideresnet_28_10', \n",
    "\n",
    "              'deit_small_patch16_224.fb_in1k',\n",
    "              'robust_deit_small_patch16_224',\n",
    "              \n",
    "              'vit_base_patch16_224.augreg_in1k',\n",
    "              'vit_base_patch16_224.augreg_in21k',\n",
    "              'robust_vit_base_patch16_224' ]\n",
    "\n",
    "\n",
    "for backbone in backbones:\n",
    "    print(backbone)\n",
    "    args.backbone = backbone\n",
    "    N = 10\n",
    "    model = load_architecture(args,N, rank= 0)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name)\n",
    "\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (init_conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer): Sequential(\n",
       "    (0): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batchnorm): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (relu): SiLU(inplace=True)\n",
       "  (logits): Linear(in_features=640, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import peft\n",
    "\n",
    "args.backbone = \"robust_wideresnet_28_10\"\n",
    "args.N = 10\n",
    "model = load_architecture(args,)\n",
    "model\n",
    "# Configure LoRA for intermediate layers in DeiT\n",
    "\n",
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (init_conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer): Sequential(\n",
       "    (0): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batchnorm): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (relu): SiLU(inplace=True)\n",
       "  (logits): Linear(in_features=640, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from architectures.wideresnetswish import wideresnet\n",
    "\n",
    "depth = 28\n",
    "widen = 10\n",
    "act_fn = 'swish'  # Assuming 'swish' is the desired activation function\n",
    "num_classes = 200\n",
    "model = wideresnet(depth, widen, act_fn, num_classes)\n",
    "ckpt = torch.load('./state_dicts/tiny_linf_wrn28-10.pt')\n",
    "ckpt = {k.replace('module.0.', ''): v for k, v in ckpt['model_state_dict'].items()}\n",
    "model.load_state_dict(ckpt)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "from timm.models import create_model\n",
    "\n",
    "model = timm.create_model('convnext_tiny.fb_in22k', pretrained=True)\n",
    "model_save_path = \"./state_dicts/test.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "state_dict = torch.load('./state_dicts/test.pth')\n",
    "model = timm.models.convnext.convnext_tiny(pretrained=False)\n",
    "num_features = model.head.fc.in_features\n",
    "model.head.fc = nn.Linear(num_features, 21841)  \n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "equivalencies = { 'convnext_base':'convnext_base',\n",
    "                      'convnext_base.fb_in22k':'convnext_base.fb_in22k', \n",
    "                      'robust_convnext_base':'convnext_base',\n",
    "                      \n",
    "                      'convnext_tiny':'convnext_tiny',\n",
    "                      'convnext_tiny.fb_in22k':'convnext_tiny.fb_in22k',\n",
    "                      'robust_convnext_tiny':'convnext_tiny',\n",
    "\n",
    "                      'robust_wideresnet_28_10': 'robust_wideresnet_28_10',\n",
    "\n",
    "                      'deit_small_patch16_224.fb_in1k': 'deit_small_patch16_224.fb_in1k',\n",
    "                      'robust_deit_small_patch16_224': 'deit_small_patch16_224',\n",
    "\n",
    "                      'vit_base_patch16_224.augreg_in1k':'vit_base_patch16_224.augreg_in1k',\n",
    "                      'vit_base_patch16_224.augreg_in21k':'vit_base_patch16_224.augreg_in21k',\n",
    "                      'robust_vit_base_patch16_224': 'vit_base_patch16_224'\n",
    "                           \n",
    "                }\n",
    "\n",
    "backbone = 'robust_wideresnet_28_10'\n",
    "\n",
    "from architectures.wideresnetswish import wideresnet\n",
    "model = wideresnet(depth = 28, widen = 10, act_fn = 'swish', num_classes = 200)\n",
    "\n",
    "# model = timm.create_model( equivalencies[backbone], pretrained=False )\n",
    "state_dict = torch.load('./state_dicts/{}.pt'.format(backbone) )\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (init_conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer): Sequential(\n",
       "    (0): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(16, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(160, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): _BlockGroup(\n",
       "      (block): Sequential(\n",
       "        (0): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(320, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (1): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (2): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): _Block(\n",
       "          (batchnorm_0): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_0): SiLU(inplace=True)\n",
       "          (conv_0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "          (batchnorm_1): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (relu_1): SiLU(inplace=True)\n",
       "          (conv_1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batchnorm): BatchNorm2d(640, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (relu): SiLU(inplace=True)\n",
       "  (logits): Linear(in_features=640, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model.head.in_features\n",
    "model.head = nn.Linear(num_features, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
